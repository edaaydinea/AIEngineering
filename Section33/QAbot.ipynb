{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fedb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c154698ab844f5dbd540843f6e07796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eda AYDIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Eda AYDIN\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e148b74ec814a26971e2bcbcbe45de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe110d483834b0fa5a983172fe07edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562ab2629921408b90ebea96088dfac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b82a692efd24699a4f2daf7262500c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' and 'model' are already loaded as per previous contexts\n",
    "# e.g., \n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22acc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sunset_Motors_context = \"\"\"\n",
    "The first DVD (Digital Versatile Disc) was released on March 24, 1997. It was a movie titled 'Twister' and was released in Japan. DVDs quickly gained popularity as a replacement for VHS tapes and became a common format for storing and distributing digital video and data.\n",
    "\n",
    "Sunset Motors is a renowned automobile dealership that has been a cornerstone of the automotive industry since its establishment in 1978. Located in the picturesque town of Crestwood, nestled in the heart of California's scenic Central Valley, Sunset Motors has built a reputation for excellence, reliability, and customer satisfaction over the past four decades. Founded by visionary entrepreneur Robert Anderson, Sunset Motors began as a humble, family-owned business with a small lot of used cars. However, under Anderson's leadership and commitment to quality, it quickly evolved into a thriving dealership offering a wide range of vehicles from various manufacturers. Today, the dealership spans over 10 acres, showcasing a vast inventory of new and pre-owned cars, trucks, SUVs, and luxury vehicles. One of Sunset Motors' standout features is its dedication to sustainability. In 2010, the dealership made a landmark decision to incorporate environmentally friendly practices, including solar panels to power the facility, energy-efficient lighting, and a comprehensive recycling program. This commitment to eco-consciousness has earned Sunset Motors recognition as an industry leader in sustainable automotive retail. Sunset Motors proudly offers a diverse range of vehicles, including popular brands like Ford, Toyota, Honda, Chevrolet, and BMW, catering to a wide spectrum of tastes and preferences. In addition to its outstanding vehicle selection, Sunset Motors offers flexible financing options, allowing customers to secure affordable loans and leases with competitive interest rates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da1b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FAQ_bot(question):\n",
    "    context = Sunset_Motors_context\n",
    "\n",
    "    # Encode question and context\n",
    "    # The video uses tokenizer.encode which only returns input_ids\n",
    "    input_ids = tokenizer.encode(question, context)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Create segment embeddings (token_type_ids)\n",
    "    # Find separator token ID index\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # Number of tokens in segment A (question) and B (context)\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Create list of segment IDs\n",
    "    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "    # Ensure segment_ids length matches input_ids length\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # Feed into the model\n",
    "    # Model expects a batch, so wrap input_ids and segment_ids in a list\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "    # Get answer start and end positions\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "    answer_text = \"\" # Initialize answer_text\n",
    "\n",
    "    if answer_end >= answer_start:\n",
    "        answer_tokens = tokens[answer_start:answer_end+1]\n",
    "\n",
    "        # Correcting and joining tokens\n",
    "        corrected_answer_tokens = []\n",
    "        for word in answer_tokens:\n",
    "            if word.startswith(\"##\"):\n",
    "                corrected_answer_tokens.append(word[2:])\n",
    "            else:\n",
    "                corrected_answer_tokens.append(word)\n",
    "        # This part of token joining can be improved for proper spacing\n",
    "        # The video's direct join implies subwords might not always form full words correctly without tokenizer.decode\n",
    "        # A more robust way:\n",
    "        # answer_text = tokenizer.decode(input_ids[answer_start:answer_end+1])\n",
    "        # For simplicity matching the video's approach of joining potentially cleaned sub-tokens:\n",
    "\n",
    "        # The video's approach to cleaning \"##\" and joining:\n",
    "        raw_answer = \" \".join(answer_tokens) # First join, then clean typical ## issues\n",
    "        corrected_answer_list = []\n",
    "        for word_piece in raw_answer.split(): # Split by space, which might be present in tokens like '[SEP]'\n",
    "            if word_piece.startswith(\"##\"):\n",
    "                 # This logic is a bit simplified in the video's explanation;\n",
    "                 # usually, ## means it attaches to the *previous* token.\n",
    "                 # A direct join then replace \" ##\" with \"\" or using tokenizer.decode is better.\n",
    "                 # Let's try to match the spirit of individual word correction if \"##\" appears strangely.\n",
    "                 # The video's code snippet for correction:\n",
    "                 # for word in answer.split(): (where answer was ' '.join(tokens[start:end+1]))\n",
    "                 #    if \"##\" in word:\n",
    "                 #        corrected_answer += word.replace(\"##\", \"\")\n",
    "                 #    else:\n",
    "                 #        corrected_answer += \" \" + word\n",
    "                 # This is error-prone. A simpler direct approach for the example:\n",
    "                pass # handled by string replace below or by joining smarter\n",
    "\n",
    "        # Simplified cleanup as implied by video's final join\n",
    "        answer_text = \" \".join(tokens[answer_start:answer_end+1]).replace(' ##', '').replace('##', '')\n",
    "\n",
    "\n",
    "    else:\n",
    "        answer_text = \"I'm unable to find the answer to this question. Can you please ask me another question?\"\n",
    "\n",
    "    # The video script shows a loop for `corrected_answer`\n",
    "    # Let's refine based on the described loop:\n",
    "    if answer_end >= answer_start:\n",
    "        answer_span_tokens = tokens[answer_start:answer_end+1]\n",
    "        # The video's specific cleanup loop logic:\n",
    "        temp_answer = \" \".join(answer_span_tokens) # Join with spaces\n",
    "        # The video's correction logic is a bit flawed if applied word by word after splitting by space.\n",
    "        # True WordPiece decoding is more complex.\n",
    "        # A direct replacement to remove '##' while preserving attached parts:\n",
    "        corrected_answer = temp_answer.replace(\" ##\", \"\").replace(\"##\", \"\")\n",
    "        # If it's about individual token cleaning:\n",
    "        # cleaned_tokens = [token.replace(\"##\", \"\") if \"##\" in token else token for token in answer_span_tokens]\n",
    "        # corrected_answer = \" \".join(cleaned_tokens) -> this might add extra spaces.\n",
    "        # The most robust is tokenizer.decode()\n",
    "        # For this example, sticking to the video's described simple replace on the joined string:\n",
    "        answer_text = corrected_answer\n",
    "    else:\n",
    "        answer_text = \"I'm unable to find the answer to this question. Can you please ask me another question?\"\n",
    "\n",
    "    return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51d2c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Where is the dealership Located?\n",
      "A: crestwood\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "question1 = \"Where is the dealership Located?\"\n",
    "response1 = FAQ_bot(question1)\n",
    "print(f\"Q: {question1}\\nA: {response1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b035a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What make of cars are available?\n",
      "A: ford , toyota , honda , chevrolet , and bmw\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What make of cars are available?\"\n",
    "response2 = FAQ_bot(question2)\n",
    "print(f\"Q: {question2}\\nA: {response2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49cb5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How large is the dealership?\n",
      "A: 10 acres\n"
     ]
    }
   ],
   "source": [
    "question3 = \"How large is the dealership?\"\n",
    "response3 = FAQ_bot(question3)\n",
    "print(f\"Q: {question3}\\nA: {response3}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
