{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1e312b",
   "metadata": {},
   "source": [
    "# Chat message history\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson introduces the `ChatMessageHistory` class from LangChain as a method for storing and managing the dialogue history in chatbot applications. Understanding how to maintain conversational context is crucial for building chatbots that can engage in coherent, multi-turn interactions and accurately respond to follow-up questions.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ‚ú® **Introducing `ChatMessageHistory`**: This LangChain class provides a dedicated and structured way to store the sequence of human and AI messages within a conversation. Its primary utility is to enable chatbots to \"remember\" previous parts of the dialogue, which is fundamental for contextual understanding and generating relevant responses.\n",
    "- ‚ûï **Adding Messages**: You can easily append messages to the history using the `add_user_message()` and `add_ai_message()` methods. These methods take string arguments and store them appropriately, building up the conversational record. This is essential for dynamically capturing the flow of interaction.\n",
    "- üìÇ **Accessing Stored Messages**: The entire conversation history can be retrieved as a list of message objects using the `.messages` attribute of the `ChatMessageHistory` instance. This list is then readily usable, for instance, when constructing more complex prompt templates that require past context.\n",
    "- üåü **Advantages over Python Lists**: While a simple Python list could store messages, `ChatMessageHistory` offers additional benefits. These include methods for easy conversion to other formats (like JSON or dictionaries), a `clear()` method for resetting history, and importantly, designed integration with more advanced LangChain memory objects. This makes managing chat states more robust and developer-friendly.\n",
    "- üîó **Contextual Follow-up Demonstration**: The lesson illustrates how to take the `.messages` list from `ChatMessageHistory`, combine it with a new `HumanMessagePromptTemplate` for a follow-up question, and feed this into a `ChatPromptTemplate`. This combined prompt, when used with an `LLMChain`, allows the language model to consider the prior conversation context to answer subsequent questions accurately.\n",
    "- ‚ö†Ô∏è **Scope and Limitations**: The `ChatMessageHistory` class itself is excellent for managing message lists, especially when history needs to be handled outside a chain or fed explicitly. However, as demonstrated, simply using it to provide context for one call doesn't mean the chain inherently \"remembers\" that context for a *separate, subsequent* `invoke` call unless the history is continually managed and re-fed or more sophisticated memory classes (to be covered in later lessons) are used.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- Why is ChatMessageHistory important to know or understand?\n",
    "    \n",
    "    It forms the basic building block for creating stateful conversational AI. Without a mechanism to record and refer to past interactions, chatbots would treat every user input as a brand new conversation, making them incapable of engaging in meaningful, extended dialogues or understanding context-dependent queries.\n",
    "    \n",
    "- How does it connect with real-world tasks, problems, or applications?\n",
    "    \n",
    "    This concept is directly applicable in any scenario requiring a chatbot to remember previous parts of a conversation. Examples include customer service bots that need to recall earlier user issues, virtual assistants remembering preferences or prior commands, and interactive learning tools that adapt based on previous user responses.\n",
    "    \n",
    "- What other concepts, techniques, or areas is this related to?\n",
    "    \n",
    "    ChatMessageHistory is closely related to:\n",
    "    \n",
    "    - **Prompt Engineering**: The history stored is often incorporated into new prompts to provide context to the LLM.\n",
    "    - **State Management**: It's a form of state management for conversational applications.\n",
    "    - **LangChain Memory Modules**: `ChatMessageHistory` is a foundational component often used by or integrated into LangChain's more comprehensive memory classes (e.g., `BufferMemory`, `ConversationSummaryMemory`), which provide more automated ways of managing and utilizing conversation history within chains.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Initializing `ChatMessageHistory` and adding messages:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "    from langchain.chains import LLMChain\n",
    "    # Assuming HumanMessage and AIMessage are part of the schema if not directly imported\n",
    "    # from langchain.schema import HumanMessage, AIMessage\n",
    "    from langchain.memory import ChatMessageHistory\n",
    "    \n",
    "    # Assume 'chat' is an initialized ChatOpenAI object\n",
    "    # chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=\"YOUR_API_KEY\")\n",
    "    \n",
    "    background_info = ChatMessageHistory()\n",
    "    background_info.add_user_message(\"Can you give me an interesting fact I probably didn't know about?\")\n",
    "    background_info.add_ai_message(\"Sure. Did you know that the longest place name on the planet is 85 letters long?\")\n",
    "    \n",
    "    # To see the messages:\n",
    "    # print(background_info.messages)\n",
    "    ```\n",
    "    \n",
    "- **Creating a chat prompt template with history and a follow-up question:**\n",
    "    \n",
    "    ```python\n",
    "    # message_template_human will capture the user's follow-up\n",
    "    message_template_human = HumanMessagePromptTemplate.from_template(\"{follow_up_question}\")\n",
    "    \n",
    "    # Construct the chat prompt template\n",
    "    # background_info.messages will be a list like [HumanMessage(...), AIMessage(...)]\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        background_info.messages + [message_template_human]\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Creating and invoking the LLM chain with context:**\n",
    "    \n",
    "    ```python\n",
    "    # Assume 'chat' is an initialized ChatOpenAI instance\n",
    "    # chain = LLMChain(llm=chat, prompt=chat_template)\n",
    "    \n",
    "    # response = chain.invoke({\"follow_up_question\": \"What is the name?\"})\n",
    "    # print(response['text'])\n",
    "    # Expected output would be the long place name.\n",
    "    \n",
    "    # Demonstrating the limitation with a second, independent follow-up:\n",
    "    # response_2 = chain.invoke({\"follow_up_question\": \"What does the name mean?\"})\n",
    "    # print(response_2['text'])\n",
    "    # The model might not remember it provided the name in the previous invoke without further memory management.\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "You can use `ChatMessageHistory` when developing or testing conversational AI components to explicitly manage and inject conversational context, making it easier to debug interactions or to simulate multi-turn dialogues for evaluating model responses.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "`ChatMessageHistory` is a tool that helps a chatbot keep a written record of your conversation so it can remember what was said earlier and give better, more connected answers.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "This is highly relevant for projects involving any form of chatbot or virtual assistant, including customer support automation, interactive information retrieval systems, personalized learning tools, and applications requiring sustained dialogue with users.\n",
    "\n",
    "# Conservation buffer memory: Implementing the setup\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson focuses on setting up the foundational components in LangChain for implementing persistent chatbot memory, specifically paving the way for using `ConversationBufferMemory`. Key elements introduced include the `MessagesPlaceholder` class, essential for dynamically injecting conversation history into prompts, and the `set_verbose` function, which aids in debugging by providing detailed logs of chain execution.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üîç **Addressing Memory Gaps**: The lesson begins by acknowledging the previous session's finding: while chat history could be manually fed into a prompt, the chain itself didn't retain this memory for ongoing, seamless conversation. This highlights the need for dedicated memory components.\n",
    "- üìö **Introduction to `ConversationBufferMemory`**: This LangChain memory class is flagged as a primary solution. It works by storing all human and AI messages and then feeding the entire accumulated conversation back to the chain during subsequent interactions, ensuring context is maintained. (The actual implementation of the memory object itself is deferred to the next lesson).\n",
    "- ‚ú® **Key New Imports**: For this setup, the crucial classes to import are `MessagesPlaceholder` (from `langchain.prompts`) and `ConversationBufferMemory` (from `langchain.memory`). Additionally, the utility function `set_verbose` is imported from `langchain_globals`.\n",
    "- üõ†Ô∏è **Enabling Verbose Mode with `set_verbose`**: By calling `set_verbose(True)`, developers can activate a detailed logging mode for LangChain executions. This mode displays the final response along with all intermediate steps, proving invaluable for monitoring the chain's internal workings, understanding how prompts are constructed with memory, and simplifying the debugging process.\n",
    "- üé≠ **Defining Chatbot Persona with `SystemMessage`**: The lesson demonstrates using a `SystemMessage` to give the chatbot a specific instruction, such as \"Answer questions sarcastically.\" This sets the overall tone and behavioral guidelines for the AI's responses.\n",
    "- üîÑ **Dynamic History with `MessagesPlaceholder`**: The `MessagesPlaceholder` class plays a critical role. An instance of this class is created (e.g., named `message_history`) and included in the `ChatPromptTemplate`. It acts as a dynamic slot where the accumulated conversation log will be inserted by the memory component at runtime.\n",
    "- üìõ **The `variable_name` Parameter**: When creating a `MessagesPlaceholder`, the `variable_name` parameter (e.g., set to `\"message_log\"`) is crucial. This string becomes the key that the memory system uses to identify where in the prompt template the conversation history should be injected.\n",
    "- üìä **Structuring the `ChatPromptTemplate`**: The `ChatPromptTemplate` is constructed using the `from_messages` method, typically including the `SystemMessage`, the `MessagesPlaceholder` instance, and then the `HumanMessagePromptTemplate` (for the current user input). The order of these elements in the list is important for the final prompt structure.\n",
    "- ‚è≥ **Prepping Initial Context with `ChatMessageHistory`**: An instance of `ChatMessageHistory` (e.g., `background_info`) is created and populated with an initial user message and an AI reply. This hints at how pre-existing conversation snippets can be loaded into or used to initialize a memory object.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- Why is MessagesPlaceholder important to know or understand?\n",
    "    \n",
    "    MessagesPlaceholder is essential for creating flexible prompt templates that can accommodate a dynamic list of messages, such as a conversation history. Memory components like ConversationBufferMemory need a designated, named placeholder in the prompt to know where to insert the growing list of past interactions. Without it, the prompt structure would be rigid.\n",
    "    \n",
    "- How does set_verbose(True) connect with real-world tasks, problems, or applications?\n",
    "    \n",
    "    In any development process, especially with complex systems like LLM chains, set_verbose(True) provides crucial visibility. It allows developers to see the exact data flowing through the chain, including the fully formed prompts with memory, helping to quickly identify and resolve issues related to prompt construction, memory integration, or unexpected LLM outputs.\n",
    "    \n",
    "- What other concepts, techniques, or areas is this related to?\n",
    "    \n",
    "    These concepts are related to:\n",
    "    \n",
    "    - **Prompt Engineering**: `MessagesPlaceholder` directly impacts how prompts are dynamically constructed.\n",
    "    - **Stateful Applications**: Implementing memory is key to building stateful conversational AI that remembers past interactions.\n",
    "    - **Debugging and Logging**: `set_verbose` is a practical tool for debugging and understanding LangChain applications.\n",
    "    - **LangChain Memory Modules**: This lesson lays the groundwork for using various memory modules in LangChain, with `ConversationBufferMemory` being the next focus.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Imports and Verbose Mode Setup:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai import ChatOpenAI # Assuming this was part of the \"usual setup\"\n",
    "    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "    from langchain.schema import SystemMessage # Or from langchain.schema.messages import SystemMessage\n",
    "    from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "    from langchain_globals import set_verbose\n",
    "    \n",
    "    # Assumed: Environment variables configured and ChatOpenAI object initialized\n",
    "    # chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=\"YOUR_API_KEY\")\n",
    "    \n",
    "    # Activate verbose output to see intermediate steps in chain execution\n",
    "    set_verbose(True)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **System Message and Human Message Template:**\n",
    "    \n",
    "    ```python\n",
    "    # Define the chatbot's personality\n",
    "    message_system = SystemMessage(content=\"Answer questions sarcastically.\")\n",
    "    \n",
    "    # Define the template for the user's current question\n",
    "    message_template_human = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Messages Placeholder for Conversation History:**\n",
    "    \n",
    "    ```python\n",
    "    # Create a placeholder for the conversation log/history\n",
    "    # The 'variable_name' is how the memory component will identify this placeholder\n",
    "    message_history = MessagesPlaceholder(variable_name=\"message_log\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Chat Prompt Template Incorporating the Placeholder:**\n",
    "    \n",
    "    ```python\n",
    "    # Construct the chat prompt template, ensuring the correct order\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        message_system,             # System instruction first\n",
    "        message_history,            # Placeholder for past messages\n",
    "        message_template_human      # Current human input\n",
    "    ])\n",
    "    \n",
    "    # To inspect the input variables expected by this template:\n",
    "    # print(chat_template.input_variables)\n",
    "    # Expected output: ['message_log', 'question']\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Preparing Initial Chat Message History (to potentially load into memory later):**\n",
    "    \n",
    "    ```python\n",
    "    background_info = ChatMessageHistory()\n",
    "    background_info.add_user_message(\"Hi\")\n",
    "    background_info.add_ai_message(\"Oh, you really know how to make an entrance, don't you?\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "Use `MessagesPlaceholder` when designing LangChain prompts that need to include dynamic lists of messages from memory, and always enable `set_verbose(True)` during development and debugging of conversational chains to get a clear view of the prompt construction and message flow.\n",
    "- **Can I explain `MessagesPlaceholder` to a beginner in one sentence?**\n",
    "`MessagesPlaceholder` is like a special blank spot in your chatbot's instructions where the chatbot will later write down everything you've talked about before, so it can remember the conversation.\n",
    "- **Which type of project or domain would `ConversationBufferMemory` (which these concepts prepare for) be most relevant to?**\n",
    "`ConversationBufferMemory` is particularly relevant for chatbots in interactive roles like customer support, virtual assistants, or educational tools where remembering the exact preceding turns of conversation is crucial for context and coherence.\n",
    "\n",
    "# Conservation buffer memory: Configuring the chain\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson details the implementation of chatbot memory using the `ConversationBufferMemory` class, allowing a chatbot to recall previous parts of a conversation. This is crucial for creating more coherent, context-aware, and interactive conversational AI applications, as it enables the chatbot to provide relevant responses based on past interactions.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üß† **Introducing `ConversationBufferMemory`**: This class is used to store and manage the history of a conversation.\n",
    "    - **Relevance**: Essential for building chatbots that can remember previous turns in a dialogue, leading to more natural and intelligent interactions. Useful in customer service bots, personal assistants, and any application requiring contextual understanding.\n",
    "- üîë **`memory_key` Parameter**: This parameter links the memory object to the message placeholder in a prompt template.\n",
    "    - **Relevance**: Ensures that the conversation history is correctly inserted into the prompt that's sent to the language model. Critical for the LLM to have access to the conversational context.\n",
    "- üîó **`chat_memory` Parameter**: Connects a `ChatMessageHistory` instance (which stores the actual messages) to the `ConversationBufferMemory` object.\n",
    "    - **Relevance**: Allows for the explicit loading of pre-existing chat history or the management of messages as they accumulate.\n",
    "- üìú **`return_messages` Parameter**: Controls the format of the retrieved memory (list of chat messages vs. a single string).\n",
    "    - **Relevance**: Setting this to `true` is important when the prompt template expects a list of message objects (e.g., HumanMessage, AIMessage), ensuring compatibility within the processing chain.\n",
    "- üîÑ **Loading Memory**: The `load_memory_variables({})` method retrieves the stored conversation.\n",
    "    - **Relevance**: Allows inspection of the current memory state and is used internally by the chain to provide context to the LLM.\n",
    "- ‚õìÔ∏è **Integrating with `LLMChain`**: The `ConversationBufferMemory` object is passed to an `LLMChain` instance via the `memory` parameter.\n",
    "    - **Relevance**: This connects the memory management directly with the language model execution, enabling the chain to automatically load history and save new interactions.\n",
    "- üó£Ô∏è **Verbose Mode**: Setting `verbose=True` (globally via `set_verbose(True)` or per object) provides detailed logs of the chain's operations.\n",
    "    - **Relevance**: Extremely useful for debugging and understanding the flow of data, including how messages are processed and what context is being fed to the LLM.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is `ConversationBufferMemory` important to know or understand?**\n",
    "    - It's a fundamental component for moving beyond stateless chatbots. Understanding it allows developers to create applications that feel more like a continuous conversation rather than a series of disconnected queries.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - In customer support, it can remember the user's issue across multiple messages. In educational tools, it can track a student's questions and progress. In personal assistants, it remembers preferences and prior commands.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "    - It's closely related to `ChatMessageHistory` (for message storage), prompt engineering (for incorporating history into prompts), `LLMChain` (for orchestrating the LLM call with memory), and other memory types like `ConversationBufferWindowMemory` (for keeping a fixed-size window of recent messages).\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "While no direct code blocks are in the input, the lesson describes the following Python class instantiations and method calls:\n",
    "\n",
    "- Defining a `ConversationBufferMemory` instance:\n",
    "`chat_memory = ConversationBufferMemory(memory_key=\"message_log\", chat_memory=background_info, return_messages=True)`\n",
    "- Loading memory variables:\n",
    "`chat_memory.load_memory_variables({})`\n",
    "- Integrating memory into an `LLMChain`:\n",
    "`chain = LLMChain(llm=chat, prompt=chat_template, memory=chat_memory)`\n",
    "- Invoking the chain with a question:\n",
    "`response = chain({\"question\": \"Can you give me an interesting fact I probably didn't know about?\"})`\n",
    "- Setting verbose mode:\n",
    "`set_verbose(True)` or `set_verbose(False)`\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can use `ConversationBufferMemory` to build more sophisticated applications that interact with users conversationally, such as internal helpdesks that remember past queries or tools that guide users through complex data analysis tasks by remembering their previous inputs and choices. For learning, experimenting with it helps solidify understanding of state management in LLM applications.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - `ConversationBufferMemory` gives a chatbot a short-term memory, allowing it to remember what you just talked about so the conversation makes more sense.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This concept is most relevant to projects involving conversational AI, such as customer service chatbots, virtual assistants, interactive storytelling applications, and educational tutors, where maintaining context across turns is crucial for a good user experience.\n",
    "\n",
    "# Conservation buffer window memory\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson introduces the `ConversationBufferWindowMemory` class, a type of chatbot memory that retains only a fixed number of the most recent interactions. This approach addresses the inefficiency and increasing token usage associated with storing an entire conversation history, offering a balance between contextual awareness and resource management.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ventanas **Introducing `ConversationBufferWindowMemory`**: This class is a variation of `ConversationBufferMemory` that limits the number of past interactions stored.\n",
    "    - **Relevance**: Solves the problem of infinitely growing memory, which can become slow and expensive in terms of token consumption for the language model. Useful for applications where only recent context is paramount.\n",
    "- üî¢ **The `k` Parameter**: This crucial parameter defines the \"window size,\" i.e., the maximum number of recent user-AI interaction pairs to keep in memory.\n",
    "    - **Relevance**: Allows developers to control the trade-off between context length and computational/cost efficiency. A smaller `k` means less memory and faster processing but also less long-term context.\n",
    "- üóëÔ∏è **Old Message Pruning**: Once the number of interactions exceeds `k`, the oldest interactions are automatically dropped from the memory buffer.\n",
    "    - **Relevance**: Ensures the memory buffer doesn't grow indefinitely, maintaining performance and cost-effectiveness.\n",
    "- üõ°Ô∏è **System Message Preservation**: The system message is not affected by the memory window (`k`) and remains part of the context.\n",
    "    - **Relevance**: This is important because the system message often contains crucial instructions or persona definitions for the chatbot, which should persist throughout the conversation.\n",
    "- ‚öñÔ∏è **Efficiency vs. Context Trade-off**: While `ConversationBufferWindowMemory` is more efficient than keeping the full log, it means the chatbot might lose context from earlier parts of a long conversation.\n",
    "    - **Relevance**: This makes it suitable for use cases where long-term memory is not critical, or where sessions are typically short. For instance, quick Q&A bots or scenarios where the immediate preceding turns are most relevant.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "    - It provides a practical solution to manage memory in LLM applications, preventing performance degradation and excessive token costs as conversations lengthen. Understanding `k` allows for fine-tuning the chatbot's contextual awareness based on specific needs.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - This is particularly useful for chatbots that handle many short conversations or where the most recent exchanges are most pertinent (e.g., a bot answering follow-up questions about an immediately preceding statement). It helps in scenarios where long-term recall isn't essential but immediate context is.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "    - It's a direct alternative/improvement over `ConversationBufferMemory` for certain use cases. It's related to token management, prompt optimization, and strategies for maintaining conversational state efficiently. It contrasts with more complex memory systems like summary memory, which try to condense older information rather than just dropping it.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "While no direct code blocks are in the input, the lesson describes the following Python class instantiation:\n",
    "\n",
    "- Defining a `ConversationBufferWindowMemory` instance:\n",
    "`chat_memory = ConversationBufferWindowMemory(memory_key=\"message_log\", chat_memory=background_info, return_messages=True, k=2)`\n",
    "(Note: `background_info` and `return_messages=True` are assumed from the context of modifying the previous lesson's code.)\n",
    "The key part introduced is `k=2`.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - When building chatbots or conversational agents, especially for resource-constrained environments or when dealing with APIs where token count impacts cost, use `ConversationBufferWindowMemory` with an appropriate `k` value to maintain recent context efficiently. Experiment with different `k` values to see the impact on conversation quality and token usage.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - `ConversationBufferWindowMemory` gives a chatbot a memory that only holds the last few things said, like remembering only the last `k` turns of a game, to keep things quick and not get overloaded with old information.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This is most relevant for applications requiring quick, contextually-aware responses where long-term memory is less critical, such as quick support bots, interactive FAQs, or any chatbot where conversations are typically short and focused on recent topics. It's also good for managing costs in high-volume chat applications.\n",
    "\n",
    "# Conservation summary memory\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson introduces the `ConversationSummaryMemory` class, which offers an efficient way to manage chat history by summarizing the conversation progressively rather than storing messages verbatim. This method aims to retain essential information from earlier parts of a long dialogue, mimicking how humans recall past conversations, and uses a separate language model to create these summaries.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üìù **Introducing `ConversationSummaryMemory`**: This memory type condenses the conversation history into a running summary.\n",
    "    - **Relevance**: Addresses the inefficiency of storing entire chat logs (like `ConversationBufferMemory`) and the context loss of dropping old messages (like `ConversationBufferWindowMemory`). It aims for a balance by keeping the gist of the conversation.\n",
    "- üß† **LLM for Summarization**: A dedicated language model instance (e.g., `ChatOpenAI` with GPT-3.5 Turbo) is required by this memory class to perform the summarization task.\n",
    "    - **Relevance**: This allows the main conversational LLM (e.g., GPT-4) to focus on generating responses, while a potentially different, often faster or cheaper, LLM handles the background task of summarizing the dialogue as it progresses.\n",
    "- üîÑ **Progressive Summarization**: After each new human-AI interaction, the memory class prompts its LLM to update the existing summary with the new information.\n",
    "    - **Relevance**: This ensures the summary is always current and incorporates the latest exchange, providing evolving context to the main LLM.\n",
    "- üìú **String Template Compatibility**: This example uses a string `PromptTemplate` instead of chat message objects, requiring `return_messages` to be set to `False`.\n",
    "    - **Relevance**: Demonstrates flexibility in how memory can be integrated with different prompt structures. The summary itself becomes a string variable (e.g., \"message_log\") in the prompt.\n",
    "- üîç **Dual LLM Chain Process (Verbose Mode)**: When using this memory, verbose mode reveals two LLM chain invocations: one for generating the chat response and a second internal one for updating the conversation summary.\n",
    "    - **Relevance**: Provides insight into the operational mechanics, showing the main prompt for the response and a separate prompt used by the memory to instruct the summarization LLM.\n",
    "- üí° **Analogy to Human Memory**: The approach is likened to how humans remember conversations ‚Äì by recalling key points and summaries rather than exact wordings.\n",
    "    - **Relevance**: Offers a more natural and potentially more scalable way to handle long-term conversational context.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "    - It offers a sophisticated way to manage long conversations by extracting and preserving the most salient information. This is crucial for chatbots that need to maintain context over extended interactions without being overwhelmed by the full history or losing important details from early on.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - This is highly beneficial for complex problem-solving dialogues, long-term customer support interactions where understanding the entire history is key, or any AI assistant that needs to \"remember\" user preferences and past discussions over time without the high cost or token limits of full verbatim memory.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "    - It's related to text summarization techniques, prompt engineering (for both the main chat and the summarization task), and other advanced memory types like `ConversationSummaryBufferMemory` (which combines summarization with a buffer of recent messages) and `ConversationTokenBufferMemory`. It directly contrasts with `ConversationBufferMemory` and `ConversationBufferWindowMemory`.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "While no direct code blocks are in the input, the lesson describes the following Python class instantiations and parameter settings:\n",
    "\n",
    "- Defining a `ConversationSummaryMemory` instance:\n",
    "`chat_memory = ConversationSummaryMemory(llm=ChatOpenAI(), memory_key=\"message_log\", return_messages=False)`\n",
    "(The `ChatOpenAI()` instance is specifically for summarization and could use a model like GPT-3.5 Turbo).\n",
    "- Using a string `PromptTemplate`:\n",
    "    \n",
    "    ```python\n",
    "    template = \"\"\"The following is a friendly conversation between a human and an AI.\n",
    "    The AI is talkative and provides lots of specific details from its context.\n",
    "    If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "    \n",
    "    Current conversation:\n",
    "    {message_log}\n",
    "    \n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    "    prompt_template = PromptTemplate.from_template(template=template)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Loading memory variables (initially an empty string for the summary):\n",
    "`chat_memory.load_memory_variables({})`\n",
    "- The prompt used internally by `ConversationSummaryMemory` for summarization (as described):\n",
    "\"Progressively summarize the lines of conversation provided. Adding onto the previous summary Returning a new summary.\" (Followed by an example, the current summary, and the new lines of conversation).\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can use `ConversationSummaryMemory` when developing chatbots that need to handle extended dialogues, such as research assistants that discuss topics over many turns or therapeutic bots that track a user's journey. Experimenting with different LLMs for summarization can also be a valuable learning experience in optimizing cost and performance.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - `ConversationSummaryMemory` helps a chatbot remember long conversations by creating a short summary of what's been said, much like how you'd remember the main points of a long chat with a friend instead of every single word.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This concept is highly relevant for projects involving long-running, in-depth conversations such as AI tutors, complex customer support systems, narrative-driven interactive experiences, or any application where understanding the broad context of a lengthy interaction is more important than recalling every verbatim detail.\n",
    "\n",
    "# Combined memory\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson demonstrates how to use the `CombinedMemory` class to integrate multiple memory types, such as `ConversationBufferMemory` and `ConversationSummaryMemory`, into a single LLM chain. This approach allows a chatbot to benefit from different memory strategies simultaneously, for example, by having access to both a verbatim log of recent messages and a running summary of the entire conversation, providing a richer context to the language model.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ü§ù **Introducing `CombinedMemory` Class**: This class allows the integration of two or more different memory objects into a single memory unit for an LLM chain.\n",
    "    - **Relevance**: Enables more sophisticated memory management by leveraging the strengths of different memory types concurrently (e.g., short-term verbatim recall + long-term summarized recall).\n",
    "- ‚ûï **Combining `ConversationBufferMemory` and `ConversationSummaryMemory`**: The lesson specifically shows an example of combining these two types.\n",
    "    - **Relevance**: Provides the LLM with both a detailed recent history (from buffer) and a condensed overview of the entire conversation (from summary), offering a comprehensive contextual understanding.\n",
    "- üîë **`memory_key` for Each Memory**: Each individual memory object (buffer and summary) needs its own distinct `memory_key` (e.g., `message_buffer_log`, `message_summary_log`).\n",
    "    - **Relevance**: These keys serve as placeholders in the prompt template, allowing the chain to inject the content from each memory type into the correct part of the prompt.\n",
    "- ‚û°Ô∏è **`input_key` Parameter**: When combining memories, an `input_key` parameter (e.g., set to \"question\") is required for each individual memory object.\n",
    "    - **Relevance**: This likely helps the `CombinedMemory` class and the individual memory components correctly identify and process the user's current input in relation to the stored history.\n",
    "- üß© **`memories` Parameter in `CombinedMemory`**: The `CombinedMemory` instance is initialized with a `memories` parameter, which takes a list of the individual memory objects to be combined.\n",
    "    - **Relevance**: This is the core configuration step that groups the different memory instances together.\n",
    "- üìù **Prompt Template Design**: The prompt template must include separate placeholders for each memory type being combined.\n",
    "    - **Relevance**: Allows the LLM to receive and utilize the distinct pieces of context provided by each memory component (e.g., one placeholder for the buffer log, another for the summary).\n",
    "- üîÑ **Dual Context in Prompt**: The final prompt fed to the LLM contains inputs from both memory sources (e.g., the recent raw messages and the overall summary).\n",
    "    - **Relevance**: Gives the LLM a richer, multi-faceted view of the conversation history, potentially leading to more informed and contextually appropriate responses.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "    - It allows for the creation of highly nuanced conversational AI. Instead of choosing just one memory strategy, developers can layer them to provide different facets of conversational history to the LLM, catering to complex interaction scenarios.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - In advanced virtual assistants or complex support bots, the AI might need to recall exact details from the last turn (buffer) while also understanding the overarching theme of a long interaction (summary). This combination helps in maintaining both precision and broad context.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "    - This builds directly upon understanding individual memory types (`ConversationBufferMemory`, `ConversationSummaryMemory`). It's closely related to advanced prompt engineering (designing prompts to accommodate multiple context sources) and the overall architecture of conversational AI systems.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "While no direct code blocks are in the input, the lesson describes the following Python class instantiations and parameter settings:\n",
    "\n",
    "- **Prompt Template with multiple memory placeholders**:\n",
    "    \n",
    "    ```python\n",
    "    template = \"\"\"Instructions to the chatbot...\n",
    "    Recent messages:\n",
    "    {message_buffer_log}\n",
    "    \n",
    "    Summary of conversation:\n",
    "    {message_summary_log}\n",
    "    \n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    "    prompt_template = PromptTemplate.from_template(template=template)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Individual Memory Instances**:\n",
    "    \n",
    "    ```python\n",
    "    # Conversation Buffer Memory\n",
    "    chat_buffer_memory = ConversationBufferMemory(\n",
    "        memory_key=\"message_buffer_log\",\n",
    "        return_messages=False,\n",
    "        input_key=\"question\"\n",
    "    )\n",
    "    \n",
    "    # Conversation Summary Memory\n",
    "    chat_summary_memory = ConversationSummaryMemory(\n",
    "        llm=ChatOpenAI(), # For summarization\n",
    "        memory_key=\"message_summary_log\",\n",
    "        return_messages=False,\n",
    "        input_key=\"question\"\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **CombinedMemory Instance**:\n",
    "    \n",
    "    ```python\n",
    "    chat_combined_memory = CombinedMemory(\n",
    "        memories=[chat_buffer_memory, chat_summary_memory]\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **LLMChain with Combined Memory**:\n",
    "    \n",
    "    ```python\n",
    "    chain = LLMChain(llm=chat_openai_instance, prompt=prompt_template, memory=chat_combined_memory)\n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - AI-generated answer: You can use `CombinedMemory` when designing chatbots that need to handle both immediate, detailed recall and long-term thematic understanding. For instance, in a project management bot, it could remember the specifics of the last command while also retaining a summary of the project's goals discussed earlier.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - AI-generated answer: `CombinedMemory` lets a chatbot use multiple types of memory at once, like having both detailed notes of the last few things said and a general summary of the whole conversation, so it can be both precise and broadly aware.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - AI-generated answer: This concept is most relevant for sophisticated conversational AI projects requiring a deep understanding of context, such as advanced customer service bots dealing with complex, multi-turn issues, AI-powered research assistants that need to track evolving discussions, or interactive educational tools that build upon previous interactions in diverse ways."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
