{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of data preparation\n",
    "\n",
    "### Summary\n",
    "\n",
    "This segment emphasizes the critical role of data preprocessing in NLP, highlighting that the quality of input data directly impacts the accuracy of machine learning outcomes. It outlines the key steps involved in cleaning and formatting text data for effective analysis.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üßπ Data quality is paramount for accurate NLP results.\n",
    "- üóëÔ∏è \"Garbage in, garbage out\" principle applies to NLP data.\n",
    "- üìù Preprocessing involves cleaning, noise removal, and formatting.\n",
    "- üìÇ General cleaning organizes and tidies text data.\n",
    "- üîá Noise removal eliminates irrelevant data, reducing memory usage.\n",
    "- üõ†Ô∏è Formatting prepares data for specific machine learning algorithms.\n",
    "- üìà Preprocessing transforms raw text into a clean, analyzable format.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Example: General Cleaning (Conceptual)\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters, correct formatting\n",
    "    cleaned_text = remove_special_chars(text)\n",
    "    cleaned_text = correct_spelling(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example: Noise Removal (Conceptual)\n",
    "def remove_noise(text):\n",
    "    # Remove stop words, punctuation, etc.\n",
    "    no_noise_text = remove_stopwords(text)\n",
    "    no_noise_text = remove_punctuation(no_noise_text)\n",
    "    return no_noise_text\n",
    "\n",
    "# Example: Formatting (Conceptual)\n",
    "def format_data(text):\n",
    "    # Convert text to a suitable format for the model\n",
    "    formatted_text = tokenize_text(text)\n",
    "    formatted_text = vectorize_text(formatted_text)\n",
    "    return formatted_text\n",
    "```\n",
    "\n",
    "# Lowercase\n",
    "\n",
    "### Summary\n",
    "\n",
    "This segment explains the importance of converting text data to lowercase in NLP for consistency and uniformity. It demonstrates how to use Python's `lower()` function for this purpose, highlighting its benefits and potential drawbacks.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üî° Lowercasing ensures consistent word recognition in NLP.\n",
    "- ‚öñÔ∏è It prevents models from treating capitalized and lowercase words differently.\n",
    "- üßπ Lowercasing simplifies further data cleaning processes.\n",
    "- ‚ö†Ô∏è It can alter the meaning of certain words or abbreviations (e.g., \"US\").\n",
    "- üêç Python's `lower()` function efficiently converts strings to lowercase.\n",
    "- üìù Lowercasing can be applied to individual strings or lists of strings.\n",
    "- üöÄ It streamlines text data preparation for analysis and modeling.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Example: Lowercasing a single sentence\n",
    "sentence = \"Her Cat's name is Luna.\"\n",
    "lowercase_sentence = sentence.lower()\n",
    "print(lowercase_sentence)\n",
    "\n",
    "# Example: Lowercasing a list of sentences\n",
    "sentence_list = [\"The Dog is friendly.\", \"Cats are playful.\", \"BIRDS can fly.\"]\n",
    "lowercase_sentence_list = [x.lower() for x in sentence_list]\n",
    "print(lowercase_sentence_list)\n",
    "```\n",
    "\n",
    "# Removing stop words\n",
    "\n",
    "### Summary\n",
    "\n",
    "This segment demonstrates how to remove stopwords from text using the NLTK library in Python. Stopwords are common words that don't contribute much to the meaning of a text and their removal can simplify data, improve machine learning accuracy, and speed up processing.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üõë Stopwords are common, low-meaning words (e.g., \"the,\" \"and,\" \"a\").\n",
    "- üßπ Removing stopwords simplifies data and improves model performance.\n",
    "- üì¶ NLTK library is used for stopword removal.\n",
    "- üì• NLTK's `stopwords` corpus provides a list of common stopwords.\n",
    "- üìù Stopwords can be customized by adding or removing words.\n",
    "- üöÄ Removing stopwords results in a smaller, cleaner dataset.\n",
    "- üêç Python code efficiently filters out stopwords from text.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"it was too far to go to the shop and he did not want her to walk.\"\n",
    "\n",
    "# Remove stopwords\n",
    "sentence_no_stopwords = \" \".join([word for word in sentence.split() if word not in stop_words])\n",
    "print(sentence_no_stopwords)\n",
    "\n",
    "# Customize stopwords\n",
    "stop_words.remove(\"did\")\n",
    "stop_words.remove(\"not\")\n",
    "stop_words.append(\"go\")\n",
    "\n",
    "# Remove custom stopwords\n",
    "sentence_no_stopwords_custom = \" \".join([word for word in sentence.split() if word not in stop_words])\n",
    "print(sentence_no_stopwords_custom)\n",
    "```\n",
    "\n",
    "# Regular expressions\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This video tutorial introduces regular expressions (regex) in Python, explaining their syntax and usage for pattern matching within strings. It covers essential functions like `re.search` and `re.sub`, demonstrating how to find and replace text, filter reviews based on specific criteria, and remove punctuation.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üêç Importing the `re` package is the first step to using regular expressions in Python.\n",
    "- üìù Raw strings, denoted by `r`, are crucial for treating backslashes literally, avoiding unintended escape sequences.\n",
    "- üîç `re.search` helps identify if a pattern exists within a string, returning the match or `None`.\n",
    "- üîÑ `re.sub` enables replacing specific patterns with new text, useful for correcting errors or standardizing text.\n",
    "- ‚ùì The question mark `?` makes a preceding character optional in a pattern.\n",
    "- üöÄ The caret `^` symbol matches the start of a string, and the dollar sign `$` matches the end.\n",
    "- üîó The pipe `|` operator allows matching multiple patterns, like \"needed\" or \"wanted\".\n",
    "- üßπ Regex is powerful for removing punctuation, using `[^\\\\w\\\\s]` to target non-word and non-whitespace characters.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "- Importing the `re` package:\n",
    "    \n",
    "    ```python\n",
    "    import re\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Using raw strings:\n",
    "    \n",
    "    ```python\n",
    "    file_path = r\"c:\\desktop\\notes\"\n",
    "    print(file_path)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Using `re.search`:\n",
    "    \n",
    "    ```python\n",
    "    pattern = \"Sarah?\"\n",
    "    string = \"Sarah was able to help.\"\n",
    "    result = re.search(pattern, string)\n",
    "    print(result)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Using `re.sub`:\n",
    "    \n",
    "    ```python\n",
    "    string = \"Sarah was able to help.\"\n",
    "    new_string = re.sub(\"Sarah\", \"Sara\", string)\n",
    "    print(new_string)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Removing punctuation\n",
    "    \n",
    "    ```python\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    string = \"Hello, world!\"\n",
    "    no_punct = re.sub(pattern, \"\", string)\n",
    "    print(no_punct)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "# Tokenization\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video explains tokenization in Natural Language Processing (NLP), focusing on word and sentence tokenization using the NLTK library. Tokenization breaks text into smaller units (tokens) for better analysis, and it's a crucial step before further processing like vectorization.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üß© Tokenization is the process of breaking text into smaller units called tokens.\n",
    "- üìù Word tokenization splits text into individual words, while sentence tokenization splits it into sentences.\n",
    "- üìö NLTK (Natural Language Toolkit) is a Python library used for NLP tasks, including tokenization.\n",
    "- üì• NLTK requires downloading additional resources for tokenization.\n",
    "- ‚úÇÔ∏è `sent_tokenize` function from NLTK splits a text into sentences.\n",
    "- üî° `word_tokenize` function from NLTK splits a sentence into words.\n",
    "- üìâ Case sensitivity can affect token analysis, highlighting the importance of lowercasing text for consistency.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- Importing NLTK and necessary functions:\n",
    "    \n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('punkt') # Download required resources\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Sentence tokenization:\n",
    "    \n",
    "    ```python\n",
    "    text = \"Her cat's name is Luna. Her dog's name is Max.\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(sentences)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Word tokenization:\n",
    "    \n",
    "    ```python\n",
    "    sentence = \"Her cat's name is Luna.\"\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Word tokenization of a longer sentence:\n",
    "    \n",
    "    ```python\n",
    "    sentence = \"Her cat's name is Luna. Her dog's name is Max.\"\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "# Stemming\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This video explains stemming, a text standardization technique in NLP that reduces words to their base form. It uses the Porter stemmer from the NLTK library to demonstrate how words like \"connecting\" and \"learned\" are transformed. While stemming simplifies text and reduces data complexity, it can sometimes produce non-meaningful or improper words.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üõ†Ô∏è Stemming standardizes text by reducing words to their root form.\n",
    "- ‚úÇÔ∏è It removes word suffixes, but can result in non-standard words.\n",
    "- üìâ Stemming reduces the number of unique words, simplifying data for machine learning.\n",
    "- üìö NLTK's Porter stemmer is a common tool for this process.\n",
    "- üîÑ Words like \"connecting\" become \"connect,\" and \"learning\" becomes \"learn.\"\n",
    "- ‚ö†Ô∏è Some words, like \"worse,\" might be stemmed to non-words (\"wors\").\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "- Importing the Porter stemmer:\n",
    "    \n",
    "    ```python\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Initializing the stemmer:\n",
    "    \n",
    "    ```python\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Stemming words:\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"connecting\", \"connected\", \"connectivity\", \"connect\", \"connects\"]\n",
    "    for token in tokens:\n",
    "        print(ps.stem(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Stemming different words:\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"learned\", \"learning\", \"learn\", \"learns\", \"learner\", \"learners\"]\n",
    "    for token in tokens:\n",
    "        print(ps.stem(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Example with \"likes,\" \"better,\" and \"worse\":\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"likes\", \"better\", \"worse\"]\n",
    "    for token in tokens:\n",
    "        print(ps.stem(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This video contrasts stemming with lemmatization, focusing on how lemmatization reduces words to their base form while maintaining meaning. It uses WordNet Lemmatizer from the NLTK library to demonstrate how lemmatization preserves word meaning better than stemming, though it results in a larger dataset.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üß† Lemmatization reduces words to their base form using a dictionary for context.\n",
    "- üìú It aims to produce meaningful base words, unlike stemming which can create non-words.\n",
    "- üìà Lemmatization often results in a larger dataset as it preserves more word variations.\n",
    "- üìö NLTK's WordNet Lemmatizer is used for this process.\n",
    "- üîÑ Words are reduced to meaningful base forms, e.g., \"learners\" to \"learner.\"\n",
    "- üí¨ Lemmatization retains word meaning, e.g., \"worse\" remains \"worse,\" unlike stemming which produced \"wors.\"\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "- Downloading WordNet and importing the lemmatizer:\n",
    "    \n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Initializing the lemmatizer:\n",
    "    \n",
    "    **Python**\n",
    "    \n",
    "    ```python\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Lemmatizing \"connect\" tokens:\n",
    "    \n",
    "    **Python**\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"connecting\", \"connected\", \"connectivity\", \"connect\", \"connects\"]\n",
    "    for token in tokens:\n",
    "        print(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Lemmatizing \"learn\" tokens:\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"learned\", \"learning\", \"learn\", \"learns\", \"learner\", \"learners\"]\n",
    "    for token in tokens:\n",
    "        print(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Lemmatizing \"likes\" tokens:\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"likes\", \"better\", \"worse\"]\n",
    "    for token in tokens:\n",
    "        print(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "# N-grams\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video explains n-grams, which are sequences of n neighboring words or tokens used to analyze text data. It demonstrates how to compute and visualize unigrams, bigrams, and trigrams using Python libraries like NLTK, pandas, and matplotlib, highlighting their utility in preprocessing analysis and feature creation for machine learning.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üî¢ N-grams are sequences of n adjacent words or tokens in a text.\n",
    "- üìä Unigrams (n=1), bigrams (n=2), and trigrams (n=3) are common types of n-grams.\n",
    "- üêç Libraries like NLTK, pandas, and matplotlib are used for n-gram analysis and visualization.\n",
    "- üìà Visualizing n-grams, especially unigrams, helps identify frequent words or phrases.\n",
    "- üìù N-gram analysis can reveal interesting patterns and insights in text data.\n",
    "- üìâ Preprocessing, like stop word removal, can significantly alter n-gram results.\n",
    "- üñºÔ∏è Matplotlib allows for creating charts, like horizontal bar plots, to represent n-gram frequencies.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- Importing necessary libraries:\n",
    "    \n",
    "    ```python\n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Creating tokens:\n",
    "    \n",
    "    ```python\n",
    "    tokens = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \"the\", \"dog\", \"jumps\"]\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Computing and displaying unigrams:\n",
    "    \n",
    "    ```python\n",
    "    unigrams = pd.Series(nltk.ngrams(tokens, 1)).value_counts()\n",
    "    print(unigrams)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Visualizing top 10 unigrams:\n",
    "    \n",
    "    ```python\n",
    "    unigrams[:10].sort_values().plot.barh(color='lightsalmon', width=0.8, figsize=(12, 6), title='Ten Most Frequently Occurring Unigrams')\n",
    "    plt.show()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Computing and displaying bigrams:\n",
    "    \n",
    "    ```python\n",
    "    bigrams = pd.Series(nltk.ngrams(tokens, 2)).value_counts()\n",
    "    print(bigrams)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- Computing and displaying trigrams:\n",
    "    \n",
    "    ```python\n",
    "    trigrams = pd.Series(nltk.ngrams(tokens, 3)).value_counts()\n",
    "    print(trigrams)\n",
    "    \n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
