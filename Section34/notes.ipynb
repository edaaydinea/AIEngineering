{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9e7bf3",
   "metadata": {},
   "source": [
    "# GPT vs BERT vs XLNET\n",
    "\n",
    "### Summary\n",
    "\n",
    "This content introduces XLNet, a large language model developed by Google AI, explaining its architecture and key innovations. It primarily focuses on XLNet's permutation-based training approach, which allows it to capture bidirectional context more effectively than previous models like BERT and traditional autoregressive models, leading to improved performance on various NLP tasks. This understanding is crucial for selecting and applying advanced language models in tasks requiring deep contextual understanding.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ‚ú® **XLNet Architecture:** XLNet is a decoder-only large language model, available in base (110M parameters) and large (340M parameters) sizes.\n",
    "    - **Relevance:** Knowing the architecture helps in understanding its operational differences from encoder-decoder or encoder-only models like BERT, which influences its suitability for certain NLP tasks.\n",
    "- üîÑ **Permutation Language Modeling:** XLNet's core innovation is its pre-training method that considers all permutations of the input sequence to predict words.\n",
    "    - **Relevance:** This allows the model to learn bidirectional context more comprehensively than BERT's masking or GPT's left-to-right approach, leading to a deeper understanding of text. This is highly useful in tasks requiring nuanced context interpretation like question answering or sentiment analysis.\n",
    "- **‚úçüèª Enhanced Contextual Understanding:** By sampling different permutations, XLNet learns to predict a word given the entire context, not just adjacent words or a fixed directional context.\n",
    "    - **Relevance:** This leads to superior performance in NLP tasks that benefit from long-range dependencies and a holistic understanding of the text, such as document summarization or complex text classification.\n",
    "- ‚öñÔ∏è **Overcoming Limitations of Previous Models:** XLNet addresses issues found in autoregressive models (like limited bidirectional context) and BERT (pretrain-finetune discrepancy due to [MASK] token).\n",
    "    - **Relevance:** This makes XLNet a strong candidate for tasks where these limitations might hinder performance, offering a more robust way to model language.\n",
    "- üèÜ **State-of-the-Art Performance:** XLNet demonstrated leading results on various NLP benchmarks including question answering, text classification, and language modeling.\n",
    "    - **Relevance:** Its proven success makes it a valuable tool for data scientists aiming for high accuracy in NLP projects, especially in academic research or competitive industry applications.\n",
    "- üß† **Independence Assumption:** XLNet assumes that the probability of generating a word is conditionally independent of other words in the sequence, given the *entire* context.\n",
    "    - **Relevance:** This highlights that the model prioritizes the overall context rather than strict word order for prediction, contributing to its robust contextual understanding. Useful in tasks where syntactic order is less critical than semantic relationships.\n",
    "\n",
    "# A note on the following lecture\n",
    "\n",
    "For the next lesson, please make sure that the \"emotion-labels-train.csv\", \"emotion-labels-test.csv\", and \"emotion-labels-val.csv\" files are placed in a folder called \"emotions_data\". The folder must be located in the same directory as your active Jupyter notebook. This will help ensure that the files are accessed without any errors.\n",
    "\n",
    "# Preprocessing our data\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This content outlines the initial data pre-processing steps required to fine-tune an XLNet model for an emotion classification task. It details loading text data, cleaning it by removing emojis and punctuation, balancing the dataset to handle unequal class distributions, encoding categorical labels into integers, and finally, splitting and formatting the data into a `DatasetDict` suitable for training with the Hugging Face `transformers` library. These steps are crucial for preparing a high-quality dataset, which directly impacts the performance and reliability of the fine-tuned language model.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üìö **Package Importing:** The initial step involves importing necessary Python libraries for data manipulation, text cleaning, model building, and evaluation.\n",
    "    - **Relevance:** Having the right tools (like `pandas` for dataframes, `transformers` for XLNet components, `sklearn` for utilities) is fundamental for any machine learning workflow, ensuring all required functionalities are available.\n",
    "- üßπ **Text Cleaning:** The process includes removing emojis and punctuation from the text data.\n",
    "    - **Relevance:** Text cleaning reduces noise and standardizes the input text, which can lead to better model performance by focusing on meaningful content. This is vital in NLP for data normalization.\n",
    "- ‚öñÔ∏è **Handling Class Imbalance:** The dataset is balanced by undersampling the majority classes to match the count of the least frequent class.\n",
    "    - **Relevance:** Addressing class imbalance prevents the model from being biased towards the majority class, leading to more equitable performance across different emotion labels. This is critical in classification tasks for fair and accurate predictions.\n",
    "- üî¢ **Label Encoding:** Categorical emotion labels (e.g., \"joy\", \"sadness\") are converted into numerical integers.\n",
    "    - **Relevance:** Machine learning models typically require numerical input, so label encoding transforms textual labels into a format suitable for the model's loss function and output layer.\n",
    "- üìä **Data Splitting:** The dataset is divided into training, testing, and validation sets.\n",
    "    - **Relevance:** This standard practice allows for training the model, evaluating its performance on unseen data, and tuning hyperparameters using the validation set, ensuring a robust evaluation of the model's generalization capabilities.\n",
    "- üìñ **Dataset Formatting:** The processed data is structured into `pandas` DataFrames and then converted into a Hugging Face `DatasetDict`.\n",
    "    - **Relevance:** The `DatasetDict` is the standard input format for training models using the Hugging Face `Trainer` API, streamlining the fine-tuning process. This promotes interoperability within the Hugging Face ecosystem.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Package Importing**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It's the foundational step that makes all subsequent operations possible by providing access to specialized functions and classes for each part of the workflow.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - In any data science project, whether it's NLP, computer vision, or tabular data analysis, importing the correct libraries (e.g., `pandas`, `numpy`, `scikit-learn`, `tensorflow`/`pytorch`, `transformers`) is always the starting point.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Python programming, software dependencies, library management, modular programming.\n",
    "- **Text Cleaning**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Raw text data is often noisy (containing irrelevant characters, HTML tags, special symbols, etc.). Cleaning ensures the model trains on meaningful information, improving efficiency and accuracy.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential for preparing text from social media (emojis, slang), web scraping (HTML tags), or user-generated content (typos, punctuation variations) for analysis in sentiment analysis, topic modeling, or chatbot development.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Regular expressions (regex), tokenization, stemming, lemmatization, stop-word removal, data normalization.\n",
    "- **Handling Class Imbalance**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - If not addressed, models trained on imbalanced datasets tend to perform well on the majority class but poorly on minority classes, leading to misleading accuracy metrics and poor real-world utility.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Crucial in fraud detection (fraudulent transactions are rare), medical diagnosis (rare diseases), and defect detection in manufacturing, where minority classes are often of high importance. The described method is undersampling.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Undersampling, oversampling (e.g., SMOTE), weighted loss functions, stratified sampling, F1-score, precision, recall, confusion matrix.\n",
    "- **Label Encoding**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Models perform mathematical operations, so categorical data needs to be represented numerically. `LabelEncoder` assigns a unique integer to each class.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used in any classification task where target labels are categorical, such as document categorization (e.g., sports, politics, tech), spam detection (spam/not-spam), or object recognition (dog, cat, car).\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - One-hot encoding, ordinal encoding, feature engineering, target variable transformation.\n",
    "- **Data Splitting**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It's essential for evaluating a model's ability to generalize to new, unseen data. Training and testing on the same data would lead to overfitting and an overly optimistic performance estimate.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - A universal practice in machine learning model development across all domains to ensure the model is learning patterns, not just memorizing the training data.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Overfitting, underfitting, cross-validation, hold-out validation, model evaluation metrics.\n",
    "- **Dataset Formatting**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Specific libraries and frameworks (like Hugging Face `transformers`) often expect data in a particular structure. Adhering to this format simplifies the training pipeline.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - When using specialized ML libraries (e.g., TensorFlow's `tf.data.Dataset`, PyTorch's `DataLoader`, Hugging Face `datasets`), data needs to be prepared in their expected formats for efficient processing and compatibility with their training loops and utilities.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Data pipelines, API integration, data structures (like dictionaries and dataframes), library-specific data objects.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "The text describes the following operations and implies code snippets:\n",
    "\n",
    "**1. Importing Packages:**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import re\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datasets \n",
    "import evaluate\n",
    "import random\n",
    "```\n",
    "\n",
    "**2. Loading Data (Conceptual):**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "data_train = pd.read_csv('./emotions_data/emotion-labels-train.csv') \n",
    "data_test = pd.read_csv('./emotions_data/emotion-labels-test.csv')\n",
    "data_val = pd.read_csv('./emotions_data/emotion-labels-val.csv')\n",
    "# data should be saved in a folder called 'emotions' which is saved in the same place as your notebook\n",
    "```\n",
    "\n",
    "**3. Concatenating DataFrames:**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "data = pd.concat([data_train, data_test, data_val], ignore_index=True\n",
    "```\n",
    "\n",
    "**4. Text Cleaning (Emojis and Punctuation):**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "# Assuming 'cleaner' is a function from a library like 'clean-text'\n",
    "data['text_clean'] = data['text'].apply(lambda x: clean(x, no_emoji=True))df['text_clean'] = df['text'].apply(lambda x: cleaner(x, no_emoji=True)) # Conceptual for emoji removal\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: re.sub('@[^\\s]+', '', x)) # Conceptual for punctuation\n",
    "data.head(20)\n",
    "\n",
    "```\n",
    "\n",
    "*(The audio mentions `cleaner` function for emojis and `re.sub` for punctuation, applying it to update a 'text_clean' column.)*\n",
    "\n",
    "**5. Handling Class Imbalance (Undersampling Logic):**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "g = data.groupby('label')\n",
    "data = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))\n",
    "data['label'].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "```\n",
    "\n",
    "*(The audio describes grouping by label, finding the minimum count, and sampling to that minimum.)*\n",
    "\n",
    "**6. Label Encoding:**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "data['label_int'] = LabelEncoder().fit_transform(data['label']\n",
    "NUM_LABELS = 4\n",
    "```\n",
    "\n",
    "**7. Data Splitting:**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "train_split, test_split = train_test_split(data, train_size = 0.8)\n",
    "train_split, val_split = train_test_split(train_split, train_size = 0.9)\n",
    "\n",
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))\n",
    "```\n",
    "\n",
    "**8. Creating DataFrames and `DatasetDict`:**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "train_df = pd.DataFrame({\n",
    "    \"label\": train_split.label_int.values,\n",
    "    \"text\": train_split.text_clean.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"label\": test_split.label_int.values,\n",
    "    \"text\": test_split.text_clean.values\n",
    "})\n",
    "\n",
    "train_df = datasets.Dataset.from_dict(train_df)\n",
    "test_df = datasets.Dataset.from_dict(test_df)\n",
    "\n",
    "dataset_dict = datasets.DatasetDict({\"train\":train_df, \"test\":test_df})\n",
    "dataset_dict\n",
    "\n",
    "```\n",
    "\n",
    "*(The audio explicitly mentions creating `train_df` and `test_df` from selected columns, then converting to `datasets.Dataset.from_dict` or `from_pandas` and finally into a `DatasetDict`.)*\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - The described pre-processing pipeline (cleaning, balancing, encoding, splitting, formatting) is a standard workflow applicable to most supervised learning tasks involving text data, not just emotion classification with XLNet. I can adapt these steps for sentiment analysis, topic modeling, or any text classification project.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Before teaching a computer to understand emotions in text, we first need to clean up the text messages, make sure we have a fair amount of examples for each emotion, turn emotion words into numbers, and organize everything neatly for the computer to learn.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This entire pre-processing sequence is highly relevant for any Natural Language Processing (NLP) project focused on text classification, such as sentiment analysis of customer reviews, topic categorization of news articles, spam detection in emails, or intent recognition in chatbot interactions, especially when using libraries like Hugging Face Transformers.\n",
    "\n",
    "# XLNet Embeddings\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This content describes the process of creating text embeddings for an XLNet model, a crucial step before fine-tuning. It involves loading a pre-trained XLNet tokenizer, defining a tokenization function to convert text into numerical IDs with padding and truncation, and then applying this function to the dataset. The resulting tokenized data includes input IDs, token type IDs, and attention masks, which are essential for the model to understand and process the text sequences effectively.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- tokenizer **Tokenizer Initialization:** The first step is loading the `XLNetTokenizer` from a pre-trained model, specifically `\"xlnet-base-cased\"`.\n",
    "    - **Relevance:** Using the correct tokenizer associated with the pre-trained model ensures that the text is converted into a format (vocabulary and token IDs) that the model understands, which is vital for effective transfer learning.\n",
    "- ‚úÇÔ∏è **Defining the Tokenization Function:** A function is created to process text examples, applying padding to `max_length` (128 in this case) and truncating longer sequences.\n",
    "    - **Relevance:** This standardizes the length of all input sequences, which is a requirement for batch processing in neural networks. Truncation prevents excessively long sequences from consuming too much memory or computation.\n",
    "- üó∫Ô∏è **Applying Tokenization to Dataset:** The tokenization function is applied to the entire dataset dictionary efficiently using the `.map()` method with `batched=True`.\n",
    "    - **Relevance:** Batch processing speeds up the tokenization of large datasets. The `.map()` function is a convenient way to apply transformations in the Hugging Face `datasets` library.\n",
    "- üÜî **Understanding `input_ids`:** These are the numerical representations of the text tokens, including special tokens and padding tokens (e.g., ID 5 for `<pad>`).\n",
    "    - **Relevance:** Models operate on numbers, not raw text. `input_ids` are the primary input the model uses to learn from the text. Padding ensures uniform sequence length.\n",
    "- üé≠ **Understanding `token_type_ids`:** For XLNet, these IDs help distinguish different parts of the input, such as the actual sentence (ID 0), special tokens (ID 2), and padding tokens (ID 3).\n",
    "    - **Relevance:** These IDs provide additional context to the model about the nature of each token, which can be used by the model architecture to process segments differently.\n",
    "- üëÅÔ∏è **Understanding `attention_mask`:** This binary mask (0s and 1s) indicates which tokens the model should pay attention to (1s for real tokens) and which it should ignore (0s for padding tokens).\n",
    "    - **Relevance:** The attention mechanism can focus on relevant parts of the input and ignore padding, leading to more efficient and accurate processing.\n",
    "- ü§è **Subsetting Data (Optional):** For demonstration or quicker training iterations, a smaller subset of the tokenized dataset is created.\n",
    "    - **Relevance:** Working with smaller datasets during development and debugging can save significant time and computational resources.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "From the provided Jupyter notebook \"Text classification with XLNET.ipynb\":\n",
    "\n",
    "**1. Load Tokenizer:**\n",
    "\n",
    "```python\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "```\n",
    "\n",
    "**2. Define Tokenize Function:**\n",
    "\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "```\n",
    "\n",
    "**3. Apply Tokenize Function to Dataset:**\n",
    "\n",
    "```python\n",
    "# Assuming 'dataset_dict' is a Hugging Face DatasetDict\n",
    "# (from previous preprocessing steps in the notebook)\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "```\n",
    "\n",
    "*(The notebook cell `[21]` shows this exact code.)*\n",
    "\n",
    "**4. Inspecting Tokenized Output (Conceptual examples based on text description):**\n",
    "\n",
    "```python\n",
    "# Print a sample text\n",
    "print(tokenized_datasets['train']['text'][0])\n",
    "\n",
    "# Print its input_ids\n",
    "print(tokenized_datasets['train']['input_ids'][0])\n",
    "\n",
    "# Decode a specific ID (e.g., the padding token ID 5 for XLNet)\n",
    "# print(tokenizer.decode(5)) # Notebook cell [25] shows this for ID 5\n",
    "\n",
    "# Print token_type_ids for the sample\n",
    "print(tokenized_datasets['train']['token_type_ids'][0])\n",
    "\n",
    "# Print attention_mask for the sample\n",
    "print(tokenized_datasets['train']['attention_mask'][0])\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cells `[23]`, `[24]`, `[26]`, `[27]` demonstrate these print statements.)*\n",
    "\n",
    "**5. Create Smaller Datasets for Demonstration:**\n",
    "\n",
    "```python\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[28]` shows this.)*\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - When working with any Transformer-based model for NLP tasks, these tokenization steps (loading the correct tokenizer, defining a function with padding/truncation, mapping it to data, and understanding `input_ids`, `attention_mask`, `token_type_ids`) are fundamental. I can use this as a template for preparing text data for various models and tasks.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Tokenization is like translating human language into a secret code of numbers that a computer model can understand, making sure every message is the same length by adding blanks if needed, and telling the model which parts are real words and which are just blanks.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This tokenization process is relevant to virtually all Natural Language Processing (NLP) projects that utilize pre-trained Transformer models, including text classification, sentiment analysis, question answering, machine translation, text generation, and summarization, across any domain (e.g., healthcare, finance, customer service).\n",
    "\n",
    "# Fine tuning XLNet\n",
    "\n",
    "### Summary\n",
    "\n",
    "This content details the process of fine-tuning an XLNet model for a sequence classification task (emotion detection) using the Hugging Face `transformers` library. It covers initializing the `XLNetForSequenceClassification` model, setting up evaluation metrics, defining a function to compute these metrics during training, configuring `TrainingArguments`, and finally, utilizing the `Trainer` API to conduct the training and evaluation. This structured approach simplifies the fine-tuning workflow, enabling users to adapt pre-trained models to their specific datasets and tasks efficiently.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üöÄ **Model Initialization:** The `XLNetForSequenceClassification` model is loaded from `\"xlnet-base-cased\"`, specifying the number of labels and an `id2label` mapping.\n",
    "    - **Relevance:** This step correctly configures the pre-trained model's classification head for the specific downstream task (e.g., 4 emotion labels), ensuring the output layer matches the number of classes.\n",
    "- üìä **Metric Definition:** An evaluation metric (accuracy in this case) is loaded using the `evaluate.load()` function.\n",
    "    - **Relevance:** Defining a clear metric allows for objective assessment of the model's performance during and after training, guiding model improvements and selection.\n",
    "- üíª **`compute_metrics` Function:** A custom function is defined to calculate predictions from logits and compute the chosen metric using the true labels.\n",
    "    - **Relevance:** This function is essential for the `Trainer` to evaluate the model on the validation set at specified intervals, providing insights into learning progress.\n",
    "- ‚öôÔ∏è **`TrainingArguments` Configuration:** Training hyperparameters like output directory, evaluation strategy (per epoch), and number of epochs are set.\n",
    "    - **Relevance:** These arguments control various aspects of the training loop, including how often to evaluate, how long to train, and where to save model artifacts, allowing for reproducible and customized training runs.\n",
    "- üë®‚Äçüè´ **`Trainer` API Setup:** The `Trainer` object is initialized with the model, training arguments, training dataset, evaluation dataset, and the `compute_metrics` function.\n",
    "    - **Relevance:** The Hugging Face `Trainer` API abstracts away the manual training loop, simplifying the process of fine-tuning Transformer models and integrating features like logging, evaluation, and checkpointing.\n",
    "- üöÇ **Model Training:** The fine-tuning process is initiated by calling `trainer.train()`.\n",
    "    - **Relevance:** This command starts the actual training process, where the model learns to adapt its weights based on the custom dataset and task.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "From the provided Jupyter notebook \"Text classification with XLNET.ipynb\":\n",
    "\n",
    "**1. Initialize Model:**\n",
    "\n",
    "```python\n",
    "from transformers import XLNetForSequenceClassification\n",
    "# Assuming NUM_LABELS was defined earlier, e.g., NUM_LABELS = 4\n",
    "# And an id2label mapping exists, e.g., id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'}\n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label={0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'} # Example mapping\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[29]` shows this, where `NUM_LABELS` is 4, and the `id2label` map is directly provided as `{0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'}`)*\n",
    "\n",
    "**2. Load Evaluation Metric:**\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[30]` shows this.)*\n",
    "\n",
    "**3. Define `compute_metrics` Function:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[31]` shows this.)*\n",
    "\n",
    "**4. Specify Training Arguments:**\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",       # Directory to save results\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "    num_train_epochs=3               # Number of training epochs\n",
    "    # Other arguments can be added here\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[32]` shows this.)*\n",
    "\n",
    "**5. Set up Trainer:**\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "# Assuming 'small_train_dataset' and 'small_eval_dataset' are prepared tokenized datasets\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,    # Or your full tokenized train_dataset\n",
    "    eval_dataset=small_eval_dataset,      # Or your full tokenized eval_dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[33]` shows this, using `small_train_dataset` and `small_eval_dataset`.)*\n",
    "\n",
    "**6. Train the Model:**\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[34]` shows this.)*\n",
    "\n",
    "**7. Evaluate Model (Post-training, example):**\n",
    "\n",
    "```python\n",
    "# This is how you would evaluate after training, or if the trainer didn't do it.\n",
    "# The notebook shows trainer.evaluate() in cell [35]\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - I can use the Hugging Face `Trainer` API and these steps as a blueprint for fine-tuning various Transformer models (not just XLNet) on different text classification datasets by appropriately setting the model name, number of labels, evaluation metric, and training arguments.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Fine-tuning a smart language model is like taking a student who knows a lot generally (pre-trained model) and giving them specific lessons (your data and settings) so they become an expert in a particular subject (like recognizing emotions in text).\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This fine-tuning process is highly relevant for projects requiring custom text classification solutions where general pre-trained models need to be adapted for specific nuances, such as industry-specific sentiment analysis (e.g., financial news sentiment), medical text classification (e.g., identifying patient conditions from notes), or customer support ticket categorization.\n",
    "\n",
    "# Evaluating our model\n",
    "\n",
    "### Summary\n",
    "\n",
    "This content describes the final steps after fine-tuning an XLNet model: evaluating its performance using `trainer.evaluate()`, saving the custom model with `model.save_pretrained()`, and then loading it back for inference. It demonstrates how to use the Hugging Face `pipeline` API for easy text classification predictions with the fine-tuned model, showcasing how to get classification scores for all labels on new text samples. These steps are crucial for operationalizing a fine-tuned model and applying it to real-world data.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üìâ **Model Evaluation:** The `trainer.evaluate()` method is used to assess the fine-tuned model's performance on the evaluation dataset, providing metrics like loss and accuracy.\n",
    "    - **Relevance:** This formal evaluation step is critical to understand how well the model has learned and generalized to unseen data, confirming its readiness for deployment.\n",
    "- üíæ **Saving the Fine-tuned Model:** The customized model is saved to a specified directory using `model.save_pretrained()`.\n",
    "    - **Relevance:** Saving the model allows for its reuse without retraining, sharing with others, or deploying it in production environments. This preserves the learned weights and configuration.\n",
    "- üîÑ **Loading the Fine-tuned Model:** The saved model is loaded back for use with `XLNetForSequenceClassification.from_pretrained()`, pointing to the directory where it was saved.\n",
    "    - **Relevance:** This demonstrates how to access and utilize a previously fine-tuned model, which is essential for making predictions on new data or continuing training later.\n",
    "- ‚öôÔ∏è **Creating an Inference Pipeline:** The Hugging Face `pipeline()` function is used to create a high-level text classification interface with the fine-tuned model and its tokenizer.\n",
    "    - **Relevance:** Pipelines simplify the process of making predictions by handling the pre-processing (tokenization) and post-processing (interpreting model outputs) steps automatically, making inference straightforward.\n",
    "- üéØ **Making Predictions:** The created pipeline is used to classify new text samples, with `top_k=None` to retrieve prediction scores for all trained labels.\n",
    "    - **Relevance:** This shows the practical application of the fine-tuned model for its intended task (emotion classification),\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "From the provided Jupyter notebook \"Text classification with XLNET.ipynb\":\n",
    "\n",
    "**1. Evaluate Model:**\n",
    "\n",
    "```python\n",
    "# Assuming 'trainer' is an initialized and trained Hugging Face Trainer object\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[35]` shows `trainer.evaluate()`.)*\n",
    "\n",
    "**2. Save Model:**\n",
    "\n",
    "```python\n",
    "# Assuming 'model' is the fine-tuned Hugging Face model object\n",
    "model.save_pretrained(\"fine_tuned_model\") # \"fine_tuned_model\" is the directory name\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[36]` shows this.)*\n",
    "\n",
    "**3. Load Fine-tuned Model:**\n",
    "\n",
    "```python\n",
    "from transformers import XLNetForSequenceClassification\n",
    "\n",
    "fine_tuned_model = XLNetForSequenceClassification.from_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[37]` shows this.)*\n",
    "\n",
    "**4. Create Inference Pipeline:**\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "# Assuming 'tokenizer' is the XLNetTokenizer loaded earlier\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[38]` shows this.)*\n",
    "\n",
    "**5. Make Predictions:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "# Assuming 'val_split' is your validation data (e.g., a pandas Series or list of texts)\n",
    "# And 'dataset_dict' from notebook context for val_split access\n",
    "# The notebook uses `val_split['text_clean'][rand_int]` which implies `val_split` is like a dataframe/dict.\n",
    "# If `val_split` was from `tokenized_datasets['test']` and we wanted original text:\n",
    "# For demonstration, let's assume `val_texts` is a list of texts from the validation set.\n",
    "# The notebook uses: val_split = train_test_split(train_split, train_size = 0.9)[1]\n",
    "# and train_split was from train_test_split(data, train_size = 0.8)[0]\n",
    "# So val_split is a pandas DataFrame containing 'text_clean'\n",
    "\n",
    "# rand_int = random.randint(0, len(val_split) - 1) # Corrected for 0-based indexing\n",
    "# sample_text = val_split['text_clean'].iloc[rand_int] # If val_split is a DataFrame\n",
    "# print(sample_text)\n",
    "# answer = clf(sample_text, top_k=None)\n",
    "# print(answer)\n",
    "\n",
    "# The exact code from notebook cell [39]:\n",
    "rand_int = random.randint(0, len(val_split)) # Note: potential off-by-one if len(val_split) is used directly as upper bound for indexing\n",
    "# A safer way if val_split is indexable from 0 to len-1:\n",
    "# rand_idx = random.randint(0, len(val_split) -1)\n",
    "# text_to_predict = val_split['text_clean'].iloc[rand_idx] # if val_split is a DataFrame\n",
    "# If val_split is a Hugging Face dataset: text_to_predict = val_split[rand_idx]['text_clean']\n",
    "# For the notebook's direct usage (assuming val_split DataFrame indices are continuous from 0):\n",
    "text_to_predict = val_split['text_clean'].values[rand_int] # Accessing underlying numpy array\n",
    "print(text_to_predict)\n",
    "answer = clf(text_to_predict, top_k=None)\n",
    "print(answer)\n",
    "\n",
    "```\n",
    "\n",
    "*(Notebook cell `[39]` shows the logic for random sampling and prediction. Note that the exact structure of `val_split` (DataFrame, Dataset) and how `rand_int` is used for indexing can be nuanced. The notebook implies `val_split` is a DataFrame and accesses its `text_clean` column. The `val_split` in the notebook is a DataFrame obtained from `train_test_split`.)*\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - After training any model, I can use these steps (evaluate, save, load, and create a pipeline for inference) to systematically assess its quality, persist it for future use, and easily make predictions on new data, which is applicable across various machine learning projects.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Once you've taught your computer model a new skill, you check its report card (evaluate), save its brain (save model), and then set up an easy way (pipeline) for it to use that skill to give you answers on new questions (predict).\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - These steps are relevant for any project where a machine learning model is developed and intended for practical use, such as deploying a custom sentiment analyzer for social media monitoring, an emotion detector for customer feedback analysis, or any specialized text classifier in fields like finance, healthcare, or legal tech."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
