{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f3bd1",
   "metadata": {},
   "source": [
    "# Introduction to the course\n",
    "\n",
    "### Summary\n",
    "\n",
    "This introductory course will teach you about large language models (LLMs), including their functionality and application in projects. You'll learn about the Transformer architecture, experiment with GPT models, integrate custom data using Langchain, and utilize the Hugging Face package. The course culminates in practical lessons with various LLM examples.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ—£ï¸ Introduction to large language models and their capabilities.\n",
    "- ğŸ§  Deep dive into the Transformer architecture that powers LLMs.\n",
    "- ğŸ¤– Hands-on experience with GPT models.\n",
    "- ğŸ”— Learning to integrate custom data with LLMs using Langchain.\n",
    "- ğŸ¤— Introduction to the Hugging Face Python package for LLM interaction.\n",
    "- ğŸ§ª Practical lessons experimenting with different types of language models.\n",
    "- ğŸ’¼ Building a portfolio with diverse LLM application examples.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸ The transcript mentions the \"Hugging Face package\" as a key Python tool for working with large language models. While no specific code is provided, this refers to the `transformers` library in Python, which allows users to easily access and utilize various pre-trained language models.\n",
    "- ğŸ”— The transcript also mentions \"Lange Chain,\" a framework for working with large language models and integrating custom data. Although no direct code example is given, Langchain provides Python libraries that enable the connection of LLMs with external data sources and tools.\n",
    "\n",
    "# Course materials and notebooks\n",
    "\n",
    "https://github.com/l-newbould/intro-to-llms-365\n",
    "\n",
    "# What are LLMs?\n",
    "\n",
    "### Summary\n",
    "\n",
    "Large language models (LLMs) have gained significant attention, with OpenAI's ChatGPT being a prominent example known for its diverse capabilities like writing and coding. LLMs are being used in various applications, including real-time language translation for international events and disaster relief, as well as aiding in healthcare. These models represent a major advancement in AI and NLP, leveraging deep learning and Transformer architecture to understand and generate human language with unprecedented complexity. Key characteristics of LLMs include their large size, general-purpose nature, and ability to be pre-trained and fine-tuned.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ“° LLMs, exemplified by ChatGPT, have become a significant topic in global news due to their versatile abilities.\n",
    "- ğŸŒ These models are being utilized for real-time language translation in international settings and critical information dissemination during disaster relief.\n",
    "- âš•ï¸ LLMs are also finding applications in healthcare, assisting with medical research and diagnosis.\n",
    "- ğŸ§  At their core, LLMs utilize deep learning, inspired by the human brain's neural networks.\n",
    "- âš™ï¸ The Transformer architecture is a key innovation enabling LLMs to learn the intricacies of language effectively.\n",
    "- ğŸ“ LLMs are characterized by their substantial size, contributing to their remarkable performance.\n",
    "- ğŸ› ï¸ Their general-purpose nature and the capacity for pre-training and fine-tuning further distinguish LLMs.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸ“ The transcript mentions ChatGPT's ability to write code, implying that users can provide prompts in natural language, and the LLM can generate code snippets in various programming languages. However, no specific code examples are provided in this excerpt.\n",
    "- ğŸ—£ï¸ The application of LLMs in real-time language translation suggests the use of these models to process text or speech in one language and output its equivalent in another. While the transcript highlights this capability, it does not include specific code or API calls that would be used to implement such a feature.\n",
    "\n",
    "# How large is an LLM?\n",
    "\n",
    "### Summary\n",
    "\n",
    "Large language models (LLMs) are characterized by their size, which is measured by the number of parameters they possess. These parameters are like tiny pieces of information that enable the model to understand and generate language. LLMs have millions to trillions of parameters, with models like BERT having 345 million and GPT-4 reaching 1.7 trillion. Additionally, LLMs are trained on vast amounts of text data from the internet, including books, websites, articles, social media, and more, allowing them to learn the patterns, grammar, and vocabulary of human language on a massive scale.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ”¢ The size of an LLM is determined by its number of parameters, which are fundamental units of information for language processing.\n",
    "- ğŸ§  A larger number of parameters generally equates to a better ability to understand and work with language.\n",
    "- ğŸŒ LLMs are trained on enormous datasets of text from diverse sources across the internet.\n",
    "- ğŸ“š This training data includes books covering a wide range of subjects.\n",
    "- ğŸ“° Websites, including news articles and blogs, contribute to the linguistic knowledge of LLMs.\n",
    "- ğŸ’¬ Social media posts and online chats help LLMs understand everyday communication styles.\n",
    "- ğŸ“– Encyclopedic sources like Wikipedia provide factual and historical context for LLMs.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸ“Š The transcript mentions the parameter sizes of specific LLMs: \"Bert, a large language model developed by Google, has 345 million parameters and GPT four gets even bigger at 1.7 trillion parameters.\" These figures illustrate the scale of these models but do not represent executable code.\n",
    "- ğŸ•¸ï¸ The description of LLMs being trained on \"massive amounts of text data from the internet\" implies the use of data scraping and processing techniques. However, the transcript does not provide specific code for how this data ingestion is performed.\n",
    "\n",
    "# General purpose models\n",
    "\n",
    "### Summary\n",
    "\n",
    "The second key characteristic of large language models (LLMs) is their general-purpose nature. This means they are trained on a diverse range of internet text data, enabling them to understand and generate human language in a versatile way across various tasks. Unlike models trained for specific tasks like classification, LLMs are initially pre-trained to acquire a broad understanding of knowledge and language. This general understanding then allows for fine-tuning the LLM for more specific applications or industries.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸŒ LLMs are trained on a wide array of text data from the internet, making them versatile language tools.\n",
    "- ğŸ› ï¸ Their general-purpose design allows them to assist with many different jobs involving words and communication.\n",
    "- ğŸ¯ Unlike task-specific models, LLMs are first pre-trained for a broad understanding of language and knowledge.\n",
    "- âš™ï¸ This pre-training enables them to solve general-purpose problems effectively.\n",
    "- ğŸ”¬ Subsequently, LLMs can be fine-tuned for specific tasks or to cater to particular industries.\n",
    "- ğŸ’¡ The initial goal is to equip the model with a comprehensive grasp of how language functions.\n",
    "- ğŸš€ This general understanding forms a strong foundation for later specialization through fine-tuning.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸ“š The transcript mentions training a model for specific tasks like \"classification or clustering,\" which are common machine learning applications. In Python, libraries like Scikit-learn (`sklearn`) provide tools for these tasks. For example, a classification task might involve code like:\n",
    "Python\n",
    "    \n",
    "    ```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    # Sample data (replace with your actual data)\n",
    "    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "    y = [0, 0, 1, 1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- âš™ï¸ The process of \"pre-training\" and \"fine-tuning\" LLMs is a more complex process often done with frameworks like TensorFlow or PyTorch, especially using libraries like Hugging Face's Transformers. While no specific code is given in the transcript, fine-tuning a pre-trained model might involve loading a pre-trained model and then training it on a task-specific dataset:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "```\n",
    "\n",
    "# Example: Fine-tuning a pre-trained model for sentiment analysis\n",
    "\n",
    "```python\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "```\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "\n",
    "```python\n",
    "train_texts = [\"This movie is great\", \"I did not like it\"]\n",
    "train_labels = [1, 0]\n",
    "test_texts = [\"Excellent film\", \"Terrible acting\"]\n",
    "test_labels = [1, 0]\n",
    "```\n",
    "\n",
    "```python\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "\tdef **init**(self, encodings, labels):\n",
    "\t\tself.encodings = encodings\n",
    "\t\tself.labels = labels\n",
    "```\n",
    "\n",
    "```python\n",
    "  def __getitem__(self, idx):\n",
    "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "      item['labels'] = torch.tensor(self.labels[idx])\n",
    "      return item\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.labels)\n",
    "```\n",
    "\n",
    "```python\n",
    "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "test_dataset = SimpleDataset(test_encodings, test_labels)\n",
    "```\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "output_dir='./results',\n",
    "num_train_epochs=1,\n",
    "per_device_train_batch_size=16,\n",
    "per_device_eval_batch_size=64,\n",
    "warmup_steps=500,\n",
    "weight_decay=0.01,\n",
    "logging_dir='./logs',\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=test_dataset,\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "# Pre-training and fine tuning\n",
    "\n",
    "### Summary\n",
    "\n",
    "Pre-training and fine-tuning are key processes in developing large language models (LLMs). Pre-training involves exposing the model to vast amounts of internet text data, allowing it to learn fundamental aspects of language such as grammar, vocabulary, and common sense by predicting the next words in sentences. This phase is analogous to the broad education received in early schooling. Fine-tuning then involves training the pre-trained model on smaller, specialized datasets to excel in specific tasks or industries like medicine or customer service. This is similar to focusing on specific subjects for a chosen career, building upon the foundational knowledge. Notably, due to their extensive pre-training, LLMs can sometimes perform well in \"few-shot\" or \"zero-shot\" scenarios with minimal or no additional task-specific data.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ“š Pre-training equips LLMs with a broad understanding of language through exposure to massive datasets.\n",
    "- ğŸ§  During pre-training, the model learns language basics by predicting subsequent words in text.\n",
    "- ğŸ“ This initial phase is comparable to acquiring general knowledge in early education.\n",
    "- ğŸ”¬ Fine-tuning customizes LLMs for specific tasks or industries using smaller, focused datasets.\n",
    "- ğŸ¯ This specialization allows the model to become highly proficient in particular applications.\n",
    "- âœ¨ LLMs can sometimes perform well in \"few-shot\" or \"zero-shot\" settings due to their extensive pre-training.\n",
    "- ğŸš€ This capability enables using LLMs effectively even without significant task-specific training data.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸŒ The pre-training phase involves processing large amounts of text data. While the transcript doesn't provide specific code, this often involves techniques for data loading, cleaning, and feeding it into the LLM architecture. Frameworks like TensorFlow and PyTorch, along with libraries like Hugging Face's Transformers, are commonly used for this. An abstract example of loading text data might look like:\n",
    "Python\n",
    "    \n",
    "    ```python\n",
    "    # Abstract example using a hypothetical data loading function\n",
    "    def load_large_text_dataset(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    text_data = load_large_text_dataset('path/to/large_text_file.txt')\n",
    "    # Further processing and feeding to the model would follow\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- ğŸ¯ The fine-tuning phase uses task-specific datasets. For example, fine-tuning for sentiment analysis might involve loading a dataset of text and their corresponding sentiment labels:\n",
    "    \n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Sample sentiment analysis dataset\n",
    "    data = {'text': [\"This is great!\", \"I hated it.\", \"It was okay.\"],\n",
    "            'sentiment': [1, 0, 2]} # 1: positive, 0: negative, 2: neutral\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_texts = train_df['text'].tolist()\n",
    "    train_labels = train_df['sentiment'].tolist()\n",
    "    test_texts = test_df['text'].tolist()\n",
    "    test_labels = test_df['sentiment'].tolist()\n",
    "    \n",
    "    # This data would then be tokenized and used to fine-tune a pre-trained model\n",
    "    # using a framework like Hugging Face Transformers.\n",
    "    ```\n",
    "    \n",
    "\n",
    "# What can LLMs be used for?\n",
    "\n",
    "### Summary\n",
    "\n",
    "Large language models (LLMs) are highly versatile due to their training on extensive datasets, enabling them to excel in numerous tasks. These include content creation like articles and stories, language translation for real-time communication, answering questions across various topics, and developing chatbots for customer service and information retrieval. LLMs can also perform text analysis to determine sentiment, generate concise summaries of long texts, and power recommendation systems for various types of content. Furthermore, they assist programmers with code generation and debugging, aid healthcare professionals with diagnosis and research, support legal teams in document review, and enable personalized marketing campaigns.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- âœï¸ LLMs are adept at content creation, producing human-like articles, blog posts, and creative writing.\n",
    "- ğŸŒ They facilitate language translation, often powering real-time translation apps for travelers and multilingual communication.\n",
    "- â“ LLMs can answer questions on a wide array of subjects, providing information based on their broad knowledge.\n",
    "- ğŸ’¬ They can be used to build chatbots and virtual assistants for customer interaction and task assistance.\n",
    "- ğŸ“Š LLMs can analyze text to determine its sentiment, which is valuable for businesses and social media monitoring.\n",
    "- ğŸ“‘ They can generate concise summaries of lengthy documents, aiding in quick comprehension of key information.\n",
    "- ğŸ’¡ LLMs power recommendation systems, suggesting content like movies, books, and products based on user preferences.\n",
    "- ğŸ’» For programmers, they can generate code snippets, assist with debugging, and explain complex concepts.\n",
    "- âš•ï¸ In healthcare, LLMs can analyze medical records and support diagnosis.\n",
    "- âš–ï¸ In the legal field, they can review documents and case histories to extract relevant information.\n",
    "- ğŸ“ˆ For marketing, LLMs can analyze customer data to create personalized campaigns.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- ğŸ“ Content Creation: While the transcript highlights the ability of LLMs to write, generating such content typically involves interacting with an LLM API. For example, using the OpenAI API in Python might look like this:\n",
    "Python\n",
    "    \n",
    "    ```python\n",
    "    import openai\n",
    "    \n",
    "    openai.api_key = 'YOUR_API_KEY'\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=\"Write a short story about a robot who wants to be a painter.\",\n",
    "      max_tokens=150\n",
    "    )\n",
    "    \n",
    "    story = response.choices[0].text\n",
    "    print(story)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- ğŸŒ Language Translation: Similar to content creation, translation often utilizes LLM APIs. An example using a hypothetical translation API:\n",
    "Python\n",
    "    \n",
    "    ```python\n",
    "    # Hypothetical translation function\n",
    "    def translate_text(text, target_language):\n",
    "        # In a real scenario, this would involve an API call\n",
    "        if target_language == 'es':\n",
    "            return f\"TraducciÃ³n al espaÃ±ol de: {text}\"\n",
    "        else:\n",
    "            return f\"Translation to {target_language} of: {text}\"\n",
    "    \n",
    "    english_text = \"Hello, how are you?\"\n",
    "    spanish_translation = translate_text(english_text, 'es')\n",
    "    print(spanish_translation)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- ğŸ’¬ Chatbot: Building a chatbot with an LLM also involves API interaction to send user input and receive generated responses. Continuing with the OpenAI example:\n",
    "    \n",
    "    ```python\n",
    "    import openai\n",
    "    \n",
    "    openai.api_key = 'YOUR_API_KEY'\n",
    "    \n",
    "    user_input = \"What is the capital of France?\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=user_input,\n",
    "      max_tokens=50\n",
    "    )\n",
    "    \n",
    "    bot_response = response.choices[0].text.strip()\n",
    "    print(bot_response)\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
