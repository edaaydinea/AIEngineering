{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30aacce6",
   "metadata": {},
   "source": [
    "# Piping a prompt, model, and an output parser\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This lesson introduces LangChain Expression Language (LCEL) as a modern, intuitive framework for building robust and efficient data processing pipelines using \"runnable\" components. It demonstrates constructing a chain to generate a list of pet names, first by manually invoking individual components (prompt template, chat model, output parser) and then by using the concise LCEL pipe (`|`) syntax, highlighting LCEL's advantages in code clarity, modularity, and ease of debugging for real-world data science tasks.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **LCEL as the New Standard**: LangChain Expression Language (LCEL) replaces older chain mechanisms (like the specific `LLMChain` class), offering a more powerful, intuitive, and efficient way to compose language model workflows. Adopting LCEL is key for developers aiming to build maintainable and scalable AI applications with the latest LangChain capabilities.\n",
    "- **The Runnable Protocol**: LCEL components, known as \"runnables,\" implement a standard interface. This interface includes methods like `invoke` (for single, blocking calls), `stream` (for streaming responses), `batch` (for processing multiple inputs efficiently), and their asynchronous counterparts (`ainvoke`, `astream`, `abatch`). This consistency simplifies how different parts of a chain interact and makes complex data flows easier to manage.\n",
    "- **Pipe Operator (`|`) for Composition**: LCEL leverages the pipe symbol (`|`) to link runnables together, where the output of one component directly feeds into the input of the next. This syntax allows for highly readable and declarative chain definitions, significantly improving code clarity and resembling familiar shell command piping.\n",
    "- **Practical Chain Example (Pet Names Generation)**: The lesson walks through building a chain that takes a type of pet (e.g., \"dog\") as input and outputs a Python list of three suggested names. This example clearly illustrates a common pattern in LLM applications: dynamic prompt formatting, model inference, and structured output parsing.\n",
    "- **Core Components in Action**: The demonstration utilizes `ChatPromptTemplate` to dynamically structure the input passed to the language model, `ChatOpenAI` as the LLM to generate responses, and `CommaSeparatedListOutputParser` to convert the model's raw string output into a usable Python list. These components are fundamental building blocks for controlling LLM interactions and their outputs in data science projects.\n",
    "- **Explicit Invocation vs. LCEL Conciseness**: The video first shows how to achieve the desired outcome by manually invoking each component (template, model, parser) in sequence and passing results. This is contrasted with the streamlined LCEL approach (`chain = chat_template | chat_model | list_output_parser`), effectively showcasing LCEL's elegance and how it abstracts the underlying execution flow.\n",
    "- **Dynamic Prompts with Input Variables**: The `ChatPromptTemplate` incorporates placeholders (e.g., `{pet}` for the animal type and implicitly `{format_instructions}` for output guidance) that are populated at runtime when the chain is invoked. This capability is fundamental for creating adaptable and context-aware prompts that can handle various inputs in real-world applications.\n",
    "- **Importance of Output Parsers for Structured Data**: Output parsers, such as the `CommaSeparatedListOutputParser`, play a crucial role in transforming the often unstructured text output from LLMs into well-defined data formats (like lists, JSON objects, etc.). This step is critical for seamlessly integrating LLM-generated information into downstream software processes, databases, or further data analysis workflows.\n",
    "- **Enhanced Debuggability of LCEL Chains**: A significant advantage of LCEL is that each runnable component within a chain can be independently invoked and its output inspected. This granular control greatly simplifies the debugging process for complex chains, allowing developers to isolate and troubleshoot issues at any specific point in the pipeline.\n",
    "- **Streaming for Improved User Experience**: Although its full implementation is deferred to a later lesson, the `stream` method is highlighted as a means to process and display LLM outputs token by token as they are generated. This feature is particularly important for enhancing the perceived performance and responsiveness of user-facing applications, especially when dealing with longer text generations.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **The Runnable Protocol and its Significance in LCEL**\n",
    "    1. **Why is this concept important?** The Runnable protocol provides a standardized interface for all components that can be part of a LangChain Expression Language (LCEL) chain. This means that any object adhering to this protocol (e.g., prompt templates, models, output parsers, custom functions) can be seamlessly interconnected using the pipe (`|`) operator. It guarantees common invocation methods like `invoke`, `stream`, `batch`, and their async versions, making components interchangeable and predictable.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** In real-world data science projects, you often need to build complex data processing pipelines. The Runnable protocol allows you to construct these pipelines in a modular way. For instance, you could build a sentiment analysis chain by piping a data loader runnable, a text preprocessing runnable, a prompt template runnable, an LLM runnable, and an output parsing runnable. If you need to swap out the LLM or change the preprocessing logic, you can do so easily as long as the new component is also a runnable. This modularity simplifies development, testing, and maintenance of complex AI systems.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?** Understanding functional programming concepts like composition and higher-order functions can be beneficial, as LCEL's piping mechanism is analogous to function composition. Delving deeper into Python's data model and special methods (like `__or__` for the `|` operator if it were implemented that way, though LCEL uses its own internal mechanics) can provide insights. Also, exploring LangChain's various built-in runnables (retrievers, document transformers, tools) and learning how to create custom runnables will allow for more sophisticated chain construction.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "The lesson describes the following code implementation steps:\n",
    "\n",
    "1. **Setting up the Environment and Imports**:\n",
    "    \n",
    "    ```python\n",
    "    import os\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "    \n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "    \n",
    "    ```\n",
    "    \n",
    "2. **Defining Components**:\n",
    "    \n",
    "    ```python\n",
    "    # Output formatting instructions\n",
    "    list_instructions = \"Your response should be a list of comma separated values, eg: `foo, bar, baz`\"\n",
    "    \n",
    "    # Prompt Template\n",
    "    # Initial template string: \"I've recently adopted a {pet}. Could you suggest three pet names?\"\n",
    "    # Format instructions are appended to this string.\n",
    "    # Example of constructing the template:\n",
    "    prompt_string = \"I've recently adopted a {pet}. Could you suggest three pet names?\\n{format_instructions}\"\n",
    "    chat_template = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", prompt_string)\n",
    "    ])\n",
    "    # Or, more aligned with the video's description for format_instructions injection later:\n",
    "    # human_template = \"I've recently adopted a {pet}. Could you suggest three pet names?\\nFollow the sentence with a new line character for later convenience.\"\n",
    "    # chat_template_for_manual_formatting = ChatPromptTemplate.from_messages([(\"human\", human_template)])\n",
    "    # formatted_prompt_string = chat_template_for_manual_formatting.messages[0].prompt.template + \"\\n\" + list_instructions (conceptually)\n",
    "    \n",
    "    # Language Model\n",
    "    chat_model = ChatOpenAI(temperature=0.0)\n",
    "    \n",
    "    # Output Parser\n",
    "    list_output_parser = CommaSeparatedListOutputParser()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "3. **Manual Invocation (Conceptual Flow)**:\n",
    "    \n",
    "    ```python\n",
    "    # 1. Format the prompt with initial input and formatting instructions\n",
    "    # This step is slightly simplified here as the video describes adding format_instructions to the template string directly.\n",
    "    # For the purpose of LCEL, format_instructions can be passed in the invoke dictionary.\n",
    "    # formatted_prompt_value = chat_template.invoke({\n",
    "    #     \"pet\": \"dog\",\n",
    "    #     \"format_instructions\": list_instructions # If included as a variable in prompt\n",
    "    # })\n",
    "    # If list_instructions is part of the template string already:\n",
    "    formatted_prompt_value = chat_template.invoke({\"pet\": \"dog\", \"format_instructions\": list_instructions})\n",
    "    \n",
    "    # 2. Invoke the model with the formatted prompt\n",
    "    # chat_result = chat_model.invoke(formatted_prompt_value)\n",
    "    # The actual output would be an AIMessage, e.g., AIMessage(content=\"Buddy, Max, Lucy\")\n",
    "    \n",
    "    # 3. Parse the model's output\n",
    "    # parsed_output = list_output_parser.invoke(chat_result)\n",
    "    # parsed_output would be ['Buddy', 'Max', 'Lucy']\n",
    "    \n",
    "    ```\n",
    "    \n",
    "4. **LCEL Chain Construction and Invocation**:\n",
    "    \n",
    "    ```python\n",
    "    # Define the chain using the pipe operator\n",
    "    chain = chat_template | chat_model | list_output_parser\n",
    "    \n",
    "    # Invoke the chain\n",
    "    # Assuming list_instructions is baked into the chat_template string:\n",
    "    # chat_template = ChatPromptTemplate.from_template(\n",
    "    #     \"Suggest three names for a {pet}. Your response should be a list of comma separated values, eg: `foo, bar, baz`\"\n",
    "    # )\n",
    "    # result = chain.invoke({\"pet\": \"cat\"})\n",
    "    # print(result) # Expected: ['Whiskers', 'Shadow', 'Cleo'] (or similar)\n",
    "    \n",
    "    # If format_instructions is a separate variable in the prompt template:\n",
    "    # chat_template_with_format_var = ChatPromptTemplate.from_template(\n",
    "    #     \"Suggest three names for a {pet}.\\n{format_instructions}\"\n",
    "    # )\n",
    "    # chain_with_format_var = chat_template_with_format_var | chat_model | list_output_parser\n",
    "    # result = chain_with_format_var.invoke({\n",
    "    #     \"pet\": \"dog\",\n",
    "    #     \"format_instructions\": list_instructions\n",
    "    # })\n",
    "    # print(result) # Expected: ['Buddy', 'Max', 'Lucy'] (or similar)\n",
    "    \n",
    "    # The video's final approach integrates format instructions directly into the prompt string.\n",
    "    # For example, the human message template becomes:\n",
    "    # \"I've recently adopted a {pet}. Could you suggest three pet names?\\nYour response should be a list of comma separated values, eg: `foo, bar, baz`\"\n",
    "    # Then the chain is:\n",
    "    final_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"I've recently adopted a {pet}. Could you suggest three pet names?\\nYour response should be a list of comma separated values, eg: `foo, bar, baz`\")\n",
    "    ])\n",
    "    chain = final_prompt_template | chat_model | list_output_parser\n",
    "    result = chain.invoke({\"pet\": \"dog\"})\n",
    "    # print(result) # Should produce a list of three dog names\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** Which specific dataset or project could benefit from this concept of LCEL chaining?\n",
    "    - *Answer:* A project involving automated summarization of customer reviews could greatly benefit from LCEL. An LCEL chain could ingest raw review text, use a prompt template to instruct an LLM to extract key sentiments and topics, and then use an output parser to structure this information into a JSON format for easy analysis and reporting.\n",
    "2. **Teaching:** How would you explain LCEL and the pipe operator (`|`) to a junior colleague, using one concrete example?\n",
    "    - *Answer:* \"Think of LCEL as building with smart LEGOs. Each LEGO block (like a prompt, a model, or a parser) is a 'runnable' that does one job. The pipe `|` just snaps these blocks together, so the output of one block automatically becomes the input for the next, creating a clean data processing pipeline—like `Prompt | Model | OutputParser` to get structured data from a question.\"\n",
    "3. **Extension:** What related technique or area should you explore next, and why?\n",
    "    - *Answer:* Exploring LangChain's **Retrieval Augmented Generation (RAG)** techniques would be a logical next step. RAG chains often use LCEL to combine data retrieval from external sources (like vector databases) with LLM generation, which is crucial for building more knowledgeable and context-aware AI applications that can answer questions based on specific documents.\n",
    "\n",
    "# Batching\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson focuses on the `batch` method within LangChain Expression Language (LCEL), highlighting its capability to process multiple input sets in parallel for enhanced efficiency. It demonstrates constructing a chain with multiple input variables and then contrasts the slower, sequential `invoke` calls with the faster `batch` method, empirically proving `batch`'s time-saving benefits through a performance test, which is particularly relevant for scaling data processing tasks in data science.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **Efficient Multi-Input Processing with `batch`**: LCEL chains offer a `batch` method designed to handle a list of input dictionaries simultaneously, providing a more efficient alternative to sequentially calling `invoke` for each input. This is crucial for applications needing to process data in bulk, such as performing sentiment analysis on numerous customer reviews or generating product descriptions for a catalog.\n",
    "  - **Parallelism as the Key to Speed**: The `batch` method achieves its efficiency by executing the operations for each input in the list concurrently (in parallel), rather than one after another. This significantly reduces overall wall-clock time when dealing with multiple data points, a common scenario in data science and API serving.\n",
    "  - **Demonstration of `batch` with Varied Inputs**: The lesson shows `chain.batch()` taking a list of dictionaries, for example, `[{\"pet\": \"dog\", \"breed\": \"Shepard\"}, {\"pet\": \"dragon\", \"breed\": \"Nightfury\"}]`, and returning a corresponding list of AI messages. This illustrates its practical use for sending diverse, batched queries to an LLM chain in a single, efficient operation.\n",
    "  - **Time Efficiency Validated via `%%time`**: By using IPython's `%%time` magic command, the transcript demonstrates that the execution time for a `batch` call processing multiple inputs is notably less than the cumulative time of individual `invoke` calls for the same set of inputs. This provides concrete, empirical evidence of its performance advantage in real-world scenarios.\n",
    "  - **Scalability for Production Workloads**: The time-saving benefits of the `batch` method become increasingly pronounced with a larger number of inputs or when individual LLM calls are inherently time-consuming (e.g., generating lengthy or complex responses). This makes `batch` an essential feature for deploying performant LLM applications that need to handle significant traffic or large data volumes effectively.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **Parallel Execution in the `batch` Method**\n",
    "    1.  **Why is this concept important?** Parallel execution allows multiple tasks (in this case, processing different inputs through the LLM chain) to be performed concurrently rather than sequentially. For I/O-bound operations like making API calls to an LLM, where the program spends much of its time waiting for external responses, parallelism can drastically reduce the total execution time because multiple waiting periods can overlap.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** Many data science applications involve processing large numbers of items: summarizing many articles, translating multiple documents, classifying numerous customer reviews, or generating personalized content for several users. The `batch` method's parallelism allows these tasks to be completed much faster, improving system throughput and user experience. For example, a service that provides personalized recommendations could use batching to generate recommendations for multiple users simultaneously.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** To understand parallelism better, one might explore concepts like asynchronous programming (`async`/`await` in Python), multithreading, and multiprocessing. For LCEL, understanding how the underlying HTTP clients or SDKs handle concurrent requests can also be beneficial. Additionally, learning about rate limits and token usage with LLM APIs is important when sending large batches.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson describes the following code implementation steps:\n",
    "\n",
    "1.  **Setting up Environment and Imports**:\n",
    "\n",
    "    ```python\n",
    "    # Presumed magic commands for environment, e.g., for auto-reloading modules\n",
    "    # %load_ext autoreload\n",
    "    # %autoreload 2\n",
    "\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Assumed from previous context\n",
    "    ```\n",
    "\n",
    "2.  **Defining Components and Chain with Multiple Input Variables**:\n",
    "\n",
    "    ```python\n",
    "    # Prompt Template with 'pet' and 'breed' input variables\n",
    "    prompt_template_string = \"Could you suggest several training tips for a {pet} of breed {breed}?\"\n",
    "    chat_template = ChatPromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "    # Language Model\n",
    "    chat_model = ChatOpenAI(temperature=0.0) # Or just ChatOpenAI()\n",
    "\n",
    "    # Chain Definition\n",
    "    chain = chat_template | chat_model\n",
    "    ```\n",
    "\n",
    "3.  **Using `invoke` with Multiple Variables**:\n",
    "\n",
    "    ```python\n",
    "    # Single invocation with two input variables\n",
    "    # result_invoke = chain.invoke({\"pet\": \"dog\", \"breed\": \"Shepard\"})\n",
    "    # print(result_invoke.content)\n",
    "    ```\n",
    "\n",
    "4.  **Using `batch` with a List of Inputs**:\n",
    "\n",
    "    ```python\n",
    "    # Batch invocation with a list of two dictionaries\n",
    "    inputs_list = [\n",
    "        {\"pet\": \"dog\", \"breed\": \"Shepard\"},\n",
    "        {\"pet\": \"dragon\", \"breed\": \"Nightfury\"} # Fictional example from transcript\n",
    "    ]\n",
    "    # results_batch = chain.batch(inputs_list)\n",
    "    # for res in results_batch:\n",
    "    #     print(res.content)\n",
    "    ```\n",
    "\n",
    "5.  **Performance Comparison using `%%time`**:\n",
    "    The lesson describes using IPython's cell magic `%%time` to measure execution time.\n",
    "\n",
    "    *Cell 1 (Batch processing):*\n",
    "\n",
    "    ```python\n",
    "    # %%time\n",
    "    # batch_results = chain.batch(inputs_list)\n",
    "    ```\n",
    "\n",
    "    *Cell 2 (First invoke):*\n",
    "\n",
    "    ```python\n",
    "    # %%time\n",
    "    # invoke_result1 = chain.invoke(inputs_list[0])\n",
    "    ```\n",
    "\n",
    "    *Cell 3 (Second invoke):*\n",
    "\n",
    "    ```python\n",
    "    # %%time\n",
    "    # invoke_result2 = chain.invoke(inputs_list[1])\n",
    "    ```\n",
    "\n",
    "    The transcript notes that the wall time for Cell 1 (batch) is expected to be less than the sum of wall times for Cell 2 and Cell 3.\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Application:** Which specific dataset or project could benefit from the `batch` method?\n",
    "      - *Answer:* A project requiring the generation of personalized email subject lines for a large marketing campaign (e.g., 10,000 customers) would significantly benefit from the `batch` method. Instead of invoking an LLM chain 10,000 times sequentially, inputs (customer data, product focus) could be batched, drastically reducing the overall time needed to generate all subject lines.\n",
    "2.  **Teaching:** How would you explain the primary benefit of `batch` over multiple `invoke` calls to a non-technical stakeholder, using an analogy?\n",
    "      - *Answer:* \"Imagine you need to ask our expert (the LLM) 100 different questions. Using `invoke` 100 times is like queuing up for the expert, asking one question, getting an answer, then going to the back of the line to ask the next. Using `batch` is like giving the expert all 100 questions at once; they can work on answering them more efficiently, perhaps delegating parts or thinking about related questions together, so you get all your answers much faster than waiting in line 100 times.\"\n",
    "\n",
    "# Streaming\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson introduces the `stream` method in LangChain Expression Language (LCEL), which enables receiving responses from LLM chains as a sequence of chunks rather than a single consolidated output. It explains that `stream` returns a Python generator, detailing how generators work (yielding values, preserving state, single-use, memory efficiency) and then demonstrates iterating through these `AIMessageChunk` objects to display a response piece by piece, mimicking a real-time chatbot experience, which is vital for user-facing data science applications.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **Introducing the `stream` Method**: LCEL chains provide a `stream` method that, instead of returning a full response at once, yields a generator. This allows for processing the LLM's output in chunks as it's generated, essential for applications like live chatbots or processing very long documents where immediate feedback or memory efficiency is key.\n",
    "  - **Python Generators Explained**: The `stream` method returns a Python generator. Generators are iterators that produce a sequence of values using the `yield` statement, preserving their state between calls. They are memory-efficient as they generate values on-the-fly and are single-use, meaning once exhausted, they cannot be re-iterated.\n",
    "  - **Iterating Through Streamed `AIMessageChunk`s**: When `chain.stream()` is called (typically with a chat model), it yields `AIMessageChunk` objects. Each chunk contains a part of the total response, accessible via its `content` attribute. These chunks can be processed one by one using `next()` or a `for` loop.\n",
    "  - **Demonstrating Streaming Behavior**: The lesson illustrates how to use `next(generator_object)` to get individual chunks until a `StopIteration` exception is raised. It then shows a more practical approach using a `for` loop to iterate through all chunks and print their content with `end=\"\"` to display the response progressively on a single line, like a streaming chatbot.\n",
    "  - **Practical Application for User Experience**: Streaming responses significantly enhances user experience in interactive applications by providing immediate visual feedback. Users see text appearing incrementally, making the system feel more responsive, rather than waiting for the entire (potentially long) response to be generated.\n",
    "  - **Memory Efficiency with Large Outputs**: By processing data in chunks, the `stream` method avoids loading the entire LLM response into memory. This is particularly beneficial when dealing with extensive outputs or when operating in memory-constrained environments, common in some data science deployments.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson describes the following Python code snippets for using the `stream` method with an LCEL chain:\n",
    "\n",
    "1.  **Basic `stream` Call and Generator Output**:\n",
    "\n",
    "    ```python\n",
    "    # Assuming 'chain' is an already defined LCEL chain (e.g., prompt | model)\n",
    "    # response_generator = chain.stream({\"pet\": \"dog\", \"breed\": \"Shepard\"})\n",
    "    # print(response_generator) # Output shows it's a generator object\n",
    "    ```\n",
    "\n",
    "2.  **Modifying Model Parameters for Demonstration (e.g., `max_tokens`)**:\n",
    "\n",
    "    ```python\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    # chat_model = ChatOpenAI(max_tokens=5)\n",
    "    # chain = chat_template | chat_model # Re-define chain with the new model\n",
    "    ```\n",
    "\n",
    "3.  **Accessing Chunks with `next()` and `StopIteration`**:\n",
    "\n",
    "    ```python\n",
    "    # response = chain.stream({\"pet\": \"dog\", \"breed\": \"Shepard\"}) # Re-create generator\n",
    "    # try:\n",
    "    #     while True:\n",
    "    #         chunk = next(response)\n",
    "    #         print(chunk.content, end=\"\") # AIMessageChunk has a 'content' attribute\n",
    "    # except StopIteration:\n",
    "    #     print(\"\\nGenerator exhausted.\")\n",
    "    ```\n",
    "\n",
    "4.  **Iterating with a `for` Loop to Stream Content**:\n",
    "\n",
    "    ```python\n",
    "    # Revert max_tokens to a suitable value (e.g., 100) and redefine chat_model and chain\n",
    "    # response = chain.stream({\"pet\": \"dog\", \"breed\": \"Shepard\"}) # Re-create generator\n",
    "    # for chunk in response:\n",
    "    #     print(chunk.content, end=\"\")\n",
    "    # print() # For a final newline\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Application:** In what type of data science application would the `stream` method be particularly advantageous over `invoke` or `batch`?\n",
    "      - *Answer:* The `stream` method is particularly advantageous in interactive applications like AI-powered customer service chatbots or live coding assistants. Here, providing immediate, incremental feedback as the LLM generates its response significantly improves user engagement and perceived performance, unlike `invoke` (waits for full response) or `batch` (processes multiple full responses).\n",
    "2.  **Teaching:** How would you explain the concept of a \"generator\" in Python to someone who only knows basic functions that `return` a single value, using an analogy?\n",
    "      - *Answer:* \"Imagine a regular function is like a gumball machine: you put in a coin (`invoke` it), and it gives you one gumball (`return` a value) and it's done. A generator is like a Pez dispenser: each time you tilt it (`next()`), it gives you one candy (`yield` a value) but remembers its state, ready to give you the next candy until it's empty, rather than giving you all candies at once.\"\n",
    "\n",
    "# The Runnable and RunnableSequence classes\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson clarifies the underlying object model in LangChain Expression Language (LCEL), explaining that chains formed using the pipe operator are `RunnableSequence` instances. It emphasizes that all chain components (prompts, models, parsers) are fundamentally \"Runnables,\" often through class inheritance, and thus support a common interface with methods like `invoke`, `batch`, and `stream`. The session guides users to LangChain documentation to trace this inheritance and to use a reference table for the expected input/output types of various runnables, which is vital for robust chain construction and debugging.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **LCEL Chains are `RunnableSequence` Instances**: A chain created by linking components with the pipe (`|`) operator in LCEL (e.g., `prompt | model | parser`) is an instance of the `RunnableSequence` class. This class manages the sequential execution and data flow, where the output of one component becomes the input for the next.\n",
    "  - **Core Components are \"Runnables\"**: All fundamental building blocks in LCEL, such as prompt templates (`ChatPromptTemplate`), language models (`ChatOpenAI`), and output parsers, implement the \"Runnable\" interface. This interface guarantees they support standard operations like `invoke`, `batch`, and `stream`, ensuring consistent interaction patterns.\n",
    "  - **Runnable Status Through Inheritance**: While a component's immediate Python type might not be `Runnable` (e.g., `type(chat_template)` returns `ChatPromptTemplate`), they become runnables by inheriting from a base `Runnable` class, often via an intermediate class like `RunnableSerializable`. This hierarchical relationship can be traced through LangChain's official documentation.\n",
    "  - **The \"Runnable\" Interface Definition**: A \"Runnable\" in LCEL is defined as a unit of work that can be invoked (for a single input/output), batched (for multiple inputs), streamed (for chunked outputs), transformed (for modifying input/output, though not covered in detail), and composed (linked with other runnables). This consistent interface is key to LCEL's flexibility.\n",
    "  - **Chains Themselves are Composable Runnables**: Crucially, a `RunnableSequence` (an LCEL chain) is itself a `Runnable`. This powerful feature means that entire chains can be treated as single, reusable components and can be piped together to form more complex, nested, or extended data processing workflows.\n",
    "  - **Standardized Input/Output Schemas for Runnables**: Each type of runnable component adheres to a defined input and output schema (e.g., a prompt template typically accepts a dictionary and outputs a `PromptValue`; a chat model takes a `PromptValue` and outputs a `ChatMessage`). LangChain's documentation provides a reference table detailing these schemas, which is essential for ensuring components are compatibly connected within a chain.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **The Power of the Runnable Interface and Inheritance in LCEL**\n",
    "    1.  **Why is this concept important?** The `Runnable` interface provides a universal contract for all components in LCEL. By ensuring that diverse elements—from simple prompts to complex models or even entire sub-chains—all share common methods (`invoke`, `stream`, `batch`), LCEL allows them to be interchanged and connected seamlessly. Inheritance allows specialized components (like `ChatPromptTemplate`) to gain these `Runnable` capabilities without redundant code, promoting a clean and extensible architecture.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In real-world data science projects, you often build complex pipelines. The `Runnable` interface means you can construct these pipelines from various building blocks (data loaders, preprocessors, LLMs, parsers, custom tools) with confidence that they will \"plug together\" correctly using the `|` operator. If you need to swap an OpenAI model for an Anthropic model, as long as both are `Runnable`, the change is often minimal. This modularity simplifies development, testing, and maintenance of sophisticated AI systems.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Understanding object-oriented programming principles, especially polymorphism and inheritance, is beneficial. Exploring design patterns like the \"Chain of Responsibility\" or \"Pipes and Filters\" can provide a broader context for how LCEL's `RunnableSequence` operates. Delving into Python's abstract base classes (ABCs) can also offer insight into how such interfaces are defined and enforced.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson focuses on using Python's built-in `type()` function for introspection and refers to checking LangChain documentation:\n",
    "\n",
    "1.  **Checking the Type of an LCEL Chain**:\n",
    "\n",
    "    ```python\n",
    "    # Assuming 'chain' is an LCEL object, e.g., chain = prompt | model\n",
    "    # chain_type = type(chain)\n",
    "    # print(chain_type)\n",
    "    # Expected output (or similar): <class 'langchain_core.runnables.base.RunnableSequence'>\n",
    "    ```\n",
    "\n",
    "2.  **Checking the Type of a Component**:\n",
    "\n",
    "    ```python\n",
    "    # Assuming 'chat_template' is an instance of ChatPromptTemplate\n",
    "    # template_type = type(chat_template)\n",
    "    # print(template_type)\n",
    "    # Expected output: <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
    "    ```\n",
    "\n",
    "    The lesson then explains that to confirm `chat_template` is a `Runnable`, one would consult the LangChain documentation to trace its class inheritance back to the `Runnable` base class.\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Application:** How does the fact that an entire LCEL chain is also a `Runnable` facilitate the creation of modular and reusable AI systems?\n",
    "      - *Answer:* Because chains are themselves `Runnables`, a complex sub-task (e.g., a chain that performs document retrieval, summarization, and then sentiment analysis) can be developed and tested as a single unit. This \"sub-chain\" can then be easily imported and used as a single, well-defined component within multiple larger, more sophisticated AI workflows, promoting modular design, reducing code duplication, and simplifying overall system architecture.\n",
    "2.  **Troubleshooting:** A data scientist is building an LCEL chain and receives a `ValidationError` or `TypeError` when the chain runs, particularly at the junction between two components. How would referencing the input/output table for runnables (as mentioned in the lesson) help them debug this?\n",
    "      - *Answer:* By consulting the input/output table for runnables, the data scientist can systematically check the output type of the component *before* the error against the expected input type of the component *where* the error occurs. A mismatch (e.g., one component outputs a plain string, but the next expects a dictionary or a specific `ChatMessage` object) is a common source of such errors, and the table helps quickly identify this broken data contract in the chain.\n",
    "\n",
    "# Piping chains and the RunnablePassthrough class\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson teaches how to pipe multiple LangChain Expression Language (LCEL) chains together to create more complex, sequential workflows, introducing the `RunnablePassthrough` class as a crucial component for managing and reshaping data flow between these distinct chains. It demonstrates this by building two separate chains—one to list essential tools for a given profession and another to suggest learning strategies for those tools—and then combines them. The key to this combination is using `RunnablePassthrough` within a dictionary structure, which correctly formats the string output of the first chain into the dictionary input expected by the second, thereby highlighting LCEL's powerful modularity and composability for sophisticated AI applications.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **Piping Chains Together**: LCEL allows entire chains, which are `RunnableSequence` instances and thus `Runnables` themselves, to be piped together using the `|` operator. This enables the creation of longer, more complex sequences of operations by linking pre-built, independent chains.\n",
    "  - **Introducing `RunnablePassthrough`**: The `RunnablePassthrough` class acts as an identity function within LCEL. It takes an input and passes it through to the output without any modification. Its utility shines when needing to pass data along or structure it within a chain.\n",
    "  - **Illustrative Scenario: Tools and Strategy Chains**: The lesson employs a practical example involving two chains:\n",
    "    1.  `chain_tools`: Takes a `{job_title}` as input and outputs a string listing the five most important tools for that profession.\n",
    "    2.  `chain_strategy`: Takes a list of `{tools}` as input and outputs a string suggesting strategies for learning and mastering them.\n",
    "        The objective is to combine these so the output of `chain_tools` automatically feeds into `chain_strategy`.\n",
    "  - **Addressing Input/Output Mismatch**: A common challenge when combining chains is that the output format of one chain (e.g., a raw string from `chain_tools` due to a `StringOutputParser`) may not match the expected input format of the next chain (e.g., `chain_strategy` expecting a dictionary like `{\"tools\": \"string_listing_tools\"}`).\n",
    "  - **Using `RunnablePassthrough` for Input Shaping**: The solution demonstrated involves piping the output of the first chain (`chain_tools`) into a dictionary literal where `RunnablePassthrough()` is assigned to the key expected by the second chain. Specifically, the structure `{\"tools\": RunnablePassthrough()}` is used. When the string output from `chain_tools` is passed to this dictionary, `RunnablePassthrough` ensures this string value is correctly assigned to the \"tools\" key, creating the dictionary `{\"tools\": \"output_string_from_chain_tools\"}`.\n",
    "  - **Construction of the Combined Chain**: The final combined chain is effectively formed as `chain_combined = chain_tools | {\"tools\": RunnablePassthrough()} | chain_strategy`. This sequence ensures that the output of `chain_tools` is correctly reshaped before being passed to `chain_strategy`.\n",
    "  - **Detailed Data Flow Tracing**: The lesson carefully traces the input and output types through each component of the individual and combined chains. This step-by-step analysis clarifies how `RunnablePassthrough`, used within the dictionary, facilitates the necessary data transformation for seamless inter-chain communication.\n",
    "  - **Modularity as a Key LCEL Advantage**: This example strongly highlights the modularity of LCEL. Developers can create and test smaller, self-contained \"bite-sized\" chains that perform specific tasks and then combine them to build more comprehensive and sophisticated applications.\n",
    "  - **Alternative: Single Long Chain Definition**: The lesson acknowledges that the combined functionality can also be achieved by defining one continuous LCEL chain with all individual components piped in sequence. It also suggests using parentheses and new lines for improved readability when chains become very long.\n",
    "  - **`ChatPromptTemplate.from_template()` Convenience**: The example uses `ChatPromptTemplate.from_template(\"template_string_with_{variable}\")`, pointing out that this factory method conveniently assumes the provided template is intended as a human message by default, simplifying prompt creation.\n",
    "  - **Component Reusability**: The individual components like `chat_model` and `string_parser` are reused in the definitions of both `chain_tools` and `chain_strategy`, showcasing efficient use of resources.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **Using `RunnablePassthrough` within a Dictionary for Data Reshaping Between Chains**\n",
    "    1.  **Why is this concept important?** When connecting two LCEL chains where the first chain's direct output format doesn't match the second chain's expected input structure (e.g., first chain outputs a raw string, second expects a dictionary `{\"key\": value}`), a transformation is needed. Using `RunnablePassthrough` inside a dictionary literal (e.g., `{\"key\": RunnablePassthrough()}`) provides a clean LCEL-native way to take the incoming data (from the first chain) and assign it as the value to a specific key in a new dictionary, which then becomes the input for the second chain.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In many real-world AI pipelines, different stages might be developed independently or might inherently produce/consume data in different structures. For instance, a chain that extracts raw text (`string`) might need to feed into another chain that expects this text to be part of a larger context provided as a dictionary (e.g., `{\"document_text\": \"extracted_string\", \"user_query\": \"some_query\"}`). The `RunnablePassthrough` technique within a dictionary allows for this \"re-packaging\" or \"contextualization\" of data smoothly.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Understanding `RunnableLambda` is a good next step, as it allows for more complex, custom Python functions to be inserted into chains for arbitrary data transformations if `RunnablePassthrough` is too simple. Also, exploring `RunnableParallel` for creating dictionaries with multiple, potentially independent, runnable operations as values can be very useful for preparing complex inputs for a subsequent chain. The general concept of data mapping and transformation in data pipelines is relevant.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson demonstrates several key Python code snippets for LCEL:\n",
    "\n",
    "1.  **Using `RunnablePassthrough`**:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "    passthrough = RunnablePassthrough()\n",
    "    # print(passthrough.invoke(\"Hi\"))  # Output: \"Hi\"\n",
    "    # print(passthrough.invoke([1, 2, 3])) # Output: [1, 2, 3]\n",
    "    ```\n",
    "\n",
    "2.  **Defining Individual Chains (`chain_tools`, `chain_strategy`)**:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    # chat_model = ChatOpenAI() # Or specific model settings\n",
    "    # string_parser = StrOutputParser()\n",
    "\n",
    "    # chat_template_tools = ChatPromptTemplate.from_template(\n",
    "    #     \"What are the five most important tools a {job_title} needs? Answer only by listing the tools.\"\n",
    "    # )\n",
    "    # chain_tools = chat_template_tools | chat_model | string_parser\n",
    "\n",
    "    # chat_template_strategy = ChatPromptTemplate.from_template(\n",
    "    #     \"Considering the tools provided: {tools}, develop a strategy for effectively learning and mastering them.\"\n",
    "    # )\n",
    "    # chain_strategy = chat_template_strategy | chat_model | string_parser\n",
    "    ```\n",
    "\n",
    "3.  **Invoking Individual Chains**:\n",
    "\n",
    "    ```python\n",
    "    # tools_output = chain_tools.invoke({\"job_title\": \"Data scientist\"})\n",
    "    # print(tools_output)\n",
    "\n",
    "    # strategy_output = chain_strategy.invoke({\"tools\": tools_output}) # Manually passing output\n",
    "    # print(strategy_output)\n",
    "    ```\n",
    "\n",
    "4.  **Combining Chains using `RunnablePassthrough` for Input Shaping**:\n",
    "    This is the core concept for combining the chains when their input/output types don't directly match for a simple pipe.\n",
    "    The structure to pipe the output of `chain_tools` (a string) into the `tools` key of a dictionary for `chain_strategy`.\n",
    "\n",
    "    ```python\n",
    "    # combined_chain = chain_tools | {\"tools\": RunnablePassthrough()} | chain_strategy\n",
    "    ```\n",
    "\n",
    "    So the full expression would look like:\n",
    "\n",
    "    ```python\n",
    "    # combined_chain_definition = (\n",
    "    #     (chat_template_tools | chat_model | string_parser) |\n",
    "    #     {\"tools\": RunnablePassthrough()} |\n",
    "    #     (chat_template_strategy | chat_model | string_parser)\n",
    "    # )\n",
    "    # result = combined_chain_definition.invoke({\"job_title\": \"Data scientist\"})\n",
    "    # print(result)\n",
    "    ```\n",
    "\n",
    "    The video demonstrates building `chain_tools` and `chain_strategy` first, then combining them:\n",
    "\n",
    "    ```python\n",
    "    # chain_combined = chain_tools | {\"tools\": RunnablePassthrough()} | chain_strategy\n",
    "    # final_result = chain_combined.invoke({\"job_title\": \"Data scientist\"})\n",
    "    # print(final_result)\n",
    "    ```\n",
    "\n",
    "5.  **Long Chain Definition with Formatting for Readability**:\n",
    "\n",
    "    ```python\n",
    "    # equivalent_long_chain = (\n",
    "    #     chat_template_tools\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    #     | {\"tools\": RunnablePassthrough()} # This part reshapes the string output to dict for next prompt\n",
    "    #     | chat_template_strategy\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    # )\n",
    "    # result_long = equivalent_long_chain.invoke({\"job_title\": \"Data scientist\"})\n",
    "    # print(result_long)\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Application:** What's a different real-world data processing scenario where piping two distinct chains together, using `RunnablePassthrough` for data shaping, would be beneficial?\n",
    "      - *Answer:* A scenario involving a customer support system: `Chain A` could transcribe a customer's voice call to text (outputting a string). `Chain B` might then analyze this text for sentiment and categorize the issue, but it expects input as a dictionary like `{\"transcript_text\": \"transcribed_string\", \"customer_id\": \"some_id\"}`. `RunnablePassthrough` could be used with other runnables (e.g., `RunnableParallel` to fetch `customer_id`) to structure the raw transcript string into the required dictionary format for `Chain B`.\n",
    "2.  **Teaching:** How would you explain the role of `{\"tools\": RunnablePassthrough()}` when combining `chain_tools` and `chain_strategy` to a colleague who understands individual chains but is new to combining them with input reshaping?\n",
    "      - *Answer:* \"Think of it this way: `chain_tools` gives us a plain string of tools. But `chain_strategy` is expecting a labeled package, specifically a dictionary where that string is labeled as 'tools' (i.e., `{'tools': 'our_string'}`). The `{\"tools\": RunnablePassthrough()}` part acts like a packer: it takes the raw string output by `chain_tools`, the `RunnablePassthrough()` just hands that string over as-is, and the `{\"tools\": ...}` part puts it into a box labeled 'tools', exactly what `chain_strategy` needs to find its input.\"\n",
    "3.  **Design Choice:** When would you choose to define multiple smaller, named chains (like `chain_tools` and `chain_strategy`) and then pipe them together, versus defining one single, long LCEL chain for a complex task?\n",
    "      - *Answer:* You'd typically define smaller, named chains when distinct parts of the workflow are logically separate, reusable, or complex enough to warrant individual testing and development (like `chain_tools` for tool identification). This modular approach improves readability, maintainability, and allows these sub-chains to be potentially reused in other larger workflows. A single long chain might be acceptable for simpler, linear flows where intermediate steps aren't independently meaningful or reusable.\n",
    "\n",
    "# Graphing Runnables\n",
    "\n",
    "### Summary\n",
    "This lesson introduces a method for visualizing LangChain Expression Language (LCEL) chains to better understand their structure and data flow, particularly helpful for beginners. It guides users through installing the \"Gandalf\" Python library and then demonstrates how to apply its `get_graph().print_ascii()` methods to an existing LCEL chain object (like the combined chain from the previous lesson) to generate a simple ASCII art representation of the chain's components directly within a Jupyter Notebook.\n",
    "\n",
    "### Highlights\n",
    "-   **Visualizing LCEL Chains**: The lesson focuses on a technique to visually represent the structure of LCEL chains. This visualization can significantly aid in comprehending the sequence of operations and data flow within a chain, which is especially useful when first learning about LCEL or when dealing with more complex chains.\n",
    "-   **\"Gandalf\" Library for Visualization**: A third-party Python library, humorously named \"Gandalf,\" is introduced as the tool for generating these visualizations. It requires installation using pip within the active Python environment (e.g., `pip install gandalf` in an Anaconda environment).\n",
    "-   **Generating an ASCII Graph**: The process to create a visual representation involves calling the `.get_graph()` method on an LCEL chain object. This method returns a graph object, to which the `.print_ascii()` method can then be applied to render a text-based (ASCII) diagram of the chain's components and their connections.\n",
    "-   **Practical Usage in Jupyter Notebooks**: The lesson outlines the steps for use in a Jupyter Notebook: ensure the \"Gandalf\" library is installed, restart the kernel, define or load an LCEL chain (like the `chain_long` example from a previous lesson), and then execute `chain_object.get_graph().print_ascii()` in a cell to display the diagram.\n",
    "\n",
    "### Code Examples\n",
    "The lesson describes the following code-related steps:\n",
    "\n",
    "1.  **Installing the \"Gandalf\" Library** (in Anaconda Prompt or terminal):\n",
    "    ```bash\n",
    "    # pip install gandalf\n",
    "    ```\n",
    "    (Note: The user is instructed to activate their LangChain Conda environment first.)\n",
    "\n",
    "2.  **Visualizing the Chain in Python (Jupyter Notebook)**:\n",
    "    Assuming `chain_long` is an already defined LCEL chain object from the previous lesson's context:\n",
    "    ```python\n",
    "    # Ensure all necessary imports and chain definitions are loaded\n",
    "    # For example:\n",
    "    # from langchain_core.prompts import ChatPromptTemplate\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # from langchain_core.output_parsers import StrOutputParser\n",
    "    # from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "    # ... (definitions for chat_template_tools, chat_template_strategy, chat_model, string_parser) ...\n",
    "\n",
    "    # chain_long = (\n",
    "    #     chat_template_tools\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    #     | {\"tools\": RunnablePassthrough()}\n",
    "    #     | chat_template_strategy\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    # )\n",
    "\n",
    "    # After restarting the kernel and running cells to define chain_long:\n",
    "    # graph = chain_long.get_graph()\n",
    "    # graph.print_ascii()\n",
    "\n",
    "    # Or more concisely:\n",
    "    # chain_long.get_graph().print_ascii()\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** For which type of LCEL chain—a simple linear one, or a more complex one with branches (as hinted for future lessons with `RunnableParallel`)—would visualization using a tool like \"Gandalf\" be most beneficial, and why?\n",
    "    -   *Answer:* Visualization would be most beneficial for more complex chains, especially those with branches or multiple parallel paths. While helpful for linear chains for beginners, the ability to clearly see how different data flows converge or diverge in a branched chain would significantly aid in debugging and understanding intricate logic.\n",
    "2.  **Teaching:** How would seeing an ASCII graph of an LCEL chain help a beginner grasp the concept of components being piped together with the `|` operator?\n",
    "    -   *Answer:* An ASCII graph would visually reinforce the sequential nature implied by the `|` operator, showing each component as a distinct node and the connections as lines or arrows. This makes the abstract idea of \"output of one becomes input to the next\" tangible, helping a beginner see the chain not just as code but as a structured data processing pipeline.\n",
    "\n",
    "# RunnableParallel\n",
    "\n",
    "### Summary\n",
    "This lesson introduces the `RunnableParallel` class in LangChain Expression Language (LCEL), which allows for the simultaneous execution of multiple distinct chains using the same input. It demonstrates creating two separate chains—one to suggest programming books and another for project ideas based on a common programming language—and then running them concurrently using `RunnableParallel`, which outputs a dictionary containing both sets of results. The lesson also showcases visualizing this parallel execution graph and empirically proves its time efficiency compared to invoking the chains sequentially.\n",
    "\n",
    "### Highlights\n",
    "-   **Introducing `RunnableParallel`**: LCEL's `RunnableParallel` class facilitates the concurrent execution of multiple, distinct runnable chains or components. These runnables typically operate on the same initial input, enabling simultaneous, independent processing paths.\n",
    "-   **Simultaneous Execution of Different Chains**: Unlike sequentially piping chains where one chain's output feeds the next, `RunnableParallel` executes its constituent chains at the same time. The lesson demonstrates this by running `chain_books` and `chain_projects` (both for the same `{programming_language}`) in parallel.\n",
    "-   **Dictionary-Based Construction and Output**: `RunnableParallel` is often constructed by passing a dictionary where keys are user-defined strings and values are the runnable objects (e.g., `RunnableParallel(key1=runnable1, key2=runnable2)`). When invoked, it returns a dictionary where these keys hold the respective results from each runnable.\n",
    "-   **Visualizing Parallel Execution Paths**: The lesson demonstrates using `chain_parallel.get_graph().print_ascii()` to visualize the flow. The resulting ASCII graph clearly shows the input branching out to the parallel chains, which then process independently before their outputs are collected.\n",
    "-   **Significant Time Efficiency**: A practical performance comparison using IPython's `%%time` magic command reveals that invoking a `RunnableParallel` object is considerably faster than invoking its constituent chains one after another (sequentially). This highlights the advantage of parallel processing for reducing overall latency.\n",
    "-   **Distinction from the `batch` Method**: It's crucial to differentiate `RunnableParallel` from the `batch` method. `RunnableParallel` executes *several different* runnables using the *same* input (or parts of it). In contrast, the `batch` method is used to invoke the *same* runnable with *many different* input values.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **`RunnableParallel` vs. `batch` Method**\n",
    "    1.  **Why is this concept important?** Understanding the difference is crucial for choosing the correct tool for efficient execution in LCEL. Using one when the other is appropriate can lead to suboptimal performance or incorrect logic.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?**\n",
    "        -   **`RunnableParallel`**: Use when you need to perform several *different operations* on the *same piece of input* simultaneously. For example, from a single product description, you might want to extract features, generate a short ad copy, and check for compliance, all at once. The outputs are typically grouped by the nature of the operation (e.g., `{\"features\": \"...\", \"ad_copy\": \"...\", \"compliance_status\": \"...\"}`).\n",
    "        -   **`batch`**: Use when you need to perform the *same operation* on *many different pieces of input*. For example, translating 100 different sentences from English to French using the same translation chain. The output is a list of results, corresponding to each input item.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** General concepts of parallel computing, concurrency, and asynchronous programming can provide a deeper understanding of the principles behind `RunnableParallel`. For `batch`, understanding API rate limits and efficient bulk data processing is relevant. Exploring LangChain's `RunnableMap` can also be useful, as `RunnableParallel` is a convenient way to use `RunnableMap` where each key corresponds to a runnable.\n",
    "\n",
    "### Code Examples\n",
    "The lesson describes the following Python code snippets for LCEL:\n",
    "\n",
    "1.  **Importing `RunnableParallel`**:\n",
    "    ```python\n",
    "    from langchain_core.runnables import RunnableParallel\n",
    "    # Other necessary imports: ChatPromptTemplate, ChatOpenAI, StrOutputParser\n",
    "    ```\n",
    "\n",
    "2.  **Defining Individual Chains (`chain_books`, `chain_projects`)**:\n",
    "    ```python\n",
    "    # from langchain_core.prompts import ChatPromptTemplate\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    # chat_model = ChatOpenAI()\n",
    "    # string_parser = StrOutputParser()\n",
    "\n",
    "    # chat_template_books = ChatPromptTemplate.from_template(\n",
    "    #     \"Suggest three of the best intermediate level {programming_language} books. Answer only by listing the books.\"\n",
    "    # )\n",
    "    # chain_books = chat_template_books | chat_model | string_parser\n",
    "\n",
    "    # chat_template_projects = ChatPromptTemplate.from_template(\n",
    "    #     \"Suggest three interesting {programming_language} projects suitable for intermediate level programmers. Answer only by listing the projects.\"\n",
    "    # )\n",
    "    # chain_projects = chat_template_projects | chat_model | string_parser\n",
    "    ```\n",
    "\n",
    "3.  **Constructing and Invoking `RunnableParallel`**:\n",
    "    ```python\n",
    "    # chain_parallel = RunnableParallel(\n",
    "    #     books=chain_books,\n",
    "    #     projects=chain_projects\n",
    "    # )\n",
    "\n",
    "    # parallel_results = chain_parallel.invoke({\"programming_language\": \"Python\"})\n",
    "    # print(parallel_results)\n",
    "    # Expected output: {'books': '...', 'projects': '...'}\n",
    "    ```\n",
    "\n",
    "4.  **Visualizing the Parallel Chain**:\n",
    "    ```python\n",
    "    # graph = chain_parallel.get_graph()\n",
    "    # graph.print_ascii()\n",
    "    ```\n",
    "\n",
    "5.  **Performance Comparison using `%%time`**:\n",
    "    The lesson describes using IPython's cell magic `%%time` in separate cells:\n",
    "\n",
    "    *Cell 1 (Chain Books):*\n",
    "    ```python\n",
    "    # %%time\n",
    "    # result_books = chain_books.invoke({\"programming_language\": \"Python\"})\n",
    "    ```\n",
    "\n",
    "    *Cell 2 (Chain Projects):*\n",
    "    ```python\n",
    "    # %%time\n",
    "    # result_projects = chain_projects.invoke({\"programming_language\": \"Python\"})\n",
    "    ```\n",
    "\n",
    "    *Cell 3 (Chain Parallel):*\n",
    "    ```python\n",
    "    # %%time\n",
    "    # result_parallel = chain_parallel.invoke({\"programming_language\": \"Python\"})\n",
    "    ```\n",
    "    The wall time for Cell 3 is expected to be significantly less than the sum of wall times for Cell 1 and Cell 2.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Describe a real-world scenario, different from the lesson's example of books and projects, where `RunnableParallel` would be highly beneficial for a data science application.\n",
    "    -   *Answer:* For a given news article, a data science application might need to simultaneously: a) generate a concise summary, b) extract key entities (people, organizations, locations), and c) determine the overall sentiment. Using `RunnableParallel` with three distinct chains for these tasks would allow all three pieces of information to be generated concurrently from the single article input, speeding up the overall analysis.\n",
    "2.  **Design:** If you have three independent pieces of information to extract from a single user query using three different prompt/model configurations, would you use three sequential `invoke` calls, `RunnableParallel`, or the `batch` method? Explain your choice.\n",
    "    -   *Answer:* I would use `RunnableParallel`. Since there are three *different* operations (different prompt/model configurations) to be performed on the *same* single user query, and these operations are independent, `RunnableParallel` is designed for this exact scenario to execute them concurrently for better performance. Sequential `invoke` calls would be slower, and `batch` is for applying the *same* operation to *multiple different* inputs.\n",
    "\n",
    "# Piping a RunnableParallel with other Runnables\n",
    "\n",
    "### Summary\n",
    "This lesson demonstrates how to pipe the dictionary output of a `RunnableParallel` object into subsequent components in an LangChain Expression Language (LCEL) chain to perform further processing, such as estimating completion time for previously generated book and project suggestions. It also reveals a key convenience feature: LCEL automatically wraps a dictionary of runnables (e.g., `{\"key1\": runnable1, \"key2\": runnable2}`) in a `RunnableParallel` instance when it's piped in a chain, simplifying the syntax for defining these parallel execution steps while maintaining the same execution plan.\n",
    "\n",
    "### Highlights\n",
    "-   **Piping `RunnableParallel` Output**: The lesson extends the use of `RunnableParallel` by showing how its dictionary output (containing results from multiple parallel operations, like book and project suggestions) can be seamlessly piped as input into a subsequent sequence of LCEL components (e.g., a new prompt template, model, and parser) for further, aggregated processing.\n",
    "-   **End-to-End Example for Combined Insights**: An illustrative chain is constructed:\n",
    "    1.  Initially, book suggestions and project ideas for a specified programming language are generated in parallel using a `RunnableParallel` construct.\n",
    "    2.  This combined dictionary output (e.g., `{\"books\": \"list of books\", \"projects\": \"list of projects\"}`) is then fed into a new prompt template that asks an LLM to estimate the total time required to complete all these books and projects.\n",
    "-   **Automatic `RunnableParallel` Wrapping (Syntactic Sugar)**: A significant feature highlighted is LCEL's ability to automatically treat a dictionary of runnables as a `RunnableParallel` step when it's piped in a chain. This means explicitly writing `RunnableParallel(key1=runnable1, key2=runnable2)` can often be simplified.\n",
    "-   **Simplified and Preferred Chain Definition**: Due to the automatic wrapping, developers can define chains with parallel steps more concisely. For instance, instead of `RunnableParallel(books=chain_books, projects=chain_projects) | next_prompt`, one can directly use `{\"books\": chain_books, \"projects\": chain_projects} | next_prompt`. This latter, more direct syntax is noted as being commonly encountered and preferred for its brevity.\n",
    "-   **Identical Execution Plan and Visualization**: The lesson confirms that the ASCII graph generated using `get_graph().print_ascii()` for the chain is identical whether the `RunnableParallel` step is defined explicitly using the class or implicitly using the dictionary syntax. This demonstrates that both forms result in the same underlying execution plan.\n",
    "-   **Input Propagation to Parallel Branches**: When the overall chain is invoked (e.g., with `{\"programming_language\": \"Python\"}`), this initial input is correctly passed to all the constituent runnables within the (explicit or implicit) `RunnableParallel` block, allowing each parallel branch to operate on the same source data.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Automatic `RunnableParallel` Wrapping for Dictionaries of Runnables**\n",
    "    1.  **Why is this concept important?** This feature significantly enhances developer experience by reducing verbosity and making chain definitions more intuitive. It abstracts away the need to explicitly instantiate `RunnableParallel` when the structure is a simple map of keys to runnables, which is a very common pattern.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In many applications, one might want to fetch or compute several pieces of information in parallel from a common input and then use these pieces together. For example, for a user query, one might run parallel chains to get user history, product catalog info, and current promotions. The ability to define this parallel step as a simple dictionary `{\"history\": chain_history, \"catalog\": chain_catalog, \"promos\": chain_promos}` that then pipes into an aggregator makes the code cleaner and easier to understand.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Understanding Python dictionaries and how they are used for keyword arguments or structured data is fundamental. Reviewing the `RunnableParallel` class itself helps understand what happens under the hood. For more complex input/output manipulations around parallel steps, exploring `RunnableLambda` or custom runnables that can process dictionaries might be relevant.\n",
    "\n",
    "### Code Examples\n",
    "The lesson describes the following Python code snippets for LCEL:\n",
    "\n",
    "1.  **Defining a New Prompt Template for Time Estimation**:\n",
    "    ```python\n",
    "    # from langchain_core.prompts import ChatPromptTemplate\n",
    "    # Assuming 'books' and 'projects' are keys from RunnableParallel output\n",
    "    # chat_template_time = ChatPromptTemplate.from_template(\n",
    "    #     \"Based on the books: {books} and the projects: {projects}, please estimate the total time it would take to complete them.\"\n",
    "    # )\n",
    "    ```\n",
    "\n",
    "2.  **Modifying `ChatOpenAI` for Longer Responses**:\n",
    "    ```python\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # chat_model = ChatOpenAI(max_tokens=500)\n",
    "    ```\n",
    "\n",
    "3.  **Defining a Chain with Explicit `RunnableParallel` (`chain_time_one`)**:\n",
    "    ```python\n",
    "    # from langchain_core.runnables import RunnableParallel\n",
    "    # Assuming chain_books, chain_projects, chat_model, string_parser are defined\n",
    "    # chain_time_one = (\n",
    "    #     RunnableParallel(books=chain_books, projects=chain_projects)\n",
    "    #     | chat_template_time\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    # )\n",
    "    # result_one = chain_time_one.invoke({\"programming_language\": \"Python\"})\n",
    "    # print(result_one)\n",
    "    ```\n",
    "\n",
    "4.  **Defining a Chain with Implicit `RunnableParallel` (Dictionary Syntax - `chain_time_two`)**:\n",
    "    This is the key syntactic sugar highlighted.\n",
    "    ```python\n",
    "    # chain_time_two = (\n",
    "    #     {\"books\": chain_books, \"projects\": chain_projects} # Dictionary of runnables\n",
    "    #     | chat_template_time\n",
    "    #     | chat_model\n",
    "    #     | string_parser\n",
    "    # )\n",
    "    # result_two = chain_time_two.invoke({\"programming_language\": \"Python\"})\n",
    "    # print(result_two)\n",
    "    ```\n",
    "    The lesson notes that `chain_time_one` and `chain_time_two` produce the same output and have the same graph visualization.\n",
    "\n",
    "5.  **Visualizing the Combined Chain**:\n",
    "    ```python\n",
    "    # graph = chain_time_one.get_graph() # or chain_time_two.get_graph()\n",
    "    # graph.print_ascii()\n",
    "    ```\n",
    "    The graph would show the initial input splitting for the parallel `books` and `projects` chains, then their dictionary output merging into `chat_template_time`, followed by the model and parser.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Readability:** How does the syntactic sugar of automatically wrapping a dictionary of runnables into a `RunnableParallel` instance improve the readability and maintainability of complex LCEL chains?\n",
    "    -   *Answer:* It makes the chain definition more concise and declarative by focusing on the *structure* of the parallel operations (a map of keys to runnables) rather than explicitly naming the `RunnableParallel` class. This reduces boilerplate, making it easier to quickly grasp the parallel steps and their outputs, especially when reading or maintaining chains with multiple such parallel branches.\n",
    "2.  **Application:** Imagine you want to generate a product description, identify 3 key features, and suggest 2 target audience segments, all based on a product name. How would you structure this using the implicit `RunnableParallel` syntax for the parallel generation part, followed by a final prompt that perhaps combines these for a marketing brief?\n",
    "    -   *Answer:* You could define three separate chains: `chain_desc` (generates description), `chain_features` (identifies features), and `chain_audience` (suggests audience segments), each taking `{product_name}` as input. Then, use implicit `RunnableParallel`: `parallel_step = {\"description\": chain_desc, \"features\": chain_features, \"audience\": chain_audience}`. This `parallel_step` could then be piped into a final prompt template like `ChatPromptTemplate.from_template(\"Marketing Brief for {product_name}:\\nDescription: {description}\\nFeatures: {features}\\nAudience: {audience}\")` followed by a model and parser.\n",
    "\n",
    "# RunnableLambda\n",
    "\n",
    "### Summary\n",
    "This lesson introduces the `RunnableLambda` class in LangChain Expression Language (LCEL), which allows developers to seamlessly integrate custom Python functions, particularly lambda functions, into LCEL chains by converting them into runnable objects. It demonstrates creating simple lambda functions for sum and square operations, wrapping them with `RunnableLambda` to enable standard LCEL methods like `invoke`, and then piping these runnable functions together to form a new chain, which is also visualized to show the flow of data through these custom logic steps.\n",
    "\n",
    "### Highlights\n",
    "-   **Integrating Custom Functions with `RunnableLambda`**: LCEL's `RunnableLambda` class provides a straightforward mechanism to wrap any standard Python function or, more commonly, a lambda function, thereby transforming it into a first-class `Runnable` object. This allows custom Python logic to be easily incorporated as a component within LCEL chains.\n",
    "-   **Enabling Standard Runnable Interface on Custom Logic**: Once a Python function is wrapped with `RunnableLambda` (e.g., `runnable_func = RunnableLambda(my_lambda_func)`), it gains the standard runnable interface. This means methods like `invoke`, `batch`, and `stream` can be called on it, allowing it to interact consistently with other LCEL components.\n",
    "-   **Practical Example: Sum and Square Lambda Functions**: The lesson illustrates this concept by first defining two simple lambda functions: one to calculate the sum of elements in a list (`find_sum`) and another to compute the square of a number (`find_square`). These are then individually wrapped to become `runnable_sum` and `runnable_square`.\n",
    "-   **Piping `RunnableLambda` Objects into Chains**: The newly created `RunnableLambda` objects can be composed using the pipe (`|`) operator to form `RunnableSequence` chains. The example constructs `chain = runnable_sum | runnable_square`, which takes a list, calculates its sum using the first lambda, and then squares that sum using the second lambda.\n",
    "-   **Leveraging Anonymous Functions**: `RunnableLambda` is particularly well-suited for lambda functions as it allows these small, often single-use, anonymous functions to be defined directly when creating the runnable instance (e.g., `RunnableLambda(lambda x: x * 2)`) or by passing their definitions, aligning with the typical use of lambdas for concise, inline operations.\n",
    "-   **Visualization of Chains with `RunnableLambda`**: Chains that include `RunnableLambda` components can be visualized using the `get_graph().print_ascii()` method. The resulting ASCII diagram clearly depicts the input, each lambda function as a distinct processing step in the sequence, and the final output, enhancing understanding of the data flow through custom logic.\n",
    "\n",
    "### Code Examples\n",
    "The lesson demonstrates the following Python code snippets for using `RunnableLambda`:\n",
    "\n",
    "1.  **Defining Standard Lambda Functions**:\n",
    "    ```python\n",
    "    # Lambda function to sum elements in a list\n",
    "    # find_sum = lambda data_list: sum(data_list)\n",
    "    # print(find_sum([1, 2, 5]))  # Output: 8\n",
    "\n",
    "    # Lambda function to square a number\n",
    "    # find_square = lambda x: x**2\n",
    "    # print(find_square(8))  # Output: 64\n",
    "    ```\n",
    "\n",
    "2.  **Importing and Using `RunnableLambda`**:\n",
    "    ```python\n",
    "    from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "    # Wrapping the lambda functions (or their definitions)\n",
    "    # runnable_sum = RunnableLambda(find_sum)\n",
    "    # # Or directly: runnable_sum = RunnableLambda(lambda data_list: sum(data_list))\n",
    "    # print(runnable_sum.invoke([1, 2, 5]))  # Output: 8\n",
    "\n",
    "    # runnable_square = RunnableLambda(find_square)\n",
    "    # # Or directly: runnable_square = RunnableLambda(lambda x: x**2)\n",
    "    # print(runnable_square.invoke(8))  # Output: 64\n",
    "    ```\n",
    "\n",
    "3.  **Creating a Chain with `RunnableLambda` Objects**:\n",
    "    ```python\n",
    "    # chain = runnable_sum | runnable_square\n",
    "    # result = chain.invoke([1, 2, 5])  # Input list -> sum (8) -> square (64)\n",
    "    # print(result)  # Output: 64\n",
    "    ```\n",
    "\n",
    "4.  **Visualizing the Chain**:\n",
    "    ```python\n",
    "    # graph = chain.get_graph()\n",
    "    # graph.print_ascii()\n",
    "    # Or: chain.get_graph().print_ascii()\n",
    "    ```\n",
    "    The ASCII output would show the sequence: input list -> sum lambda -> square lambda -> final output.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Flexibility:** How does `RunnableLambda` enhance the flexibility and extensibility of LangChain Expression Language (LCEL) for data scientists working on custom data processing or transformation tasks within their AI pipelines?\n",
    "    -   *Answer:* `RunnableLambda` significantly enhances flexibility by allowing data scientists to inject arbitrary Python code (data cleaning, custom calculations, conditional logic) directly into LCEL chains without needing to create complex custom classes. This means existing Python utility functions or quick ad-hoc transformations can be seamlessly integrated, making LCEL adaptable to a wider range of specific data manipulation needs within a pipeline.\n",
    "2.  **Debugging:** If a chain involving a `RunnableLambda` produces an unexpected result or an error, how might you approach debugging the custom function wrapped by it, considering it's part of a larger LCEL sequence?\n",
    "    -   *Answer:* To debug the custom function within a `RunnableLambda`, one could first test the lambda function or regular Python function independently with sample inputs expected at that stage of the chain to ensure its core logic is correct. If the function itself is sound, then one can invoke the `RunnableLambda` component directly (e.g., `runnable_sum.invoke(test_input)`) to isolate whether the issue lies in its interaction with the LCEL wrapper or the data it receives from the preceding component in the chain.\n",
    "\n",
    "# The @chain decorator\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson demonstrates that LangChain's `RunnableLambda` can wrap regular named Python functions, not just anonymous lambdas, to integrate them into LangChain Expression Language (LCEL) chains. It then introduces the `@chain` decorator as a more elegant and Pythonic way to achieve this same conversion, automatically wrapping a decorated function to make it a `RunnableLambda` instance. The lesson concludes with an important caution about potential naming conflicts if a variable is named identically to the imported decorator, which could lead to runtime errors.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **`RunnableLambda` for Named Functions**: The lesson clarifies that the `RunnableLambda` class is versatile and can be used to wrap any standard Python function (defined with `def` and a name) into an LCEL `Runnable`. This extends its utility beyond just lambda functions, allowing existing, more complex functions to be seamlessly incorporated into chains.\n",
    "  - **Introducing the `@chain` Decorator**: LangChain provides a `@chain` decorator (e.g., imported from `langchain_core.runnables`) as a form of syntactic sugar. This decorator offers a cleaner, more Pythonic, and often more readable way to convert a standard Python function into a `RunnableLambda` instance directly at the point of function definition.\n",
    "  - **Decorator Functionality Explained**: When a function is decorated with `@chain` (e.g., `@chain def my_custom_function(x): ...`), the decorator effectively wraps `my_custom_function` in `RunnableLambda` behind the scenes. The name `my_custom_function` in the current scope then refers to this new `RunnableLambda` object, and the runnable's name attribute is typically set from the original function's name.\n",
    "  - **Equivalence of Methods**: Both approaches—explicitly wrapping a function using `RunnableLambda(my_function)` and decorating `my_function` with `@chain`—achieve the same fundamental outcome: the Python function becomes a runnable component. This runnable can then be invoked using methods like `.invoke()` and can be piped together with other LCEL components to form chains.\n",
    "  - **Practical Example and Comparison**: The lesson demonstrates building an identical chain that first sums elements of a list and then squares the result. This is done first by explicitly wrapping named functions (`find_sum`, `find_square`) with `RunnableLambda`, and then by defining similar functions decorated with `@chain`, leading to the same functional chain.\n",
    "  - **Crucial Name Conflict Warning**: An important practical tip is provided regarding the `@chain` decorator: if it's imported with the name `chain` (i.e., `from ... import chain`), one must avoid naming other variables (especially `RunnableSequence` chain objects) also as `chain` in the same scope. Doing so can shadow the decorator, leading to a `TypeError` if Python attempts to use the `RunnableSequence` object as a decorator function.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **Python Decorators and the `@chain` Syntactic Sugar**\n",
    "    1.  **Why is this concept important?** Decorators are a powerful Python feature that allows for modifying or enhancing functions or classes in a clean, readable way. The `@chain` decorator specifically simplifies the process of making a regular Python function compatible with the LCEL ecosystem, abstracting the `RunnableLambda` wrapping and making the intent clearer directly at the function's definition.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** When building complex data processing pipelines with LCEL, data scientists often need to insert custom logic (e.g., specific data transformations, API calls not covered by standard LangChain components, complex business rules). Using `@chain` allows these custom Python functions to be defined naturally and then immediately be ready for integration into LCEL sequences, improving code organization and reducing boilerplate.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** A good understanding of Python functions, first-class functions (functions as objects), and the general concept of decorators in Python is highly beneficial. Learning how to write your own decorators can provide deeper insight into what `@chain` does. Also, understanding how LCEL's `Runnable` protocol works gives context to why this conversion is necessary.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson describes the following Python code snippets:\n",
    "\n",
    "1.  **Defining Regular Named Functions**:\n",
    "\n",
    "    ```python\n",
    "    # def find_sum(x): # x is assumed to be a list\n",
    "    #     return sum(x)\n",
    "\n",
    "    # def find_square(x): # x is assumed to be numeric\n",
    "    #     return x**2\n",
    "    ```\n",
    "\n",
    "2.  **Wrapping Named Functions with `RunnableLambda`**:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "    # runnable_sum_explicit = RunnableLambda(find_sum)\n",
    "    # runnable_square_explicit = RunnableLambda(find_square)\n",
    "\n",
    "    # chain_one = runnable_sum_explicit | runnable_square_explicit\n",
    "    # print(chain_one.invoke([1, 2, 5]))  # Output: 64\n",
    "    ```\n",
    "\n",
    "3.  **Importing and Using the `@chain` Decorator**:\n",
    "    The transcript mentions importing `chain` from `langchain_core.runnables`. (Note: The exact import path for decorators can sometimes be specific, e.g., `langchain_core.runnables.decorator.chain` or a re-export. The user should verify the correct import based on their LangChain version if issues arise. The lesson proceeds with `from langchain_core.runnables import chain`.)\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.runnables import chain # As per transcript\n",
    "\n",
    "    # @chain\n",
    "    # def runnable_sum_decorated(x): # Function name can be different or same\n",
    "    #     return sum(x)\n",
    "\n",
    "    # @chain\n",
    "    # def runnable_square_decorated(x):\n",
    "    #     return x**2\n",
    "\n",
    "    # print(type(runnable_sum_decorated)) # Output: <class 'langchain_core.runnables.lambdas.RunnableLambda'>\n",
    "\n",
    "    # chain_two = runnable_sum_decorated | runnable_square_decorated\n",
    "    # print(chain_two.invoke([1, 2, 5]))  # Output: 64\n",
    "    ```\n",
    "\n",
    "4.  **Demonstrating Name Conflict**:\n",
    "    The lesson describes a scenario:\n",
    "\n",
    "      - Restart kernel.\n",
    "      - Import `chain` decorator: `from langchain_core.runnables import chain`.\n",
    "      - Define a variable named `chain`: `chain = RunnableLambda(find_sum) | RunnableLambda(find_square)`.\n",
    "      - Then, in a subsequent cell, try to use `@chain` to decorate a function:\n",
    "        ```python\n",
    "        # @chain # This will now refer to the RunnableSequence object, not the decorator\n",
    "        # def some_function_after_conflict(x):\n",
    "        #     return x\n",
    "        ```\n",
    "\n",
    "    This setup would lead to a `TypeError: 'RunnableSequence' object is not callable`.\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Readability & Maintainability:** How does using the `@chain` decorator compared to explicitly wrapping functions with `RunnableLambda` potentially impact the readability and maintainability of a Python script that defines many custom LCEL components?\n",
    "      - *Answer:* Using the `@chain` decorator generally improves readability by making the intent clear directly at the function definition—this function is designed to be an LCEL runnable. It reduces boilerplate code compared to explicit `RunnableLambda(my_func)` calls for each function, leading to cleaner and more maintainable scripts, especially when many custom functions are being integrated into chains.\n",
    "2.  **Scope:** If you have a complex, multi-line Python function with significant internal logic that you want to integrate into an LCEL chain, would explicitly using `RunnableLambda(my_complex_function)` or decorating it with `@chain` be more suitable, or are they equally applicable? Why?\n",
    "      - *Answer:* Both are equally applicable and achieve the same result. The choice is largely stylistic: `@chain` is often preferred for its conciseness and directness at the function definition site, making it clear that the function is intended as a runnable. Explicitly using `RunnableLambda(my_complex_function)` might be chosen if the runnable creation needs to happen conditionally or in a different part of the code than the function definition, but for straightforward conversion, `@chain` is typically more elegant.\n",
    "\n",
    "# Adding memory to a chain (Part 1): Implementing the setup\n",
    "\n",
    "### Summary\n",
    "This lesson sets the groundwork for integrating memory, specifically `ConversationSummaryMemory`, into LangChain Expression Language (LCEL) chains to enable conversational context. It first demonstrates the manual process of providing conversation history to a chain via a `message_log` input variable, highlighting the impracticality of this approach for continuous dialogue. The lesson then introduces the `ConversationSummaryMemory` object, showing how it's initialized to manage conversation summaries, and concludes by posing the challenge of how to automatically feed this memory into an LCEL chain, setting the stage for using `RunnablePassthrough` in the subsequent lesson.\n",
    "\n",
    "### Highlights\n",
    "-   **Objective: Integrating Memory in LCEL Chains**: The lesson's primary goal is to prepare for attaching memory objects, using `ConversationSummaryMemory` as the example, to LCEL chains. This is essential for building chatbots or any application that needs to remember and utilize past interactions.\n",
    "-   **Illustrating Manual Conversation History**: Initially, the lesson shows how a basic LCEL chain (prompt | model | parser) with a prompt template requiring `message_log` (conversation summary) and `question` inputs can be made to \"remember\" by manually crafting and passing the `message_log` string. This demonstrates the concept but also its limitations.\n",
    "-   **Limitations of Manual Memory Management**: The process of manually creating and updating the `message_log` for each conversational turn is shown to be impractical and not scalable for real applications. This underscores the need for an automated memory mechanism.\n",
    "-   **Introduction to `ConversationSummaryMemory`**: The `ConversationSummaryMemory` class is introduced as a dedicated component for handling conversational memory. It's initialized with an LLM (to perform the summarization) and a `memory_key` (e.g., \"message_log\") which it will use to store and retrieve the conversation summary.\n",
    "-   **The Core Challenge: Dynamic Memory Injection**: After setting up the memory object, the central problem identified is how to seamlessly and automatically feed the output of this memory object (i.e., the current `message_log`) into the LCEL chain, which expects it as an input variable for its prompt template.\n",
    "-   **Previewing `RunnablePassthrough`'s Role**: The lesson concludes by hinting that the `RunnablePassthrough` class, previously discussed for other purposes, will be a key component in solving the challenge of integrating the memory object's dynamic output with the chain's input requirements.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Role of `ConversationSummaryMemory`**\n",
    "    1.  **Why is this concept important?** In conversational AI, maintaining context is crucial for coherent and relevant interactions. `ConversationSummaryMemory` provides a mechanism to keep track of the conversation by progressively creating a summary of the dialogue. This prevents the context window from overflowing with raw history and provides a condensed version of past turns to the LLM.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's vital for chatbots, virtual assistants, and any system engaging in multi-turn dialogues. For example, a customer service bot uses it to remember what a user has already asked or stated, avoiding repetition and providing more informed responses. Without such memory, each user message would be treated in isolation.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other types of memory in LangChain (e.g., `ConversationBufferWindowMemory`, `ConversationKGMemory`), strategies for managing long-term memory, and the general architecture of conversational AI systems are relevant. Understanding how LLMs handle context windows and summarization techniques is also beneficial.\n",
    "\n",
    "### Code Examples\n",
    "The lesson describes the following Python code snippets related to setting up a conversational chain and memory:\n",
    "\n",
    "1.  **Conversational Prompt Template String**:\n",
    "    A string template that includes placeholders for the conversation history and the current user question.\n",
    "    ```python\n",
    "    # template = \"\"\"\n",
    "    # You are a helpful AI assistant.\n",
    "    # Here is the conversation so far:\n",
    "    # {message_log}\n",
    "    #\n",
    "    # Human: {question}\n",
    "    # AI:\n",
    "    # \"\"\"\n",
    "    ```\n",
    "\n",
    "2.  **Basic LCEL Chain Setup**:\n",
    "    ```python\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    # Assuming 'template' string is defined above\n",
    "    # prompt_template = ChatPromptTemplate.from_template(template)\n",
    "    # chat_model = ChatOpenAI() # Basic initialization\n",
    "    # string_parser = StrOutputParser()\n",
    "\n",
    "    # chain = prompt_template | chat_model | string_parser\n",
    "    ```\n",
    "\n",
    "3.  **Manual Invocation of the Chain (Illustrative)**:\n",
    "    *First turn (no history):*\n",
    "    ```python\n",
    "    # result1 = chain.invoke({\n",
    "    #     \"message_log\": \"\",  # Empty for the first message\n",
    "    #     \"question\": \"Can you give me an interesting fact I probably didn't know about?\"\n",
    "    # })\n",
    "    # print(result1)\n",
    "    ```\n",
    "    *Second turn (manually created summary):*\n",
    "    ```python\n",
    "    # manual_summary = \"The human asked for an interesting fact, and the AI provided one about [specific fact].\"\n",
    "    # result2 = chain.invoke({\n",
    "    #     \"message_log\": manual_summary,\n",
    "    #     \"question\": \"Can you elaborate a bit more on this fact?\"\n",
    "    # })\n",
    "    # print(result2)\n",
    "    ```\n",
    "\n",
    "4.  **Instantiating `ConversationSummaryMemory`**:\n",
    "    ```python\n",
    "    from langchain.memory import ConversationSummaryMemory\n",
    "    # from langchain_openai import ChatOpenAI # Assuming chat_model is an instance\n",
    "\n",
    "    # chat_memory = ConversationSummaryMemory(\n",
    "    #     llm=chat_model,  # LLM to use for summarization\n",
    "    #     memory_key=\"message_log\" # Key under which summary is stored/retrieved\n",
    "    # )\n",
    "    ```\n",
    "\n",
    "5.  **Loading Memory Variables (Illustrative)**:\n",
    "    ```python\n",
    "    # memory_vars = chat_memory.load_memory_variables({})\n",
    "    # print(memory_vars) # Output would be {'message_log': 'current_summary_string'}\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Motivation:** Why is it generally impractical to manually manage the `message_log` (conversation summary) when building a production-level conversational AI application?\n",
    "    -   *Answer:* Manually managing the `message_log` is impractical because it requires custom logic to store, retrieve, and update the conversation summary after every turn. This process is error-prone, difficult to scale, doesn't easily handle complex summarization needs, and tightly couples the application logic with the conversation history format, making the system brittle.\n",
    "2.  **Prediction:** Based on its name (\"passthrough\") and previous uses (e.g., passing input to a specific key in a dictionary for `RunnableParallel`), how do you anticipate `RunnablePassthrough` might help in connecting the `chat_memory.load_memory_variables({})` output to the chain's input, which expects both a `message_log` and a `question`?\n",
    "    -   *Answer:* `RunnablePassthrough` might be used to pass the user's current `question` through unchanged, while another part of the input mechanism (perhaps involving `RunnableParallel` or a similar structure) would simultaneously fetch the `message_log` from the memory object. Both could then be combined into the dictionary expected by the prompt template.\n",
    "\n",
    "# RunnablePassthrough with additional keys\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson delves into a key feature of LangChain's `RunnablePassthrough` class: the `.assign()` method, demonstrating how it can be used to dynamically add new key-value pairs to a dictionary as it passes through an LangChain Expression Language (LCEL) chain. This technique is pivotal for integrating memory into chains, as illustrated by an example where `.assign()` is used to attempt adding a `message_log` (sourced from a memory object like `chat_memory.load_memory_variables`) to an existing input dictionary that already contains a user's question. The lesson highlights that this direct usage results in a nested dictionary for the `message_log`, thereby setting up the problem for the next lesson on how to extract the actual summary string needed by the prompt.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **`RunnablePassthrough.assign()` Method**: This lesson introduces and focuses on the `.assign()` method of the `RunnablePassthrough` class. This powerful method allows for the addition or augmentation of key-value pairs within a dictionary that is being passed through an LCEL chain, without altering existing keys unless explicitly overwritten.\n",
    "  - **Dynamically Adding Data to Dictionaries in a Chain**: Using `.assign()`, new keys can be introduced into an input dictionary. The values for these new keys are generated by specified runnable or callable objects (such as lambda functions or methods from memory objects) that typically operate on the input dictionary itself.\n",
    "  - **Step-by-Step Illustration of `.assign()`**: A detailed example constructs a `RunnablePassthrough` chain using `.assign()` to add `first_letter` and `second_letter` keys to an initial dictionary like `{\"input\": \"string\"}`. This clearly demonstrates how lambda functions, when provided to `.assign()`, receive the entire current dictionary as their input and are used to compute values for the newly assigned keys.\n",
    "  - **Application to Memory Integration**: The primary application showcased is the preparation of a complete input dictionary required by a prompt template that expects both a user's `question` and a `message_log`. `RunnablePassthrough().assign(message_log=chat_memory.load_memory_variables)` is used in an attempt to inject the conversation history.\n",
    "  - **Identifying the Nested Dictionary Issue**: A key outcome demonstrated is that when `chat_memory.load_memory_variables` (which itself returns a dictionary, e.g., `{\"message_log\": \"summary\"}`) is used as the callable in `.assign(message_log=chat_memory.load_memory_variables)`, the value assigned to the `message_log` key in the main dictionary becomes this *entire returned dictionary*. This results in a nested structure like `{\"question\": \"...\", \"message_log\": {\"message_log\": \"actual_summary_string\"}}`.\n",
    "  - **Setting the Stage for String Extraction**: The lesson concludes by pinpointing the immediate challenge: the prompt template requires a simple string value for its `message_log` input variable, not the nested dictionary that results from the current use of `.assign()` with `load_memory_variables`. The solution for extracting the necessary string will be addressed in the subsequent lesson.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **Mechanism of `RunnablePassthrough.assign()` and Lambda/Callable Scoping**\n",
    "    1.  **Why is this concept important?** Understanding `.assign()` is crucial for manipulating data context within a chain. It allows you to enrich an input dictionary with new information derived from the existing input or from external callables (like memory loaders) without complex custom runnables for simple additions. The callables (e.g., lambdas) passed to `.assign()` receive the *current state of the dictionary* as their input.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In many pipelines, you start with partial input and need to augment it. For example, given a `user_id`, you might use `.assign()` to add `user_profile` by calling a function that fetches it, and add `user_purchase_history` by calling another. This enriched dictionary can then be passed to an LLM. It's fundamental for context augmentation, feature engineering within a chain, or preparing multifaceted inputs for prompts.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** `RunnableLambda` for more complex transformations if a simple assignment isn't enough. `RunnableParallel` if multiple items need to be fetched/computed independently and then merged. Python's dictionary manipulation, lambda functions, and the general concept of callable objects are also foundational. Understanding item getters (e.g., `operator.itemgetter`) can be relevant for extracting values, which is hinted as the next step.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson describes the following Python code snippets:\n",
    "\n",
    "1.  **Basic `RunnablePassthrough` Behavior**:\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "    # passthrough_identity = RunnablePassthrough()\n",
    "    # print(passthrough_identity.invoke(\"Hi\"))  # Output: \"Hi\"\n",
    "    # print(passthrough_identity.invoke({\"question\": \"Fact please?\"})) # Output: {'question': 'Fact please?'}\n",
    "    ```\n",
    "\n",
    "2.  **`RunnablePassthrough.assign()` with Lambda Functions for String Manipulation**:\n",
    "    Illustrates adding new keys based on an existing key's value.\n",
    "\n",
    "    ```python\n",
    "    # setup = RunnablePassthrough().assign(\n",
    "    #     first_letter=lambda x: list(x[\"input\"])[0],\n",
    "    #     second_letter=lambda x: list(x[\"input\"])[1]\n",
    "    # )\n",
    "    # result = setup.invoke({\"input\": \"hi\"})\n",
    "    # print(result)\n",
    "    # Expected output: {'input': 'hi', 'first_letter': 'h', 'second_letter': 'i'}\n",
    "    ```\n",
    "\n",
    "3.  **`RunnablePassthrough.assign()` with `chat_memory.load_memory_variables`**:\n",
    "    This is the core example for preparing memory input, leading to the nested dictionary issue.\n",
    "\n",
    "    ```python\n",
    "    # Assuming 'chat_memory' is an instance of ConversationSummaryMemory\n",
    "    # and its memory_key is 'message_log'.\n",
    "    # The 'chat_memory.load_memory_variables' method is a callable.\n",
    "    # When called with {} it returns something like {'message_log': 'summary string'}.\n",
    "\n",
    "    # input_augmenter = RunnablePassthrough().assign(\n",
    "    #     message_log=chat_memory.load_memory_variables\n",
    "    # )\n",
    "    # final_dict = input_augmenter.invoke({\"question\": \"Can you give me an interesting fact?\"})\n",
    "    # print(final_dict)\n",
    "    # Expected output (as per lesson's description of the problem):\n",
    "    # {\n",
    "    #   'question': 'Can you give me an interesting fact?',\n",
    "    #   'message_log': {'message_log': 'summary_of_conversation_so_far_or_empty'}\n",
    "    # }\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Data Transformation:** Why is the `.assign()` method of `RunnablePassthrough` a powerful tool for data transformation and enrichment within an LCEL chain, beyond just memory integration?\n",
    "      - *Answer:* The `.assign()` method is powerful because it allows for the inline creation of new data fields derived from existing input or external sources (via callables) directly within the flow of an LCEL chain. This enables dynamic feature engineering, context augmentation, or reformatting of data on-the-fly without needing to define separate, more verbose `RunnableLambda` components for each minor addition or modification to the data dictionary.\n",
    "2.  **Anticipation:** Given that `RunnablePassthrough().assign(message_log=chat_memory.load_memory_variables)` results in a nested dictionary like `{\"message_log\": {\"message_log\": \"summary_string\"}}`, what kind of operation or runnable component might be needed next in the chain to extract the actual \"summary\\_string\" so it can be directly used by the prompt template?\n",
    "      - *Answer:* To extract the \"summary\\_string,\" one would likely need a component that can select a value from a nested dictionary, such as a `RunnableLambda` wrapping a Python function that performs `lambda x: x['message_log']['message_log']`, or potentially by using an item getter if the structure is consistent and LCEL provides a runnable for that (like `operator.itemgetter` adapted for runnables).\n",
    "\n",
    "# Itemgetter\n",
    "\n",
    "### Summary\n",
    "\n",
    "This lesson addresses the issue from the previous session where using `RunnablePassthrough.assign()` with `chat_memory.load_memory_variables` resulted in a nested dictionary for the `message_log`. It introduces Python's `itemgetter` from the `operator` module as an efficient tool to extract specific values from data structures like dictionaries. By wrapping `itemgetter` within a `RunnableLambda`, it becomes an LangChain Expression Language (LCEL) component capable of retrieving the actual conversation summary string from the previously problematic nested structure, thus preparing the correct data format for the prompt template in the next stage of building a memory-enabled chain.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "  - **Problem Recap: Nested `message_log` Dictionary**: The lesson revisits the challenge where `RunnablePassthrough().assign(message_log=chat_memory.load_memory_variables)` produced a `message_log` value that was a dictionary (e.g., `{\"message_log\": \"summary_string\"}`) instead of the desired plain summary string.\n",
    "  - **Introducing `operator.itemgetter`**: Python's `itemgetter` function from the `operator` module is presented as a concise and efficient tool for fetching items from any object that supports the `__getitem__` dunder method, which includes dictionaries, lists, and strings.\n",
    "  - **`itemgetter` for Dictionary Value Extraction**: The utility of `itemgetter` is demonstrated, showing how `itemgetter(\"my_key\")(my_dict)` can directly retrieve the value associated with `\"my_key\"` from `my_dict`, or `itemgetter(index)(my_list)` for list elements.\n",
    "  - **Wrapping `itemgetter` with `RunnableLambda` for LCEL Integration**: To make `itemgetter` usable within an LCEL chain, it is wrapped using `RunnableLambda`. For instance, `RunnableLambda(itemgetter(\"key_to_extract\"))` creates a runnable component that performs this specific extraction task when invoked.\n",
    "  - **Solving the Nested Dictionary Issue**: The core solution involves piping the output of the `RunnablePassthrough().assign(...)` step (which generated `{\"question\": \"...\", \"message_log\": {\"message_log\": \"summary_string\"}}`) into a `RunnableLambda` that uses `itemgetter`. To get the final summary string from the nested structure, the lesson implies a sequence:\n",
    "    1.  A first `RunnableLambda(itemgetter('message_log'))` extracts the inner dictionary (e.g., `{\"message_log\": \"summary_string\"}`).\n",
    "    2.  A second `RunnableLambda(itemgetter('message_log'))` then extracts the actual summary string from this inner dictionary.\n",
    "  - **Preparing Correctly Formatted Input for Prompts**: By successfully applying this `itemgetter` technique, the conversation summary is extracted as a plain string. This makes the `message_log` data ready to be correctly combined with the user's `question` and fed into the prompt template, which expects a simple string for the summary.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "  - **Using `itemgetter` with `RunnableLambda` for Data Extraction in LCEL**\n",
    "    1.  **Why is this concept important?** In LCEL chains, data often flows as dictionaries. `itemgetter`, when wrapped in `RunnableLambda`, provides a declarative and efficient way to extract specific pieces of data from these dictionaries (or other indexable structures) without writing verbose custom lambda functions for simple lookups. This keeps the chain definition cleaner and focuses on *what* data to extract rather than *how*.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** Many LCEL components or external API calls might return complex dictionary structures. When subsequent components in a chain only need a subset of this data, or a specific nested value (like a session ID, a user preference, or in this case, a memory string), `RunnableLambda(itemgetter(...))` is a very practical tool for precise data selection to prepare inputs for the next step.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Python's `operator` module has other useful functions (like `attrgetter` for object attributes). Understanding how `RunnableLambda` works is key. For more complex data reshaping beyond simple extraction, exploring more advanced lambda functions within `RunnableLambda` or even custom `Runnable` classes might be necessary.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The lesson describes the following Python code snippets:\n",
    "\n",
    "1.  **Importing `itemgetter` and `RunnableLambda`**:\n",
    "\n",
    "    ```python\n",
    "    from operator import itemgetter\n",
    "    from langchain_core.runnables import RunnableLambda #, RunnablePassthrough\n",
    "    ```\n",
    "\n",
    "2.  **Basic Examples of `itemgetter`**:\n",
    "\n",
    "    ```python\n",
    "    # print(itemgetter(0)(\"hi\"))  # Output: 'h'\n",
    "    # print(itemgetter(2)([10, 20, 30, 40]))  # Output: 30\n",
    "    # print(itemgetter(\"message_log\")({\"message_log\": \"test_summary\"}))  # Output: 'test_summary'\n",
    "    ```\n",
    "\n",
    "3.  **Wrapping `itemgetter` in `RunnableLambda`**:\n",
    "\n",
    "    ```python\n",
    "    # runnable_extract_message_log = RunnableLambda(itemgetter(\"message_log\"))\n",
    "    # result = runnable_extract_message_log.invoke({\"message_log\": \"An empty string initially\"})\n",
    "    # print(result)  # Output: 'An empty string initially'\n",
    "    ```\n",
    "\n",
    "4.  **Conceptual Chain Segment for Extracting Nested `message_log`**:\n",
    "    This is the core application to solve the problem from the previous lesson.\n",
    "    Assume `chain_part_1` is the `RunnablePassthrough().assign(...)` from the previous lesson that outputs:\n",
    "    `{\"question\": \"User's question\", \"message_log\": {\"message_log\": \"Actual summary string\"}}`\n",
    "\n",
    "    The extraction process described involves:\n",
    "\n",
    "    ```python\n",
    "    # Step 1: Extract the inner dictionary associated with the outer 'message_log' key\n",
    "    # extract_inner_dict_runnable = RunnableLambda(itemgetter(\"message_log\"))\n",
    "\n",
    "    # Step 2: Extract the summary string from the 'message_log' key of the inner dictionary\n",
    "    # extract_summary_string_runnable = RunnableLambda(itemgetter(\"message_log\"))\n",
    "\n",
    "    # Conceptually, these would be piped:\n",
    "    # extracted_summary_string = (\n",
    "    #     chain_part_1\n",
    "    #     | extract_inner_dict_runnable # Output is {'message_log': 'Actual summary string'}\n",
    "    #     | extract_summary_string_runnable # Output is 'Actual summary string'\n",
    "    # )\n",
    "    # For example, if chain_part_1.invoke(...) yields the problematic dict:\n",
    "    # problematic_dict = {\n",
    "    #     \"question\": \"User's question\",\n",
    "    #     \"message_log\": {\"message_log\": \"Actual summary string\"}\n",
    "    # }\n",
    "    # inner_dict = extract_inner_dict_runnable.invoke(problematic_dict) # This would be {'message_log': 'Actual summary string'}\n",
    "    # final_summary = extract_summary_string_runnable.invoke(inner_dict) # This would be 'Actual summary string'\n",
    "\n",
    "    # The video seems to apply this to the output of:\n",
    "    # full_input_preparation_chain_segment = (\n",
    "    #     RunnablePassthrough().assign(message_log=chat_memory.load_memory_variables) # produces the nested dict\n",
    "    #     | RunnableLambda(itemgetter('message_log')) # extracts the inner dict {'message_log': 'summary'}\n",
    "    #     | RunnableLambda(itemgetter('message_log')) # extracts the final 'summary' string\n",
    "    # )\n",
    "    #\n",
    "    # This full_input_preparation_chain_segment would then be invoked with {\"question\": \"...\"}\n",
    "    # and its output would be the standalone summary string.\n",
    "    # How this string is then combined with the original \"question\" to form the final\n",
    "    # flat dictionary for the prompt is likely for the next lesson.\n",
    "    ```\n",
    "\n",
    "    The lesson's focus is on obtaining the clean string: \"We've successfully set the message log key to an empty string\" (or summary).\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1.  **Alternative:** Besides using two chained `RunnableLambda(itemgetter(...))` calls to extract the deeply nested summary string (from `{\"message_log\": {\"message_log\": \"summary_string\"}}` which is the value of the top-level `message_log` key), could you achieve the same extraction of the \"summary\\_string\" with a single `RunnableLambda` if you were operating on the dictionary that *contains* the nested structure? If so, what would its function look like?\n",
    "      - *Answer:* Yes, a single `RunnableLambda` could extract the deeply nested summary string. If the input to this `RunnableLambda` is the dictionary `data = {\"question\": \"...\", \"message_log\": {\"message_log\": \"summary_string\"}}`, the lambda function would be: `lambda data: data[\"message_log\"][\"message_log\"]`.\n",
    "2.  **Readability:** How does using `itemgetter` wrapped in `RunnableLambda` compare to writing a custom lambda function like `lambda x: x['key']` within a `RunnableLambda` in terms of readability and maintainability for simple, single-key extraction tasks?\n",
    "      - *Answer:* For simple, single-key extractions, `RunnableLambda(itemgetter('key'))` can be slightly more declarative and potentially more readable as it explicitly states the intent (\"get item 'key'\"). A custom `lambda x: x['key']` is also very readable and common in Python. Maintainability is similar for both; however, `itemgetter` might be marginally preferred if the project already uses the `operator` module extensively or adheres to a functional programming style.\n",
    "\n",
    "# Adding memory to a chain (Part 2): Creating the chain\n",
    "\n",
    "### Summary\n",
    "This comprehensive lesson culminates the series on LangChain Expression Language (LCEL) basics by demonstrating how to construct a complete, stateful conversational chain incorporating `ConversationSummaryMemory`. It meticulously builds this chain by first preparing the necessary inputs—the user's current question and the historical conversation summary—using a combination of `RunnablePassthrough.assign`, `RunnableLambda`, and `operator.itemgetter` to correctly fetch and format the memory. The lesson then shows how to pipe these prepared inputs through a prompt template, a chat model, and an output parser. Crucially, it details the subsequent step of saving the conversation context (both input and output) back to the memory object after each interaction. Finally, it illustrates how this entire cycle of input preparation, response generation, and memory update can be elegantly encapsulated into a single, reusable, memory-enabled runnable function using the `@chain` decorator.\n",
    "\n",
    "### Highlights\n",
    "-   **Objective: Building a Complete Stateful LCEL Chain**: The primary goal is to synthesize previously learned LCEL components (`RunnablePassthrough.assign`, `RunnableLambda`, `itemgetter`, prompt templates, models, parsers) to construct a fully functional conversational chain that maintains dialogue context using `ConversationSummaryMemory`.\n",
    "-   **Advanced Input Preparation for Memory**: A key focus is on correctly preparing the input dictionary for the prompt template, which requires both the current `question` and the `message_log` (conversation summary). This is achieved using a sophisticated `RunnablePassthrough.assign` setup:\n",
    "    `RunnablePassthrough.assign(message_log=RunnableLambda(chat_memory.load_memory_variables) | RunnableLambda(itemgetter('message_log')))`\n",
    "    This ensures that the `message_log` is dynamically fetched from the `chat_memory` object, the actual summary string is extracted from the dictionary returned by `load_memory_variables`, and then combined with the original `question` passed to the chain.\n",
    "-   **Step-by-Step Component Invocation for Clarity**: Before assembling the final chain, the lesson meticulously invokes each part of the process separately—input dictionary preparation, prompt template formatting, chat model call, output parsing, and manual memory update—to clearly trace the data flow and verify each step.\n",
    "-   **Explicitly Saving Conversation Context**: After the core chain generates a response, the `chat_memory.save_context(inputs={\"input\": question_string}, outputs={\"output\": response_string})` method is explicitly called. This step is critical for updating the `ConversationSummaryMemory` with the latest turn, enabling the chain to \"remember\" the interaction for subsequent turns.\n",
    "-   **Constructing the Core Response Generation Chain (`chain_one`)**: The main response generation logic is defined as an LCEL chain:\n",
    "    `chain_one = input_preparation_runnable | prompt_template | chat_model | string_output_parser`\n",
    "    It's important to note that the memory saving operation (`save_context`) is performed *after* this chain is invoked.\n",
    "-   **Testing and Verifying Conversational Memory**: The effectiveness of the memory integration is demonstrated by posing an initial question, saving the context, and then asking a follow-up question. The chatbot's ability to provide a contextually relevant elaboration (e.g., \"elaborate a bit more on this fact\") confirms that the memory mechanism is functioning correctly.\n",
    "-   **Encapsulating Stateful Logic with the `@chain` Decorator**: The entire multi-step process (prepare inputs with memory, invoke core chain for response, save context to memory, return response) is encapsulated within a single Python function (e.g., `memory_chain(question_param)`). This function is then transformed into a complete, standalone runnable object using the `@chain` decorator.\n",
    "-   **Creating a Reusable, Memory-Enabled Runnable Function**: The decorated `memory_chain` function becomes a self-contained, memory-enabled runnable. It internally handles the complexities of fetching from memory, generating a response, and updating memory, offering a clean interface that can be invoked simply with the user's question.\n",
    "-   **Importance of Clearing Memory During Development/Testing**: The lesson advises users to clear the `chat_memory` object (e.g., by re-initializing it) before testing new or distinct conversation flows to ensure a fresh memory state and avoid interference from previous interactions.\n",
    "-   **Holistic LCEL Workflow**: This lesson provides a holistic view of building sophisticated LCEL applications by tying together concepts of dynamic input processing, fetching data from stateful objects (memory), sequential execution, output parsing, and managing state updates across interactions.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Decoupling Chain Execution and Memory Updates in Stateful Applications**\n",
    "    1.  **Why is this concept important?** Separating the core response generation logic (the main LCEL chain like `chain_one`) from the memory update logic (`save_context`) provides clarity and modularity. The main chain focuses solely on producing a response based on current inputs. The memory update is a distinct side effect that happens *after* the response is obtained. This separation makes the system easier to reason about, test, and modify.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In complex conversational AI, you might want to perform actions between getting a response and saving it to memory (e.g., logging, sentiment analysis on the AI's response, filtering sensitive data before it's summarized). If memory saving were deeply embedded in the generation chain, these intermediate steps would be harder to insert. The pattern shown (generate response, then explicitly save context) is common in stateful systems. Encapsulating this entire sequence in a higher-level function (like the `@chain` decorated `memory_chain`) then provides a clean, high-level runnable for the application to use.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** State management patterns in software engineering, event-driven architectures (where a response event might trigger a memory update event), and functional programming concepts (side effects management) are all related. Within LangChain, understanding different memory types and their `save_context`/`load_memory_variables` methods is crucial. Also, exploring LangChain Callbacks can offer more sophisticated ways to hook into different stages of the chain execution for tasks like logging or memory updates if the explicit call pattern becomes cumbersome for very complex scenarios.\n",
    "\n",
    "### Code Examples\n",
    "The lesson walks through building a complete memory-enabled chain:\n",
    "\n",
    "1.  **Full Setup (Imports and Initial Objects)**:\n",
    "    ```python\n",
    "    # (Magic command for API key)\n",
    "    # from langchain_core.prompts import ChatPromptTemplate\n",
    "    # from langchain_openai import ChatOpenAI\n",
    "    # from langchain_core.output_parsers import StrOutputParser\n",
    "    # from langchain.memory import ConversationSummaryMemory\n",
    "    # from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "    # from operator import itemgetter\n",
    "\n",
    "    # template_string = \"\"\"... {message_log} ... Human: {question} ... AI: \"\"\"\n",
    "    # prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "    # chat_model = ChatOpenAI(max_tokens=500) # Example setting\n",
    "    # chat_memory = ConversationSummaryMemory(llm=chat_model, memory_key=\"message_log\", return_messages=False)\n",
    "    # string_parser = StrOutputParser()\n",
    "    ```\n",
    "\n",
    "2.  **Input Preparation Runnable (Combining `RunnablePassthrough.assign`, `RunnableLambda`, `itemgetter`)**:\n",
    "    This runnable takes a dictionary like `{\"question\": \"user_question_string\"}` and outputs an enriched dictionary `{\"question\": \"user_question_string\", \"message_log\": \"conversation_summary_string\"}`.\n",
    "    ```python\n",
    "    # input_preparer = RunnablePassthrough.assign(\n",
    "    #     message_log=RunnableLambda(chat_memory.load_memory_variables) | RunnableLambda(itemgetter('message_log'))\n",
    "    # )\n",
    "    ```\n",
    "    *Self-correction based on video context: `load_memory_variables` usually takes an empty dict or the current input. If the input to `assign` is `{\"question\": ...}`, then `RunnableLambda(chat_memory.load_memory_variables)` will receive this. To ensure it gets an empty dict if needed by a specific memory type, it might be `RunnableLambda(lambda x: chat_memory.load_memory_variables({}))`. However, `load_memory_variables` is often designed to accept the current main inputs if relevant.*\n",
    "    *The video implies `load_memory_variables` is called and then `itemgetter` acts on its direct output, which is `{\"message_log\": \"summary\"}`. So the pipe is correct.*\n",
    "\n",
    "3.  **Defining the Core Response Generation Chain (`chain_one`)**:\n",
    "    ```python\n",
    "    # chain_one = input_preparer | prompt_template | chat_model | string_parser\n",
    "    ```\n",
    "\n",
    "4.  **Invoking the Chain and Saving Context (Manual Approach)**:\n",
    "    ```python\n",
    "    # user_question = \"Can you give me an interesting fact I probably didn't know about?\"\n",
    "    #\n",
    "    # # To ensure fresh memory for a new conversation test run:\n",
    "    # # chat_memory = ConversationSummaryMemory(llm=chat_model, memory_key=\"message_log\", return_messages=False)\n",
    "    #\n",
    "    # response = chain_one.invoke({\"question\": user_question})\n",
    "    # print(f\"AI: {response}\")\n",
    "    #\n",
    "    # chat_memory.save_context(inputs={\"input\": user_question}, outputs={\"output\": response})\n",
    "    #\n",
    "    # print(f\"Memory: {chat_memory.load_memory_variables({})}\")\n",
    "    #\n",
    "    # # Follow-up question\n",
    "    # follow_up_question = \"Can you elaborate a bit more on this fact?\"\n",
    "    # response_follow_up = chain_one.invoke({\"question\": follow_up_question})\n",
    "    # print(f\"AI: {response_follow_up}\")\n",
    "    #\n",
    "    # chat_memory.save_context(inputs={\"input\": follow_up_question}, outputs={\"output\": response_follow_up})\n",
    "    # print(f\"Memory: {chat_memory.load_memory_variables({})}\")\n",
    "    ```\n",
    "\n",
    "5.  **Defining and Using the `@chain` Decorated Function (`memory_chain`)**:\n",
    "    ```python\n",
    "    from langchain_core.runnables import chain as runnable_decorator # Assuming decorator import\n",
    "\n",
    "    # @runnable_decorator # Using an alias to avoid conflict if 'chain' variable is used\n",
    "    # def memory_chain(inputs_dict): # Function now takes a dictionary\n",
    "    #     # The 'chain_one' definition would be inside or accessible here.\n",
    "    #     # Or, redefine the full pipeline here for encapsulation.\n",
    "    #     # The input 'inputs_dict' should contain the 'question'.\n",
    "    #     user_question_from_input = inputs_dict[\"question\"]\n",
    "    #\n",
    "    #     # For clarity, redefine chain_one's logic or use it if defined globally and chat_memory is accessible\n",
    "    #     # This assumes chat_memory is accessible in this scope (e.g., global or passed in)\n",
    "    #     _input_preparer = RunnablePassthrough.assign(\n",
    "    #         message_log=RunnableLambda(chat_memory.load_memory_variables) | RunnableLambda(itemgetter('message_log'))\n",
    "    #     )\n",
    "    #     _chain_one = _input_preparer | prompt_template | chat_model | string_parser\n",
    "    #\n",
    "    #     response_content = _chain_one.invoke({\"question\": user_question_from_input})\n",
    "    #     chat_memory.save_context(inputs={\"input\": user_question_from_input}, outputs={\"output\": response_content})\n",
    "    #     return response_content\n",
    "\n",
    "    # Invocation:\n",
    "    # chat_memory.clear() # Or re-initialize for a fresh start\n",
    "    # first_response = memory_chain.invoke({\"question\": \"Tell me a fact.\"})\n",
    "    # print(first_response)\n",
    "    # second_response = memory_chain.invoke({\"question\": \"More about that.\"})\n",
    "    # print(second_response)\n",
    "    ```\n",
    "    *Video Correction: The decorated function took `question` as a direct parameter, not a dictionary.*\n",
    "    ```python\n",
    "    # @runnable_decorator\n",
    "    # def memory_chain_from_video(question_string: str) -> str:\n",
    "    #     # Assume chat_memory, prompt_template, chat_model, string_parser are in scope\n",
    "    #     _input_preparer = RunnablePassthrough.assign(\n",
    "    #         message_log=RunnableLambda(chat_memory.load_memory_variables) | RunnableLambda(itemgetter('message_log'))\n",
    "    #     )\n",
    "    #     # The _input_preparer expects a dict, so we form it first\n",
    "    #     _response_chain = _input_preparer | prompt_template | chat_model | string_parser\n",
    "    #\n",
    "    #     # The chain needs initial input {\"question\": question_string}\n",
    "    #     # The input to the decorated function is just the question string.\n",
    "    #     # The decorator will handle how .invoke() on memory_chain_from_video works with this.\n",
    "    #     # Typically, if the function takes question_string, invoke would need {\"question_string\": \"value\"}\n",
    "    #     # or the decorator/RunnableLambda adapts it. The video likely passes the string directly\n",
    "    #     # to an outer chain that prepares the dict.\n",
    "    #     # Let's follow the video's more direct function body:\n",
    "    #     # (Copy-pasting from cell above in video implies the chain_one definition is used)\n",
    "    #     # Assuming chain_one is defined as:\n",
    "    #     # chain_one = RunnablePassthrough.assign(\n",
    "    #     #     message_log=RunnableLambda(chat_memory.load_memory_variables) | RunnableLambda(itemgetter('message_log'))\n",
    "    #     # ) | prompt_template | chat_model | string_parser\n",
    "    #\n",
    "    #     response = chain_one.invoke({\"question\": question_string}) # chain_one needs a dict\n",
    "    #     chat_memory.save_context(inputs={\"input\": question_string}, outputs={\"output\": response})\n",
    "    #     return response\n",
    "    #\n",
    "    # # Invocation as per video:\n",
    "    # # first_response = memory_chain_from_video.invoke(\"Tell me a fact.\")\n",
    "    # # This implies the decorator handles mapping the direct string arg to the dict for chain_one.\n",
    "    # # More precisely, the .invoke() method of the resulting RunnableLambda might expect a dictionary\n",
    "    # # whose key matches the function parameter name.\n",
    "    # # e.g. memory_chain_from_video.invoke({\"question_string\": \"Tell me a fact.\"})\n",
    "    # # The video's direct call `memory_chain.invoke(\"Our favorite question\")` suggests the\n",
    "    # # decorator might be smart or the underlying RunnableLambda handles single-argument functions specially.\n",
    "    # # Or, more likely, the `invoke` call in the video for the decorated function was\n",
    "    # # memory_chain.invoke({\"question\": \"Our favorite question\"})\n",
    "    # # and the function definition was def memory_chain(input_dict): question = input_dict['question'] ...\n",
    "    # # Given the video says \"set question as a required parameter\", the function signature is likely `def memory_chain(question: str):`\n",
    "    # # The most robust way a `@chain` decorator makes `def my_func(param1): ...` callable via\n",
    "    # # `my_func.invoke({\"param1\": value})` is standard.\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Design Pattern:** Why is it a common and beneficial design pattern to separate the core response generation chain (like `chain_one` in the lesson) from the memory update step (`save_context`), and then often encapsulate both within a higher-level function or runnable (like the decorated `memory_chain`)?\n",
    "    -   *Answer:* Separating these concerns improves modularity and testability. The core chain can be tested for its response generation capabilities independently of memory. The memory update is a distinct side effect. Encapsulating them together in a higher-level runnable provides a clean, single interface for stateful interaction, hiding the two-step complexity (invoke, then save) from the end-user of that runnable.\n",
    "2.  **Extensibility:** If you wanted to add a step *after* the AI responds (output from `chain_one`) but *before* saving the context to memory (e.g., to log the raw AI output for analysis or perform a quick sentiment check on the AI's response), where would you insert this logic in the context of the decorated `memory_chain` function shown in the lesson?\n",
    "    -   *Answer:* You would insert this logic inside the `memory_chain` function, right after the line `response = chain_one.invoke({\"question\": question_string})` and before the line `chat_memory.save_context(...)`. This allows you to access the `response` variable directly to perform logging or analysis before it's committed to the conversation summary.\n",
    "3.  **Alternative Memory Handling:** Could `RunnableLambda` be used to directly integrate the `chat_memory.save_context` call as the *final step* within the definition of `chain_one` itself, making `chain_one` inherently stateful in its definition? What might be the pros and cons of such an approach?\n",
    "    -   *Answer:* Yes, `RunnableLambda` could wrap a function that first gets the response and then calls `save_context`. For example: `final_step = RunnableLambda(lambda x: (response := previous_steps_output(x), chat_memory.save_context(inputs={\"input\": x[\"question\"]}, outputs={\"output\": response}), response)[-1])`.\n",
    "        **Pros:** `chain_one` becomes fully self-contained regarding memory updates.\n",
    "        **Cons:** This makes the chain less pure, as it now has a significant side effect (modifying memory) directly embedded. It might be harder to reason about the chain's output if it also implicitly modifies state. It also makes it harder to access the `response` for other purposes *before* it's saved to memory if the lambda only returns the response. Testing the core logic without triggering memory updates also becomes more complex."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
