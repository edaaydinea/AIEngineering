{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39920ff1",
   "metadata": {},
   "source": [
    "# What does GPT mean?\n",
    "\n",
    "### Summary\n",
    "\n",
    "ChatGPT, a state-of-the-art Large Language Model (LLM) developed by OpenAI, has significantly impacted Natural Language Processing (NLP). Its name, Generative Pre-trained Transformer, reflects its core capabilities. The model excels at generating human-like text, continuing prompts coherently, and performing tasks like text completion and dialogue generation due to its pre-training on vast amounts of text data and its transformer architecture.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🤖 ChatGPT is a powerful LLM from OpenAI that has revolutionized NLP.\n",
    "- ✍️ It is \"generative\" because it can produce human-like text from prompts.\n",
    "- 🗣️ ChatGPT can continue text in a contextually relevant and coherent way.\n",
    "- 🛠️ This capability makes it highly useful for text completion, dialogue, and content creation.\n",
    "- 📚 The \"pre-trained\" aspect involves learning from extensive text data, gaining deep language understanding.\n",
    "- 🧠 The \"transformer\" in its name refers to the underlying transformer architecture.\n",
    "- 📈 Numerous versions of GPT models have been developed over time.\n",
    "\n",
    "# The development of ChatGPT\n",
    "\n",
    "### Summary\n",
    "\n",
    "The evolution of GPT models showcases a significant progression in the scale, performance, and applications of large language models. Starting with GPT-1 in 2018 with 117 million parameters, the models have grown exponentially, reaching GPT-4 in 2023 with an astounding 1 trillion parameters. Each iteration brought improvements in text generation coherence, contextual understanding, and new capabilities like few-shot learning. ChatGPT is a specific version of a GPT model fine-tuned for natural and engaging conversational interactions, optimized for chatbot and virtual assistant roles. OpenAI provides an API that simplifies the process of interacting with these models for custom solutions.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🚀 The development of GPT models marks a significant leap in LLM evolution.\n",
    "- 🗓️ GPT-1, introduced in 2018, had 117 million parameters and was trained on 40GB of internet text.\n",
    "- 📈 GPT-2 (2019) dramatically increased the scale to 1.5 billion parameters, showing improved coherence.\n",
    "- 🌐 GPT-3 (2020) boasted 175 billion parameters and introduced few-shot learning.\n",
    "- ⚙️ GPT-4 (2023) reached 1 trillion parameters, representing a peak in OpenAI's LLM research at the time.\n",
    "- 💬 ChatGPT is a GPT model specifically fine-tuned for natural and engaging conversational interactions.\n",
    "- 🔑 OpenAI offers an API that makes it easy to interact with GPT models for custom applications.\n",
    "\n",
    "# OpenAI API\n",
    "\n",
    "### Summary\n",
    "\n",
    "To begin using the OpenAI API, you first need to create an API key on the OpenAI platform website. This involves logging in or creating an account, navigating to account settings, and then to the API keys section where you can generate a new secret key. It's crucial to store this key securely as it won't be visible again. You can then use this key in your code directly or, for better security, store it in a configuration file and reference it in your scripts. After importing the OpenAI package and setting your API key, you can initialize the OpenAI client. Different OpenAI models available through the API are priced per 1000 tokens, with more recent models typically being more expensive.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🔑 Creating an API key on the OpenAI platform website is the first step to use the API.\n",
    "- 🔒 The generated secret API key should be stored securely as it cannot be viewed again.\n",
    "- ⚙️ The API key can be directly used in code or stored in a config file for better management.\n",
    "- 🐍 After importing the OpenAI package, you can initialize the client using your API key.\n",
    "- 💲 OpenAI models are priced per 1000 tokens, with newer models generally costing more.\n",
    "- 🌐 Check the OpenAI website for the pricing details of different models before usage.\n",
    "- 🚀 With the API key set up, you can start interacting with the OpenAI models.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- 💾 Creating a `config.py` file to store the API key:\n",
    "    \n",
    "    ```python\n",
    "    API_KEY = \"your_secret_api_key_here\"\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- 💻 Importing necessary packages and setting the API key in your notebook:\n",
    "    \n",
    "    ```python\n",
    "    import openai\n",
    "    import config\n",
    "    \n",
    "    openai.api_key = config.API_KEY\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- 🚀 Initializing the OpenAI client (note: the `openai.api_key` assignment directly also serves this purpose):\n",
    "    \n",
    "    ```python\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    ```\n",
    "    \n",
    "\n",
    "# Generating text\n",
    "\n",
    "### Summary\n",
    "\n",
    "You can test ChatGPT's generative capabilities by creating a Python function that uses the OpenAI API to generate text. This function, named `generate_text`, takes a prompt as input, which is the text you provide to the model for completion, answering, or following instructions. Inside the function, the `openai.Completion.create` method is called, specifying the desired model (e.g., 'davinci-002'), the input prompt, and other parameters like `max_tokens` and `temperature`. The function then returns the generated text after processing the API response. By providing a starting phrase like \"Once upon a time\", you can see the model generate a continuation of the sentence. This demonstrates how to interact with the GPT model for text generation with just a few lines of code.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🧪 You can test the generative nature of GPT by creating a function for text generation.\n",
    "- 🖋️ The `generate_text` function takes a 'prompt' as input, which guides the model.\n",
    "- ⚙️ Inside the function, `openai.Completion.create` is used to interact with the OpenAI model.\n",
    "- 🧠 You can specify the model to use (e.g., 'davinci-002') within the function call.\n",
    "- 🌡️ Parameters like `max_tokens` and `temperature` influence the output (explained in the next lesson).\n",
    "- 📤 The function returns the generated text after extracting and formatting the API response.\n",
    "- 📜 By providing a prompt like \"Once upon a time\", you can observe the model generating a continuation.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- 💻 Defining the `generate_text` function:\n",
    "    \n",
    "    ```python\n",
    "    import openai\n",
    "    \n",
    "    def generate_text(prompt):\n",
    "        response = openai.Completion.create(\n",
    "            model=\"davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=50,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- 📝 Creating a prompt and generating text:\n",
    "    \n",
    "    ```python\n",
    "    prompt=\"Once upon a time\"\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\")\n",
    "    ```\n",
    "    \n",
    "\n",
    "# Customizing GPT output\n",
    "\n",
    "### Summary\n",
    "\n",
    "The `max_tokens` and `temperature` arguments in the OpenAI API's text generation function offer ways to control the output. `max_tokens` determines the length of the generated text; a higher value allows for longer responses. `temperature` controls the randomness, with values closer to zero resulting in more deterministic and predictable output, while values closer to one produce more random and potentially creative responses. Experimenting with these parameters can significantly alter the model's output, influencing its length and creativity. The OpenAI API playground provides a user-friendly interface to test different models and settings.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 📏 The `max_tokens` argument controls the length of the text generated by the model.\n",
    "- 🌡️ The `temperature` argument influences the randomness of the model's output.\n",
    "- 🎲 A `temperature` of 0 leads to more focused and deterministic responses.\n",
    "- 🤪 A `temperature` of 1 results in more random and creative, but potentially less coherent, output.\n",
    "- ⚙️ You can set `max_tokens` and `temperature` as arguments in your text generation function for flexibility.\n",
    "- 🧪 Experimenting with different values for these arguments can yield diverse results.\n",
    "- 🕹️ The OpenAI API playground allows you to visually test various models and settings.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "- 💻 Modifying the `generate_text` function to accept `max_tokens` and `temperature` as arguments:\n",
    "    \n",
    "    ```python\n",
    "    import openai\n",
    "    \n",
    "    def generate_text(prompt, max_tokens=50, temperature=0.7):\n",
    "        response = openai.Completion.create(\n",
    "            model=\"davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- 📝 Generating text with different `max_tokens` values:\n",
    "    \n",
    "    ```python\n",
    "    prompt = \"Once upon a time\"\n",
    "    generated_text_short = generate_text(prompt, max_tokens=5)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text (5 tokens): {generated_text_short}\")\n",
    "    \n",
    "    generated_text_long = generate_text(prompt, max_tokens=20)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text (20 tokens): {generated_text_long}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- 🔥 Generating text with different `temperature` values:\n",
    "    \n",
    "    ```python\n",
    "    prompt = \"Once upon a time\"\n",
    "    generated_text_temp_low = generate_text(prompt, max_tokens=50, temperature=0)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text (temperature=0): {generated_text_temp_low}\")\n",
    "    \n",
    "    generated_text_temp_high = generate_text(prompt, max_tokens=50, temperature=1)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated Text (temperature=1): {generated_text_high}\")\n",
    "    ```\n",
    "    \n",
    "\n",
    "# Key word text summarization\n",
    "\n",
    "### Summary\n",
    "\n",
    "You can create a function to summarize large text by extracting key words using the OpenAI API's chat completion feature. This involves providing the model with messages defining the task and demonstrating the desired output through system, user, and assistant roles. The system message gives instructions, user messages provide input examples (the text to summarize), and assistant messages show the expected output (key words). By providing examples, you guide the model on how to extract relevant terms from new input text. The final user message contains the actual text you want to summarize, and the function returns the model's extracted keywords.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🔑 A function can be created to extract key words from text using the OpenAI API.\n",
    "- 💬 The chat completion endpoint uses 'messages' with different roles: system, user, and assistant.\n",
    "- ⚙️ System messages instruct the model on the task and desired behavior.\n",
    "- 👤 User messages provide input, such as the text to be summarized.\n",
    "- 🤖 Assistant messages offer examples of the expected output (key words).\n",
    "- 📝 By providing example user and assistant message pairs, you demonstrate the desired summarization style.\n",
    "- 📤 The final user message contains the text you want the model to summarize, and the output will be the extracted key words.\n",
    "\n",
    "# Coding a simple chatbot\n",
    "\n",
    "### Summary\n",
    "\n",
    "By leveraging system messages and providing examples through user and assistant roles, you can create a chatbot with a specific persona, such as one that responds poetically. The system message sets the overall behavior (\"You are a poetic chatbot.\"). User messages provide example questions, and corresponding assistant messages demonstrate the desired poetic responses. This teaches the model the kind of output expected. The final user message contains the actual prompt for the chatbot. By adjusting parameters like temperature, you can influence the creativity of the poetic responses. This method allows for the creation of chatbots with unique and engaging personalities.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🎭 System messages can define a specific persona for a chatbot, like a poetic one.\n",
    "- ✍️ User and assistant message pairs demonstrate the desired style of responses.\n",
    "- ❓ Example user questions are provided to guide the chatbot's learning.\n",
    "- 📜 Corresponding assistant messages showcase poetic answers to these questions.\n",
    "- 🧠 The model learns to respond in a poetic style based on these examples.\n",
    "- 🗣️ The final user message is the actual prompt given to the poetic chatbot.\n",
    "- 🌡️ Adjusting the temperature can control the level of creativity in the poetic responses.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "def poetic_chatbot(prompt):\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a poetic chatbot.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"When was Google founded?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"In the late '90s, a spark did ignite, Google emerged, a radiant light. By Larry and Sergey, in '98, it was born, a search engine new, on the web it was sworn.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Which country has the youngest president?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Ah, the pursuit of youth in politics, a theme we explore. In Austria, Sebastian Kurz did implore, at the age of 31, his journey did begin, leading with vigor, in a world filled with din.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "        \n",
    "prompt = \"When was cheese first made?\"\n",
    "response = poetic_chatbot(prompt)\n",
    "print(\"Poetic Response:\", response)\n",
    "```\n",
    "\n",
    "<aside>\n",
    "💡\n",
    "\n",
    "Poetic Response: Oh, cheese, a gift both savory and old,\n",
    "\n",
    "Its origins shrouded in stories untold.\n",
    "\n",
    "Around 8000 years, give or take a few,\n",
    "\n",
    "When sheep were first milked, a craft born anew.\n",
    "\n",
    "Through curdling milk, by chance it was found,\n",
    "\n",
    "With rennet or acid, the curds would abound.\n",
    "\n",
    "In ancient Mesopotamia, or so they say,\n",
    "\n",
    "Cheese began its journey to modern day.\n",
    "\n",
    "</aside>\n",
    "\n",
    "# Introduction to LangChain in Python\n",
    "\n",
    "### Summary\n",
    "\n",
    "While the poetic chatbot created earlier can generate creative responses, its knowledge is limited to the data it was trained on, which has a cut-off point (around 2021). This means it cannot answer questions requiring up-to-date or specific information not present in its training data, such as future course releases on a particular platform. However, Langchain offers a solution by allowing language models to access and utilize external, user-provided data when generating responses, thus overcoming the limitations of the model's pre-existing knowledge.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🧠 The knowledge of standard GPT models is limited to their training data, which has a cut-off date.\n",
    "- ❓ Asking specific or time-sensitive questions beyond this data range will likely result in vague or unhelpful answers.\n",
    "- 📅 The poetic chatbot, while creative, cannot access real-time or proprietary information.\n",
    "- 💡 Langchain provides a way to augment language models with external data.\n",
    "- 📂 By importing your own data into Langchain, you can enable language models to reference it.\n",
    "- 🔗 This allows the models to generate responses based on information beyond their original training set.\n",
    "- 🚀 Langchain overcomes the knowledge limitations of standard language models for specific use cases.\n",
    "\n",
    "# LangChain\n",
    "\n",
    "### Summary\n",
    "\n",
    "LangChain is an open-source framework designed to connect large language models (LLMs) with external data and computation, enabling developers to build powerful, end-to-end applications. Its primary value lies in allowing LLMs to access and utilize custom or business-specific data that they were not originally trained on.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🚀 Open-source framework for combining LLMs with external data and computation.\n",
    "- ✨ Enables building end-to-end applications powered by language models.\n",
    "- 📚 Allows LLMs to access and understand data outside their training set, like course syllabi or business-specific information.\n",
    "- ➡️ Facilitates integration of custom data by loading, breaking it into chunks, and creating numerical embeddings.\n",
    "- 🔍 Uses vector stores to efficiently find similar text chunks based on their embeddings.\n",
    "- 📧 Supports follow-up actions based on LLM responses, such as sending emails.\n",
    "- 📈 Popularity surged in 2023 due to the evolution of LLMs and the need for easy integration.\n",
    "\n",
    "# Adding custom data to our chatbot\n",
    "\n",
    "### Summary\n",
    "\n",
    "The process for integrating custom web data with a large language model using LangChain involves loading the data from a specified URL, splitting it into smaller, manageable chunks, generating numerical embeddings for each chunk, storing these embeddings in a vector store for efficient retrieval, setting up a memory object to maintain conversation history, and finally initializing a conversational retrieval chain that uses these components to answer questions based on the custom data.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 📄 Use LangChain data loaders like `WebBaseLoader` to load information from various sources, such as websites, PDFs, and text files.\n",
    "- ✂️ Split the loaded text into smaller chunks using text splitters like `RecursiveCharacterTextSplitter` to suit language model input limits.\n",
    "- 🔢 Create numerical embeddings for each text chunk using embedding models, such as `OpenAIEmbeddings`, specifying an API key.\n",
    "- 💾 Store the text chunks and their embeddings in a vector store using methods like `from_documents` for quick similarity search.\n",
    "- 🧠 Implement conversation memory using objects like `ConversationBufferMemory` to track chat history for the model.\n",
    "- 🔗 Initialize a `ConversationalRetrievalChain` using `from_llm`, connecting the language model, retriever (vector store), and memory.\n",
    "- 🤔 The chain processes questions by considering chat history, retrieving relevant documents from the vector store, and passing them to a question-answering model.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Load data from a URL\n",
    "loader = WebBaseLoader(\"specified_url\")\n",
    "raw_documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(...) # Configure splitter parameters\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"your_api_key\")\n",
    "\n",
    "# Create and populate vector store\n",
    "vector_store = VectorStore.from_documents(documents, embeddings) # Replace VectorStore with actual class like Chroma, FAISS, etc.\n",
    "\n",
    "# Create memory object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Initialize conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=OpenAI(openai_api_key=\"your_api_key\", temperature=...), # Replace OpenAI with your chosen LLM\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Query the chain\n",
    "result = qa_chain({\"question\": \"your_query\"})\n",
    "answer = result['answer']\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
