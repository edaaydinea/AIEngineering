{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is topic modeling?\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section introduces topic modeling, an unsupervised machine learning technique used to identify patterns and group similar documents based on their content. It explains how topic modeling algorithms analyze text data to discover underlying themes and provides a practical example to illustrate the concept.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🔍 Topic modeling identifies patterns and groups similar documents.\n",
    "- 🤖 It's an unsupervised learning technique, requiring no labeled data.\n",
    "- 📝 Algorithms detect word patterns to determine document themes.\n",
    "- 🧩 Grouping documents based on shared themes simplifies large text datasets.\n",
    "- 💡 Example themes include equipment, media, and government regulations.\n",
    "- 🚀 Topic modeling automates the identification of key themes in text.\n",
    "- 🧠 It can uncover patterns that humans might miss in large datasets.\n",
    "- 🛠️ Two common algorithms are Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA).\n",
    "\n",
    "# When to use topic modeling?\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section provides practical examples of how topic modeling can be applied in various real-world scenarios, such as organizing news articles, analyzing customer feedback, and conducting social listening. It emphasizes the efficiency and time-saving benefits of using topic modeling to extract key themes from large volumes of text data.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 📰 Grouping news articles or research papers under relevant topics.\n",
    "- 🗣️ Analyzing customer feedback and reviews to understand consumer sentiment.\n",
    "- 📱 Monitoring social media data for brand-related discussions.\n",
    "- ⏱️ Topic modeling automates the process, saving time and effort.\n",
    "- 🤖 Identifying key themes in text data more efficiently than manual methods.\n",
    "- 📊 Organizing large datasets of text into meaningful categories.\n",
    "- 💡 Discovering insights and patterns that may be missed by human analysis.\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section explains the Latent Dirichlet Allocation (LDA) algorithm, a technique used for topic modeling. It details how LDA identifies latent topics within documents by analyzing word frequencies and distributions. The algorithm iteratively assigns words to topics, refining these assignments until a stable set of topics is achieved.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🧩 LDA identifies latent topics by analyzing word patterns in documents.\n",
    "- 📝 Documents are assumed to primarily focus on a single topic, with some words from others.\n",
    "- 🔄 LDA uses an iterative process to refine topic assignments.\n",
    "- 📊 The algorithm considers word proportions within documents and across the corpus.\n",
    "- 🔢 The number of topics (k) is pre-defined.\n",
    "- 🎲 Initial word assignments are random, but refined over iterations.\n",
    "- 💡 The algorithm reaches a steady state, providing final topic assignments.\n",
    "\n",
    "# LDA in Python\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section demonstrates how to implement Latent Dirichlet Allocation (LDA) for topic modeling using the Gensim library in Python. It covers the steps from data loading and preprocessing to model training and topic interpretation.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🐍 Utilizing Gensim for LDA topic modeling.\n",
    "- 📝 Preprocessing text data: removing punctuation, lowercasing, stop words, tokenizing, and stemming.\n",
    "- 📚 Creating a dictionary of unique words and a document-term matrix.\n",
    "- 🤖 Training the LDA model with specified parameters.\n",
    "- 📊 Printing and interpreting the top words for each identified topic.\n",
    "- 🛠️ Adjusting the number of topics or refining data preprocessing for better results.\n",
    "- 💡 Exploring alternative topic modeling methods for potentially more informative outcomes.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"news_articles.csv\")\n",
    "articles = data['content'].tolist()\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_articles = [preprocess(article) for article in articles]\n",
    "\n",
    "# Create dictionary and document-term matrix\n",
    "dictionary = corpora.Dictionary(processed_articles)\n",
    "doc_term_matrix = [dictionary.doc2bow(article) for article in processed_articles]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 2\n",
    "lda_model = LdaModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# Print topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "```\n",
    "\n",
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section introduces Latent Semantic Analysis (LSA), a topic modeling technique based on the distributional hypothesis and Singular Value Decomposition (SVD). It explains how LSA transforms text documents into vector representations, enabling the identification of similar words and documents.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🧠 LSA relies on the distributional hypothesis: words with similar meanings appear together.\n",
    "- 🔢 Singular Value Decomposition (SVD) is used for dimensionality reduction.\n",
    "- 📊 SVD transforms the document-term matrix into document-topic and term-topic matrices.\n",
    "- 📉 LSA vectors capture different aspects of meaning in the text.\n",
    "- 💡 LSA can identify similar words and documents through clustering and similarity scores.\n",
    "- 🧮 SVD helps in understanding the variance explained by each latent topic.\n",
    "- 🛠️ LSA provides a structured approach to analyze and interpret text data.\n",
    "\n",
    "# LSA in Python\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section demonstrates how to implement Latent Semantic Analysis (LSA) for topic modeling using the Gensim library. It highlights the similarities and differences between LSA and LDA, and shows how to interpret the resulting topics.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 🐍 Utilizing Gensim's `LsiModel` for LSA topic modeling.\n",
    "- 🔄 LSI and LSA are interchangeable terms in topic modeling.\n",
    "- 📊 Comparing LSA and LDA results, noting slight differences in topic composition.\n",
    "- 📝 Interpreting the most important words for each topic generated by LSA.\n",
    "- 🛠️ Preparing for the next step: optimizing the number of topics for better results.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "# Assuming doc_term_matrix, dictionary, and num_topics are already defined\n",
    "\n",
    "# Train LSA model\n",
    "lsa_model = LsiModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# Print topics\n",
    "topics = lsa_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "```\n",
    "\n",
    "# How many topics?\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section explains how to determine the optimal number of topics for LSA using coherence scores. It demonstrates how to iterate through different numbers of topics, calculate coherence scores, and visualize them to identify the best number of topics. It also emphasizes the importance of considering business context and intuition alongside mathematical metrics.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- 📊 Coherence scores help determine the optimal number of topics.\n",
    "- 🐍 Utilizing matplotlib for visualizing coherence scores.\n",
    "- 🔄 Iterating through different numbers of topics to calculate coherence.\n",
    "- 📈 Identifying the number of topics with the highest coherence score.\n",
    "- 🤖 Training the final LSA model with the optimal number of topics.\n",
    "- 🧠 Balancing mathematical accuracy with business context and intuition.\n",
    "- 💡 Manually inspecting topics to ensure they are meaningful and coherent.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Assuming doc_term_matrix, dictionary, and articles are already defined\n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "for num_topics in range(min_topics, max_topics + 1):\n",
    "    model = LsiModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=num_topics)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(model=model, texts=articles, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "# Plot coherence scores\n",
    "plt.plot(range(min_topics, max_topics + 1), coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.show()\n",
    "\n",
    "# Train final LSA model with optimal number of topics\n",
    "final_num_topics = 3  # Based on the plot\n",
    "final_lsa_model = LsiModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=final_num_topics)\n",
    "\n",
    "# Print final topics\n",
    "final_topics = final_lsa_model.print_topics(num_words=10)\n",
    "for topic in final_topics:\n",
    "    print(topic)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
