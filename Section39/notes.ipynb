{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8b6ed3",
   "metadata": {},
   "source": [
    "# The LangChain framework\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This video introduces the LangChain framework as a pivotal tool for building advanced LLM-powered applications that are stateful, context-aware, and capable of reasoning, moving beyond basic OpenAI API interactions. It provides a conceptual overview of LangChain's modular structure, detailing its core components: the Model I/O module (for managing prompts, prompt templates, and parsing model outputs), the Retrieval module (for enabling applications to use external data to become context-aware), and the Agent Tooling module (for creating LLMs that can use various tools and reason through tasks). The LangChain Expression Language (LCEL) is highlighted as the key protocol for connecting these modules.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üîó **LangChain's Core Purpose:** LangChain is a framework designed to simplify the development of sophisticated LLM applications that are **stateful** (maintaining memory or conversational history), **context-aware** (able to utilize external or proprietary data), and capable of **reasoning** (using tools and planning sequences of actions to solve complex tasks).\n",
    "    - *Relevance (Data Science):* This empowers developers and data scientists to construct more than just simple chatbots. LangChain facilitates the creation of complex data-driven workflows, systems capable of Retrieval Augmented Generation (RAG), and agent-like applications that can perform intricate problem-solving using LLMs across various domains.\n",
    "- üß± **Modular Architecture for Enhanced Development:** LangChain structures the development of complex LLM applications by breaking it down into several distinct, manageable integration components, each responsible for specific subtasks.\n",
    "    - *Relevance:* This modular design promotes flexibility, allows for the easy swapping or customization of components, and encourages a more organized and robust approach to building scalable LLM applications.\n",
    "- üîÑ **Model I/O Module: The LLM Interface:** This foundational set of components manages all aspects of the interaction with the language model. Its key parts include:\n",
    "    - **Prompts & Prompt Templates:** \"Prompts\" are the direct textual inputs (questions or instructions) given to the LLM. **\"Prompt Templates\"** offer an abstraction layer, creating reusable prompt structures with placeholders (e.g., a template for suggesting names for any given \"pet type\" by simply changing an input variable like \"dog\" or \"cat\"). This is particularly beneficial for techniques such as few-shot prompting.\n",
    "        - *Relevance:* Standardizes and simplifies how instructions are formulated and delivered to LLMs, making it easier to manage dynamic inputs, experiment with different prompting strategies, and maintain consistency.\n",
    "    - **LLM Integration:** This refers to the direct interface with the chosen language model itself (e.g., models from OpenAI, Anthropic, etc.).\n",
    "    - **Output Parsers:** These essential tools transform the raw output received from an LLM (which is often a plain text string or a structured \"assistant role message\") into more organized and usable formats that other parts of an application or external software systems can readily process (e.g., Python native objects like lists or dictionaries, pandas DataFrames, JSON, or XML for interoperability with other LLMs). The video mentions parsers for strings, datetime objects, and comma-separated lists.\n",
    "        - *Relevance:* Crucial for the seamless integration of LLM-generated outputs into broader application logic, data processing pipelines, or any workflow that requires data in specific, structured formats.\n",
    "- üìö **Retrieval Module: Enabling Context-Awareness via Retrieval Augmented Generation (RAG):** This module is key to making LLMs \"context-aware\" by enabling them to access, retrieve, and utilize information from external or proprietary data sources that were not part of their original training dataset. This is the core principle behind RAG. The typical workflow involves several sub-components:\n",
    "    - **Document Loaders:** For ingesting data from diverse sources (e.g., text files, PDFs, web pages, databases).\n",
    "    - **Text Splitters:** For dividing large documents into smaller, more manageable chunks suitable for embedding and retrieval.\n",
    "    - **Embedding Models:** For converting these text chunks into dense numerical vector representations (embeddings).\n",
    "    - **Vector Stores:** Specialized databases for storing these text embeddings and enabling efficient similarity searches.\n",
    "    - **Retrievers:** Components that use a query (also embedded) to find and fetch the most relevant text chunks from the vector store.\n",
    "    - The video mentions a course project to build a chatbot capable of answering questions about the \"365 Introduction to Data and Data Science\" course, which would leverage this module.\n",
    "    - *Relevance:* Vastly expands the practical utility of LLMs by allowing them to provide responses and perform tasks based on specific, current, or private information, rather than being limited to their generalized, pre-trained knowledge.\n",
    "- üõ†Ô∏è **Agent Tooling Module: Empowering LLMs with Reasoning, Planning, and Action:** This module allows for the creation of \"reasoning chatbots\" or more generally, \"agents.\" These LLM-powered agents can intelligently choose from a defined set of \"tools\" (which can represent other functions, APIs, external services, databases, or even other LLMs) and decide the sequence of their execution to accomplish complex, multi-step tasks.\n",
    "    - **Toolkits** are mentioned as pre-packaged collections of tools designed for specific domains (e.g., a SQL toolkit for database interaction), though their detailed coverage is noted as being outside the immediate scope of the course.\n",
    "    - *Relevance:* This elevates LLMs from being mere passive text generators to becoming active problem-solvers. Agents can interact with their environment, perform calculations, fetch real-time data, or execute actions through other systems to achieve user-defined goals.\n",
    "- ‚õìÔ∏è **LangChain Expression Language (LCEL):** This is described as a specific declarative protocol or syntax used to chain together the different LangChain components and modules (Model I/O, Retrieval, Agents, etc.). It forms a significant part of practical LangChain development and is best understood and learned through hands-on coding.\n",
    "    - *Relevance:* LCEL provides the essential \"glue\" or syntax for composing complex sequences of operations (chains). It allows developers to define and manage these sophisticated LLM-driven workflows in a clear, efficient, and often more readable manner.\n",
    "- üìÑ **LangChain Templates (for Prompts):** The framework also offers \"Templates,\" which are pre-implemented and often optimized prompt templates designed for specific common tasks. These provide developers with ready-to-use starting points.\n",
    "    - *Relevance:* These templates can significantly accelerate development and improve the performance of LLM interactions for recurring use cases by leveraging community-tested best practices in prompt engineering.\n",
    "- üìñ **Key Documentation and Learning Resources:** Two primary web resources are emphasized as being essential for learning and effectively using LangChain:\n",
    "    - **LangChain's official documentation page:** This comprehensive site contains a wealth of information, including tutorials, practical how-to guides, a conceptual guide explaining the framework's design philosophy and core ideas, and an extensive list of available integrations with various LLMs, data sources, and tools.\n",
    "    - **LangChain library's API reference:** This provides detailed documentation for every function, class, and method available within the LangChain software libraries, which is crucial for understanding the specifics of implementation and for advanced customization.\n",
    "    - *Relevance:* These resources direct learners and developers to the most authoritative and up-to-date sources for mastering the LangChain framework, troubleshooting development issues, and discovering its full range of advanced functionalities.\n",
    "- ‚è≠Ô∏è **Other LangChain Ecosystem Components (Mentioned Briefly, Beyond Core Course Scope):**\n",
    "    - **Example Selectors:** A component usually found within the Model I/O module, used for dynamically selecting relevant examples to include in prompts for few-shot learning techniques.\n",
    "    - **Toolkits (Advanced Usage):** More complex and specialized combinations of tools designed for advanced agent capabilities in specific domains.\n",
    "    - **LangSmith Platform:** A separate, complementary platform developed by LangChain for observability. It offers features like tracing, monitoring, and debugging for LangChain applications, proving particularly useful during the development lifecycle and for maintaining production systems.\n",
    "    - **LangServe Libraries:** A set of libraries designed to help developers easily deploy their LangChain applications as robust, production-ready services, often as REST APIs.\n",
    "    - *Relevance:* While not covered in depth in this introductory segment, awareness of these components provides learners with a view of the broader LangChain ecosystem available for more advanced development stages, such as optimizing prompt strategies, building highly specialized agents, and transitioning applications into production environments.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **LangChain's Core Value: Orchestrating Sophisticated, Multi-Step LLM Applications:**\n",
    "    - *Why is this concept important to know or understand?* Direct interaction with LLM APIs (like OpenAI's) provides powerful but raw text generation capabilities. LangChain offers a higher-level abstraction and a comprehensive framework to build entire *applications* around these LLMs. These applications can be far more complex and capable than simple request-response chatbots. LangChain provides the tools and structures to manage **state** (e.g., conversation history), incorporate **external knowledge** (making LLMs context-aware through RAG), enable LLMs to **interact with other tools and data sources** (acting as agents), and **chain multiple operations or LLM calls** together into coherent, logical workflows.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* LangChain enables developers to construct a wide array of practical solutions:\n",
    "        - **Contextual Chatbots and Q&A Systems:** Bots that can maintain conversational context and accurately answer questions by querying specific documents (e.g., a customer service bot for a complex product manual, a research assistant for scientific papers). This is often achieved using Retrieval Augmented Generation (RAG) patterns.\n",
    "        - **Reasoning Agents:** LLMs that can autonomously break down complex user requests into smaller, actionable steps, intelligently decide which tools to use for each step (e.g., performing a web search via an API, running a Python script for calculation, querying a SQL database), execute these tools, and then synthesize the intermediate results to provide a final, comprehensive answer or solution.\n",
    "        - **Data-Augmented Generation Systems:** Applications that first retrieve relevant data snippets from vector stores, databases, or other knowledge sources, and then dynamically feed this context into a prompt for an LLM. This results in generated text that is more factual, detailed, specific, and contextually grounded.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This approach aligns with established principles of **software engineering** (such as modularity, abstraction, and composition), the design of **agent-based systems and cognitive architectures** in AI, the practical implementation of **Retrieval Augmented Generation (RAG)**, tools for **workflow automation**, and patterns for effective **API integration**.\n",
    "- **The Synergy of LangChain's Primary Modules (Model I/O, Retrieval, and Agents):**\n",
    "    - *Why is this concept important to know or understand?* The true power of LangChain emerges from the synergistic way its core modules interact, often orchestrated by the LangChain Expression Language (LCEL). Understanding the distinct role and capabilities of each module is fundamental to designing effective and efficient LangChain applications:\n",
    "        - **Model I/O** acts as the crucial interface layer to the LLM. It is responsible for meticulously structuring the input that is fed *into* the LLM (through prompts and flexible prompt templates) and for systematically transforming the LLM's raw output *from* the LLM into a format that is readily usable by other components of the application or directly by the end-user (achieved via various output parsers).\n",
    "        - The **Retrieval** module functions as the LLM's adaptable and extensible knowledge base. It empowers the application to fetch and incorporate relevant information from a wide array of external or proprietary data sources, thereby providing the LLM with specific context that was not available in its original, static training data. This capability is what makes the LLM's responses significantly more accurate, up-to-date, and pertinent to the specific domain or query.\n",
    "        - **Agents and Tools** equip the LLM with the ability to perform actions and interact with its environment. Moving beyond merely processing and generating text, agents can use tools to perform computations, access real-time data through external APIs, query databases, or even execute other pieces of code, thereby actively working towards achieving user-defined objectives.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* These modules frequently operate in a coordinated fashion. For example, a user might pose a complex question (which is initially handled by the Model I/O module). An Agent might then determine that it requires additional information to answer adequately. It could then utilize the Retrieval module to search a specific document database for relevant context. This retrieved context, combined with the original user query, would then be formatted by the Model I/O module into a new, enriched prompt for the LLM. The LLM's subsequent response would again be processed by an Output Parser (part of Model I/O) to structure it appropriately before it is presented to the user or used as input for a subsequent step in the chain. The LangChain Expression Language (LCEL) provides the syntax and mechanisms to define and manage these interconnected \"chains\" of operations.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This architectural approach relates to established concepts in computer science such as **data pipelines** and **data flow programming**, **information retrieval systems**, **cognitive architectures** (in the field of AI, these are models proposing how intelligent systems might be structured to achieve complex behaviors), **decision-making systems**, and the broader programming paradigm of constructing applications through **chains or sequences of interlinked operations**.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **My primary goal is to develop a chatbot that can accurately answer detailed questions based on a large repository of my company's internal PDF technical manuals. Which core LangChain module, as outlined in the video, would be the most critical for achieving this functionality, and what are some of its key sub-components that would typically be involved in such a setup?**\n",
    "    - The **Retrieval module** would be the most critical for this task. Its key sub-components that would typically be involved include: **Document Loaders** (to ingest the content from your PDF technical manuals), **Text Splitters** (to break down the large documents into smaller, more manageable, and semantically coherent chunks for processing), an **Embedding Model** (to convert these text chunks into numerical vector representations, or embeddings), a **Vector Store** (to store these embeddings efficiently and enable fast similarity searches), and finally, the **Retriever** component itself (which takes a user's query, embeds it, and then fetches the most relevant document chunks from the vector store to provide context for the answer).\n",
    "- **Could you explain the difference between a \"Prompt\" and a \"Prompt Template\" as they are used within LangChain's Model I/O module, perhaps using the \"pet name suggestion\" example from the video, for someone who is new to these concepts?**\n",
    "    - A \"Prompt\" is the exact, complete text of the question or instruction you send directly to the AI model. For instance, \"I've recently adopted a dog. Could you suggest some dog names?\". A \"Prompt Template,\" in contrast, is like a reusable blueprint or a fill-in-the-blanks version of a prompt. Using the video's example, a template might look like: \"I've recently adopted a [pet_type]. Could you suggest some [pet_type] names?\". You can then easily reuse this template by simply supplying different values for the placeholder \"[pet_type]\" (e.g., \"dog,\" \"cat,\" \"bird\") to generate specific prompts, making your code more flexible and easier to manage, especially when you need to ask similar types of questions repeatedly with minor variations.\n",
    "- **If an LLM returns its response as a simple, unstructured raw text string, but for my downstream application, I need this information to be neatly organized, for instance, as a Python list of bullet points or perhaps formatted as a specific date object, which particular component of LangChain's Model I/O module is specifically designed to handle this kind of data transformation?**\n",
    "    - An **Output Parser**, which is an integral part of LangChain's Model I/O module, is specifically designed to handle this type of transformation. Its role is to take the raw text string output from the LLM and convert (or \"parse\") it into a more structured and programmatically usable format, such as a Python list, a dictionary, a datetime object, or other specific data structures that your application requires for further processing or display.\n",
    "\n",
    "# ChatOpenAI\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This video tutorial kicks off the LangChain journey by guiding users through the initial setup and usage of the `ChatOpenAI` class to interact with OpenAI's chat models, such as GPT-4. It covers installing necessary libraries (`langchain`, `langchain-openai`), configuring the environment and API key, instantiating the model with key parameters like `model_name`, `seed` (via `model_kwargs`), `temperature`, and `max_tokens`, and making a basic `invoke` call to get a response. This foundational work aims to prepare users for an upcoming mini-project: building a \"helpful but sarcastic chatbot.\"\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üöÄ **Section Goal & First Project:** The primary aim introduced is to complete a first LangChain mini-project: creating a \"helpful but sarcastic chatbot.\" This initial lesson focuses on setting up the environment and performing basic interactions with OpenAI models through the LangChain framework.\n",
    "    - *Relevance (Data Science):* This establishes a clear, practical learning objective and demonstrates how to progress from fundamental API interactions to more structured LLM application development using a dedicated framework, a common requirement in applied AI and data science projects.\n",
    "- üìö **Upcoming Core LangChain Concepts (Roadmap):** The video outlines several key LangChain elements that will be explored in subsequent lessons to build more complex applications:\n",
    "    - The `ChatOpenAI` class, which is LangChain's interface for OpenAI's chat models.\n",
    "    - The different types of messages used within LangChain (e.g., `AIMessage`, `HumanMessage`, `SystemMessage`) to structure conversations.\n",
    "    - The utility and implementation of Prompt Templates for creating dynamic and reusable prompts.\n",
    "    - The fundamental concept of a \"chain\" in LangChain, which orchestrates sequences of calls to LLMs or other utilities. This will be further detailed when covering the LangChain Expression Language (LCEL).\n",
    "    - *Relevance:* This roadmap provides learners with a structured overview of the essential building blocks they will learn to master for constructing sophisticated LLM applications.\n",
    "- üíª **Installation Steps for LangChain with OpenAI:** Users are instructed to install two main Python libraries within their designated Anaconda LangChain environment:\n",
    "    1. `pip install langchain` (to install the core LangChain library)\n",
    "    2. `pip install langchain-openai` (to install the specific integration package required for using OpenAI models with LangChain)\n",
    "    <!-- end list -->\n",
    "    - *Relevance:* These are crucial prerequisite steps for any developer intending to work with the LangChain framework and leverage OpenAI's models within that ecosystem.\n",
    "- üîë **Environment and API Key Setup Best Practices:** The tutorial emphasizes proper environment configuration for a smooth development experience:\n",
    "    - Activate the designated Anaconda environment created for LangChain (e.g., `langchain_env`).\n",
    "    - Work within a Jupyter Notebook, ensuring that the notebook is using the kernel associated with this specific LangChain environment.\n",
    "    - Place the `.env` file, which should contain the `OPENAI_API_KEY`, in the same directory as the Jupyter notebook.\n",
    "    - Utilize IPython magic commands (`%load_ext dotenv` followed by `%dotenv`) within the notebook. This loads the API key from the `.env` file into the environment variables, making it accessible to the LangChain library without hardcoding it.\n",
    "    - *Relevance:* Demonstrates standard best practices for managing Python project dependencies in isolated environments and for securely handling sensitive API keys, which is essential for both development and deployment.\n",
    "- ü§ñ **Using the `ChatOpenAI` Class for Model Interaction:** The core method for interacting with OpenAI's chat models within LangChain involves the `ChatOpenAI` class:\n",
    "    - **Import Statement:** `from langchain_openai.chat_models import ChatOpenAI`\n",
    "    - **Instantiation:** An instance of the `ChatOpenAI` class is created. An example provided is:\n",
    "    `chat = ChatOpenAI(model_name=\"gpt-4\", model_kwargs={\"seed\": 365, \"max_tokens\": 100}, temperature=0)`\n",
    "        - `model_name`: A string parameter that specifies which OpenAI model to use (e.g., \"gpt-4\"). The video notes that other supported models listed on OpenAI's website can also be used.\n",
    "        - `model_kwargs`: A dictionary used to pass additional parameters that are not direct arguments of the `ChatOpenAI` constructor but are supported by the underlying OpenAI API. The video demonstrates its use for:\n",
    "            - `seed`: An integer (e.g., `365`) used to promote reproducibility of the LLM's responses. The video clarifies that `seed` is not a direct parameter of `ChatOpenAI` but is passed via `model_kwargs`.\n",
    "            - `max_tokens`: An integer (e.g., `100`) to set a limit on the maximum length of the response generated by the model.\n",
    "        - `temperature`: A float (e.g., `0`) that controls the randomness of the LLM's output. Lower values, like 0, make the responses more deterministic and focused.\n",
    "    - *Relevance:* This showcases the primary mechanism within LangChain for programmatically interfacing with OpenAI's powerful chat models and for precisely controlling their generative behavior through various parameters.\n",
    "- üó£Ô∏è **Invoking the Model with the `invoke()` Method:**\n",
    "    - The `invoke()` method is the standard way to send a prompt to an instantiated `ChatOpenAI` object and receive a response from the LLM: `response = chat.invoke(\"Your prompt as a string\")`.\n",
    "    - The video highlights `invoke()` as a \"chief method in LangChain,\" indicating its central role in executing LLM calls. It's mentioned that its full versatility and power will become more apparent when the LangChain Expression Language (LCEL) is discussed in later sections. For this initial interaction, it directly accepts a string prompt.\n",
    "    - **Prompting Style Tip:** The tutorial suggests using triple quotes (e.g., `\"\"\"This is a multi-line prompt.\"\"\"` or `'''So is this.''') for writing prompts. This Python string literal convention conveniently allows for embedding multi-line text and including single or double quotation marks within the prompt itself without causing string termination errors or requiring escape characters.\n",
    "    - *Relevance:* Introduces the fundamental method for executing a call to the LLM within the LangChain framework and obtaining its output, forming the basis of most LLM interactions in LangChain.\n",
    "- üí¨ **Understanding and Accessing the LLM's Response Object:**\n",
    "    - The `response` object that is returned by the `chat.invoke()` method is an instance of LangChain's `AIMessage` class. (The video notes that various message types, including `AIMessage`, will be discussed in greater detail soon).\n",
    "    - The actual text content of the AI's reply, which is the part users are typically most interested in, is stored within the `response.content` attribute of this `AIMessage` object.\n",
    "    - To display this human-readable text neatly, the standard Python `print()` function can be used: `print(response.content)`.\n",
    "    - *Relevance:* Explains the structure of the object returned by an LLM call in LangChain and clarifies how to extract the essential generated text for use within an application or for display to a user.\n",
    "- ‚ú® **Practical Example of Interaction and Parameter Refinement:**\n",
    "    - A sample prompt is used to demonstrate the process: `\"I've recently adopted a dog. Could you suggest some dog names?\"`\n",
    "    - The video shows that an initial response from the model (without a token limit) can be quite lengthy and exhaustive.\n",
    "    - The `max_tokens` parameter (e.g., set to `100` via `model_kwargs` during the `ChatOpenAI` instantiation) is then effectively used to control and significantly shorten the length of the model's generated completion. This makes the output more concise and manageable.\n",
    "    - *Relevance:* Provides a tangible, step-by-step demonstration of how to interact with the LLM using LangChain and how to iteratively adjust its parameters to refine the output according to specific application requirements, such as brevity or controlled length.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **The `ChatOpenAI` Class as a LangChain Interface to OpenAI Models:**\n",
    "    - *Why is this concept important to know or understand?* The `ChatOpenAI` class is LangChain's specialized wrapper designed to streamline interaction with OpenAI's chat-based LLMs (like GPT-3.5-turbo, GPT-4, and their variants). It effectively abstracts away the lower-level complexities of direct API communication, such as manually handling HTTP requests, authentication headers, and JSON request/response formatting. Instead, it provides a standardized, Python-native way to configure model parameters (like `model_name`, `temperature`), send prompts, and receive responses, all within the cohesive LangChain ecosystem. This approach not only simplifies development but also facilitates the integration of these powerful models into more complex LangChain structures like chains and agents.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* For any application being built with LangChain that needs to harness the conversational or instruction-following capabilities of OpenAI's advanced chat models‚Äîbe it for constructing sophisticated chatbots, content generation pipelines, text summarization tools, or intricate question-answering systems‚Äîthe `ChatOpenAI` class will serve as the primary, and often indispensable, entry point for interacting with the LLM. Furthermore, its design contributes to LangChain's broader goal of model agnosticism; by adhering to a common `ChatModels` interface, LangChain allows developers (in principle) to swap out different LLM providers with minimal code changes, promoting flexibility.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This concept is directly related to the use of **API wrappers** and **Software Development Kits (SDKs)**, which simplify interaction with external services. It embodies the software design principle of **abstraction layers**, hiding implementation details to present a simpler interface. It's a core part of **model integration** within larger software frameworks and aligns with LangChain's overall `ChatModels` API, which aims to provide a consistent programming interface across diverse LLM providers.\n",
    "- **Significance of Key LLM Parameters (`temperature`, `seed`, `max_tokens`) in Controlling Output Behavior via LangChain:**\n",
    "    - *Why is this concept important to know or understand?* These parameters offer developers crucial, fine-grained control over the LLM's text generation process, allowing them to tailor the output to specific needs:\n",
    "        - `temperature`: This parameter directly influences the perceived \"creativity\" versus \"predictability\" of the LLM's output. Lower values (e.g., 0.0 to 0.3) make the output more focused, deterministic, and often more factual, as the model tends to select the tokens with the highest probability at each step. Higher values (e.g., 0.7 to 1.0 or even higher) increase randomness in token selection, leading to more diverse, novel, or \"creative\" outputs, but this can sometimes come at the cost of coherence, factual accuracy, or relevance.\n",
    "        - `seed`: When this parameter is supported by the underlying model and API (as it is for OpenAI models, passed via `model_kwargs` in LangChain), using a fixed integer seed aims to ensure the reproducibility of the LLM's output. Given the exact same input prompt and all other parameters (including the seed) remaining constant, the model should ideally produce the identical response across multiple runs. This reproducibility is invaluable for debugging, systematic testing, consistent demonstrations, and ensuring stable behavior in production applications.\n",
    "        - `max_tokens`: This parameter sets an explicit upper limit on the length of the response that the LLM will generate, measured in \"tokens\" (which are typically sub-word units). It is important for several practical reasons: managing API operational costs (as many LLM APIs charge based on the total number of input and output tokens), controlling the verbosity of responses to make them more user-friendly, ensuring that outputs fit within specific UI elements or character limits, and preventing excessively long or \"runaway\" generations that could consume unnecessary resources.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?*\n",
    "        - For applications requiring high factual accuracy, such as technical documentation generation, code generation, or concise summarization, a low `temperature` (e.g., 0 to 0.2) is generally preferred. For tasks like brainstorming, creative story writing, or generating varied marketing slogans, a higher `temperature` (e.g., 0.7 to 0.9) might yield more interesting results.\n",
    "        - The `seed` parameter is particularly essential during the development, testing, and quality assurance phases. It allows developers to consistently reproduce outputs while tweaking prompts, other parameters, or different parts of the application logic, making it easier to isolate the impact of specific changes.\n",
    "        - `max_tokens` is critical for almost all practical LLM applications. Whether it's ensuring that a chatbot provides succinct answers, a content generator produces articles of a predetermined length, or an API integration stays within budgetary token limits, controlling output length is a fundamental requirement.\n",
    "    - *What other concepts, techniques, or areas is this related to?* The adjustment of these parameters is a form of **hyperparameter tuning** specific to generative language models. It closely interacts with **prompt engineering** (as the optimal parameter settings can often depend on the structure and content of the prompt itself). It's also directly relevant to **API cost management strategies** and the broader engineering goal of achieving **controllability, predictability, and efficiency** in AI systems.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Installation (to be run in the Anaconda Prompt within the activated `langchain_env` environment):**\n",
    "    \n",
    "    ```bash\n",
    "    pip install langchain\n",
    "    pip install langchain-openai\n",
    "    ```\n",
    "    \n",
    "- **API Key Setup (typically at the beginning of a Jupyter Notebook):**\n",
    "    \n",
    "    ```python\n",
    "    # Ensure a .env file with OPENAI_API_KEY=your_actual_api_key is in the same directory\n",
    "    %load_ext dotenv\n",
    "    %dotenv\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Importing, Instantiating `ChatOpenAI`, and Invoking the Model:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    \n",
    "    # Instantiate the ChatOpenAI class with desired parameters\n",
    "    # Example demonstrating initial setup and then redefinition for max_tokens\n",
    "    chat_config_initial = {\n",
    "        \"model_name\": \"gpt-4\",\n",
    "        \"model_kwargs\": {\"seed\": 365},\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    # chat_initial_instance = ChatOpenAI(**chat_config_initial)\n",
    "    \n",
    "    # Configuration including max_tokens, as shown later in the video for shorter response\n",
    "    chat_config_with_max_tokens = {\n",
    "        \"model_name\": \"gpt-4\",\n",
    "        \"model_kwargs\": {\"seed\": 365, \"max_tokens\": 100}, # Added max_tokens\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    chat = ChatOpenAI(**chat_config_with_max_tokens)\n",
    "    \n",
    "    # Define the prompt (using triple quotes for potential multi-line input)\n",
    "    prompt_text = \"\"\"I've recently adopted a dog.\n",
    "    Could you suggest some dog names?\"\"\"\n",
    "    \n",
    "    # Invoke the model with the prompt\n",
    "    response = chat.invoke(prompt_text)\n",
    "    \n",
    "    # The 'response' object is an AIMessage instance.\n",
    "    # Access and print the actual text content of the AI's reply.\n",
    "    print(response.content)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **I am using the `ChatOpenAI` class in my LangChain application and want to ensure that for a given prompt, the AI's response is the same every time I run my tests. Which two parameters, as highlighted in the video, should I configure in the `ChatOpenAI` instantiation, and what kinds of values should I assign to them?**\n",
    "    - To ensure reproducible and consistent responses for testing, you should configure the `seed` parameter by passing it within the `model_kwargs` dictionary (e.g., `model_kwargs={\"seed\": 365}` with a fixed integer like 365), and you should set the `temperature` parameter to a very low value, ideally `0`.\n",
    "- **If the responses from my `ChatOpenAI` model in LangChain are consistently too long and verbose for my application's needs, which specific parameter mentioned in the video can I use to limit the length of the generated output, and how is it typically passed during the instantiation of the `ChatOpenAI` class?**\n",
    "    - To limit the length of the generated output, you should use the `max_tokens` parameter. As shown in the video, this is typically passed as part of the `model_kwargs` dictionary during the instantiation of the `ChatOpenAI` class, for example: `ChatOpenAI(model_kwargs={\"max_tokens\": 100})`, where 100 is the desired maximum number of tokens.\n",
    "- **When I use the `chat.invoke(\"my prompt\")` method with an instance of `ChatOpenAI` in LangChain, what type of object does this method typically return, and how do I access the actual human-readable text content generated by the AI from this returned object?**\n",
    "    - The `chat.invoke(\"my prompt\")` method typically returns an `AIMessage` object. You can access the actual human-readable text content generated by the AI by accessing the `.content` attribute of this `AIMessage` object (e.g., `response.content`).\n",
    "\n",
    "# System and human messages\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This video tutorial builds upon previous lessons by demonstrating how to use specific message types, namely `SystemMessage` and `HumanMessage`, within the LangChain framework to interact with OpenAI's chat models via the `ChatOpenAI` class. It explains that these message types correspond to the system and user roles in the OpenAI API, allowing for more nuanced control over the chatbot's persona and responses. The lesson walks through creating instances of these messages, invoking the chat model with a list containing both a system message (to define a sarcastic persona like \"Marv\") and a human message (the user's query), and observing how this combination elicits the desired sarcastic output from the AI.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üí¨ **Advancing Beyond Single String Prompts:** The lesson transitions from the previous method of invoking `ChatOpenAI` with a single string prompt to a more structured approach using distinct message types within LangChain.\n",
    "    - *Relevance (Data Science):* This progression is key for data scientists and developers looking to build more sophisticated and controllable LLM applications, as structured inputs allow for better management of conversational context and AI behavior.\n",
    "- üé≠ **Introducing LangChain Message Roles for Persona and Interaction:** The core focus is on utilizing LangChain's `SystemMessage` and `HumanMessage` classes. These are directly analogous to the \"system\" and \"user\" roles familiar from the OpenAI API.\n",
    "    - **`SystemMessage`:** This message type is used to set the AI's overall behavior, define its persona, or provide high-level instructions that should guide its responses (e.g., instructing the chatbot to be \"Marv, a chatbot that reluctantly answers questions with sarcastic responses.\").\n",
    "    - **`HumanMessage`:** This represents the input, questions, or requests originating from the end-user.\n",
    "    - *Relevance:* These structured message types allow developers to more effectively guide the LLM's behavior, define its personality, and manage conversational context, which is crucial for creating specialized and interactive chatbots.\n",
    "- üìù **Structured Lesson Strategy:** The tutorial follows a clear, step-by-step plan:\n",
    "    1. Utilize a pre-configured `ChatOpenAI` model instance.\n",
    "    2. Create an instance of LangChain's `HumanMessage` class containing the user's request.\n",
    "    3. Create an instance of LangChain's `SystemMessage` class to define the chatbot's persona.\n",
    "    4. Invoke the chat model by passing a list containing these message objects, typically with the system message first, followed by the human message.\n",
    "    5. Analyze the resulting `AIMessage` (the AI's response) to observe the impact of the system message on the output tone and content.\n",
    "    <!-- end list -->\n",
    "    - *Relevance:* Provides a clear, step-by-step methodology for learners to follow, making complex concepts more digestible.\n",
    "- ‚öôÔ∏è **Assumed Pre-configured Environment:** The lesson starts with a Jupyter Notebook where the OpenAI API key is already set as an environment variable, the `ChatOpenAI` class is imported, and a `ChatOpenAI` object is initialized with parameters like `model_name`, `model_kwargs` (for seed and max_tokens), and `temperature`.\n",
    "    - *Relevance:* Ensures learners can focus on the new concepts of message types without getting bogged down in repetitive setup tasks.\n",
    "- ‚ú® **New Imports for Message Types:** The necessary classes are imported from LangChain's core message module:\n",
    "`from langchain_core.messages import SystemMessage, HumanMessage`\n",
    "    - *Relevance:* Shows the specific LangChain components required for working with distinct message roles.\n",
    "- ‚úçÔ∏è **Creating Message Instances:**\n",
    "    - **Human Message:** `message_human = HumanMessage(content=\"I've recently adopted a dog. Can you suggest some dog names?\")`\n",
    "    - **System Message:** `message_system = SystemMessage(content=\"You are Marv, a chatbot that reluctantly answers questions with sarcastic responses.\")`\n",
    "    The `content` parameter of these classes holds the actual text for each message.\n",
    "    - *Relevance:* Demonstrates the practical instantiation of message objects with their respective content.\n",
    "- üöÄ **Invoking the Model with a List of Messages:**\n",
    "    - The `chat.invoke()` method is shown to accept not only a single string (as in the previous lesson) but also a **list of chat message objects** (e.g., instances of `SystemMessage`, `HumanMessage`).\n",
    "    - **Demonstration Flow:**\n",
    "        1. First, invoke with only `HumanMessage`: `response = chat.invoke([message_human])`. This results in a standard, helpful AI response.\n",
    "        2. Then, invoke with both `SystemMessage` (for sarcasm) and `HumanMessage`: `response = chat.invoke([message_system, message_human])`. This is the key demonstration, showing how the system message (defining Marv's sarcastic persona) influences the AI's response to the human message, resulting in the desired sarcastic tone.\n",
    "    - The order in the list generally matters (system message usually precedes the human message for setting context).\n",
    "    - *Relevance:* This is a fundamental LangChain pattern for multi-turn conversations or for setting a specific context/persona for the LLM before it processes the user's immediate query.\n",
    "- üìÑ **Output Still an `AIMessage`:** The output from `chat.invoke()` when passed a list of messages is still an `AIMessage` object. The actual text generated by the AI is accessed via its `.content` attribute (e.g., `response.content`).\n",
    "    - *Relevance:* Maintains consistency in how LangChain handles LLM responses, regardless of whether the input was a simple string or a list of structured messages.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **The Importance and Function of Message Roles (`SystemMessage`, `HumanMessage`, `AIMessage`) in LangChain for Structuring Conversational AI:**\n",
    "    - *Why is this concept important to know or understand?* In building effective conversational AI, it's crucial to clearly differentiate between different speakers or types of information being exchanged and to guide the LLM's behavior accordingly. LangChain's message types (`SystemMessage`, `HumanMessage`, and `AIMessage` ‚Äì which represents the AI's own output) provide a robust and explicit structure for this.\n",
    "        - **`SystemMessage`** serves as a powerful tool to set the overarching context, define the AI's persona or role (e.g., helpful assistant, sarcastic bot, Socratic tutor), provide high-level instructions, or establish constraints that should govern its responses throughout a conversation or for a specific interaction. It's akin to giving the AI its \"character sheet,\" \"operating manual,\" or \"prime directive.\"\n",
    "        - **`HumanMessage`** clearly demarcates input originating from the end-user. This includes their questions, commands, statements, or any information they provide to the AI.\n",
    "        - **`AIMessage`** (along with other types like `FunctionMessage` or `ToolMessage` which might be introduced later) represents the outputs, replies, or actions generated by the AI model or by tools the AI might use.\n",
    "        By using these distinct message objects, developers can construct more sophisticated and predictable conversational flows, manage dialogue history more effectively for contextually relevant responses, and fine-tune the AI's outputs to align with specific desired personas or operational objectives.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* This structured messaging is fundamental for a wide range of real-world applications:\n",
    "        - **Building Specialized Chatbots:** A customer support chatbot requires a different persona (e.g., helpful, patient, empathetic) than an educational tutor (e.g., inquisitive, guiding) or the \"Marv\" example (sarcastic). The `SystemMessage` is the primary vehicle for instilling these distinct personalities.\n",
    "        - **Maintaining Conversational Context and State:** In extended, multi-turn dialogues, a list comprising past `HumanMessage`s and `AIMessage`s (often along with an initial `SystemMessage`) can be passed back to the model with each new user turn. This enables the LLM to \"remember\" the preceding parts of the conversation and generate responses that are coherent and contextually appropriate.\n",
    "        - **Implementing Complex Instructions and Constraints:** The `SystemMessage` can contain detailed and nuanced guidelines for how the AI should process user requests, the desired format for its output, specific information it should or should not include, or constraints it must adhere to during its generation process.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This approach is directly analogous to, and builds upon, the **message roles (system, user, assistant)** that are a core feature of the OpenAI Chat Completions API. It is a central concept in **conversational AI design**, **dialogue management systems**, and advanced **prompt engineering** techniques, particularly those focused on persona-driven prompting and maintaining long-term conversational coherence.\n",
    "- **Leveraging a List of Messages in `invoke()` for Contextual Control and Persona Setting in `ChatOpenAI`:**\n",
    "    - *Why is this concept important to know or understand?* Modern chat-based LLMs are inherently designed to process a sequence of messages to understand conversational history, infer context, and maintain coherence. Invoking the `ChatOpenAI` object in LangChain with a list of `SystemMessage` and `HumanMessage` objects (and potentially past `AIMessage`s in more complex scenarios) allows developers to fully utilize this sequential processing capability. Providing a list of messages gives the model a much richer and more structured context than a single, isolated string prompt. This enables the generation of more relevant, role-consistent, and contextually aware responses.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?*\n",
    "        - **Defining and Activating a Persona:** As explicitly demonstrated in the video, placing a `SystemMessage` at the beginning of the list (e.g., the message defining \"Marv\" as sarcastic) instructs the AI on how it should behave *before* it even processes the subsequent `HumanMessage`. This pre-contexting is key to achieving the desired persona.\n",
    "        - **Enabling Multi-turn Conversations:** For an ongoing chat application, the list of messages passed to `invoke()` with each new user turn would typically include the initial `SystemMessage`, followed by an alternating sequence of previous `HumanMessage`s and `AIMessage`s that represent the dialogue history up to that point. This historical context is crucial for the AI to generate follow-up responses that are natural and logically connected to what has already been discussed.\n",
    "        - **Facilitating Few-Shot Prompting (Implicitly):** Although not the primary focus of this specific lesson, a list of messages can also be strategically constructed to provide the model with examples of desired interactions (a few \"shots\" of human questions paired with ideal AI answers in the desired style or format) before the actual user query is presented. This technique can effectively guide the model's response style and improve its performance on specific tasks.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This method of interaction directly mirrors how the **OpenAI Chat Completions API** itself expects a `messages` array as input. It is a fundamental practice for building **stateful conversational AI systems**, effective **context management** in dialogue, and various advanced **prompt engineering techniques** that depend on providing rich and structured contextual information to the LLM to elicit optimal performance.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Importing Necessary Message Classes from LangChain Core:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Conceptual Pre-configured `ChatOpenAI` Object (Parameters based on video context):**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    \n",
    "    # Assuming 'chat' is already initialized as described in the lesson's setup:\n",
    "    # This might have been done in a previous cell or at the start of the notebook.\n",
    "    chat = ChatOpenAI(\n",
    "        model_name=\"gpt-4\",  # Or another specified supported model\n",
    "        model_kwargs={\"seed\": 365, \"max_tokens\": 100}, # Example values for reproducibility and length\n",
    "        temperature=0  # Example value for deterministic output\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Creating Instances of `HumanMessage` and `SystemMessage`:**\n",
    "    \n",
    "    ```python\n",
    "    # User's request\n",
    "    message_human = HumanMessage(\n",
    "        content=\"I've recently adopted a dog. Can you suggest some dog names?\"\n",
    "    )\n",
    "    \n",
    "    # Defining the AI's persona\n",
    "    message_system = SystemMessage(\n",
    "        content=\"You are Marv, a chatbot that reluctantly answers questions with sarcastic responses.\"\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Invoking the Chat Model with a List of Messages:**\n",
    "    \n",
    "    ```python\n",
    "    # First, invoking with only the HumanMessage to see a standard response\n",
    "    # response_human_only = chat.invoke([message_human])\n",
    "    # print(\"Standard Response:\")\n",
    "    # print(response_human_only.content)\n",
    "    \n",
    "    # Then, invoking with both SystemMessage and HumanMessage for a sarcastic response\n",
    "    # The SystemMessage typically comes first to set the context/persona\n",
    "    response_sarcastic = chat.invoke([message_system, message_human])\n",
    "    print(\"\\nSarcastic Response from Marv:\")\n",
    "    print(response_sarcastic.content)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *(Note: The video describes these as sequential steps. The output from `chat.invoke()` is an `AIMessage` object in both cases, from which `.content` is extracted.)*\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **In the LangChain framework, if my goal is to instruct my `ChatOpenAI` model to consistently adopt a specific personality, such as a formal academic tone or the sarcastic \"Marv\" persona from the video, which specific type of message object should I primarily create and use, and what is its main function?**\n",
    "    - To define a specific personality or role for your chatbot, you should primarily create and use a `SystemMessage` object. Its main function is to provide the AI model with high-level instructions, context, or behavioral guidelines that shape its persona and how it should respond during interactions.\n",
    "- **When using the `chat.invoke()` method with an instance of `ChatOpenAI` in LangChain, the video demonstrates passing a list of message objects as input. If I want the `SystemMessage` (defining the AI's persona) to influence the AI's response to the `HumanMessage` (the user's query), what is the generally recommended order for these messages within the list, and why is this order usually important?**\n",
    "    - The generally recommended order is to place the `SystemMessage` first in the list, followed by the `HumanMessage`. This order is usually important because it allows the `SystemMessage` to establish the desired context, persona, or operational instructions for the AI model *before* it processes the user's actual query, thereby directly influencing how the AI formulates its subsequent response.\n",
    "- **After I call `chat.invoke([SystemMessage(...), HumanMessage(...)])` using `ChatOpenAI` in LangChain, the video states that the method returns an `AIMessage` object. How would I then access the actual textual content of the AI's generated response from this returned object?**\n",
    "    - You would access the actual textual content of the AI's generated response by using the `.content` attribute of the `AIMessage` object that is returned by the `invoke` method (e.g., if the returned object is stored in a variable named `response_from_ai`, you would use `response_from_ai.content`).\n",
    "\n",
    "# AI messages\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This video tutorial demonstrates the use of LangChain's `AIMessage` class to implement few-shot prompting with the `ChatOpenAI` model. Building on previous lessons about `SystemMessage` and `HumanMessage`, this session focuses on teaching the AI model to adopt a specific behavior (in this case, sarcasm) by providing it with examples of user requests (`HumanMessage`) paired with desired AI responses (`AIMessage`). The lesson walks through structuring these example interactions in a list, invoking the model with this list followed by a new user query, and observing how the model learns to mimic the sarcastic tone from the provided examples. It also highlights that while more examples can improve performance, they also increase prompt token usage.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üí¨ **Building on Message Types:** The lesson continues the exploration of LangChain's chat messages, following up on `SystemMessage` (for chatbot instructions) and `HumanMessage` (for user requests).\n",
    "    - *Relevance (Data Science):* Shows a progressive approach to controlling LLM behavior, moving from direct instruction to learning by example, a common paradigm in machine learning.\n",
    "- üéØ **Focus on `AIMessage` for Few-Shot Prompting:** This lesson specifically introduces how `AIMessage` (representing the AI's response) can be used not just as an output, but as an *input* to demonstrate desired behavior through few-shot prompting.\n",
    "    - **Few-Shot Prompting Explained:** A technique where the LLM is given a set of example exchanges (e.g., a user query followed by an ideal AI answer) to \"learn from\" before it responds to a new, similar query. This helps the model's subsequent responses align better with the desired style, tone, or format.\n",
    "    - *Relevance:* Few-shot prompting is a powerful technique to guide LLMs without explicit fine-tuning, allowing for quick adaptation to specific tasks or response styles using only a small number of examples.\n",
    "- üìù **Lesson Strategy: Teaching Sarcasm via Examples:**\n",
    "    1. **Modify Previous Setup:** Start with the notebook from the last session, but *remove* any `SystemMessage` that defined a sarcastic persona. The goal is to achieve sarcasm purely through examples.\n",
    "    2. **Import `AIMessage`:** Add `AIMessage` to the imports: `from langchain_core.messages import AIMessage`.\n",
    "    3. **Create Example 1 (Dog Names):**\n",
    "        - `message_human_dog`: A `HumanMessage` asking for dog name suggestions.\n",
    "        - `message_ai_dog`: An `AIMessage` containing a *sarcastic* example response to the dog names query. (The video suggests reusing a previously generated sarcastic response or creating one).\n",
    "    4. **Formulate New Query 1 (Cat Names):**\n",
    "        - `message_human_cat`: A new `HumanMessage` asking for cat name suggestions.\n",
    "    5. **First Invocation Attempt:** Invoke `chat.invoke()` with a list: `[message_human_dog, message_ai_dog, message_human_cat]`. The AI's response to the cat query is observed. (The video notes that one example might not be enough to fully capture the sarcastic tone).\n",
    "    6. **Create Example 2 (Cat Names - AI Response):**\n",
    "        - `message_ai_cat`: An `AIMessage` providing a sarcastic example response to the *cat* names query.\n",
    "    7. **Formulate New Query 2 (Fish Names):**\n",
    "        - `message_human_fish`: A new `HumanMessage` asking for fish name suggestions.\n",
    "    8. **Second Invocation Attempt:** Invoke `chat.invoke()` with an expanded list: `[message_human_dog, message_ai_dog, message_human_cat, message_ai_cat, message_human_fish]`. The AI's response to the fish query is observed, now expected to be more consistently sarcastic.\n",
    "    <!-- end list -->\n",
    "    - *Relevance:* This step-by-step demonstration clearly illustrates the iterative process of building a few-shot prompt and how providing more relevant examples can improve the model's ability to adopt the desired behavior.\n",
    "- üí° **Important Considerations for Few-Shot Prompting:**\n",
    "    - **Combining with System Messages:** The speaker notes that, in general, it's often ideal to use *both* a `SystemMessage` (for overall direction) and a fair number of few-shot examples for optimal model performance. This lesson isolates few-shot prompting purely for demonstration purposes.\n",
    "    - **Source of Example AI Responses:** For efficiency, the example `AIMessage` content can be generated in a prior session (perhaps using a `SystemMessage` to get the desired style initially) or manually crafted by the developer to perfectly reflect the target output.\n",
    "    - **Token Cost:** Providing more examples in the prompt (as `HumanMessage` / `AIMessage` pairs) will improve the model's learning and adherence to the desired style but comes at the cost of increased prompt token usage, which can affect API costs and potentially hit context window limits.\n",
    "    - *Relevance:* These practical tips provide context on how to effectively use few-shot prompting while being mindful of its nuances and trade-offs.\n",
    "- ‚û°Ô∏è **Outcome and Next Steps:** The lesson successfully demonstrates that by providing a sequence of human requests and example sarcastic AI responses, the model can learn to generate subsequent responses in a similar sarcastic tone. The next lesson will focus on Chat Prompt Templates.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Few-Shot Prompting with `AIMessage` in LangChain: Teaching by Example:**\n",
    "    - *Why is this concept important to know or understand?* Few-shot prompting is a powerful and efficient way to guide an LLM's behavior for specific tasks or styles without needing to perform computationally expensive fine-tuning. By providing just a few examples (`shots`) of input-output pairs directly within the prompt context, the LLM can infer the desired pattern, tone, or format. Using `AIMessage` in LangChain allows developers to explicitly include these \"model's turn\" examples in the conversational history given to the LLM.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* This technique is highly versatile:\n",
    "        - **Style/Tone Transfer:** Teaching the model to respond in a specific style (e.g., sarcastic, formal, empathetic, like a particular character).\n",
    "        - **Task Adaptation:** Guiding the model on how to perform a novel task by showing it a few examples (e.g., \"Classify these sentences as positive or negative: [examples here]. Now classify this new sentence...\").\n",
    "        - **Output Formatting:** Demonstrating a desired output structure (e.g., JSON, specific list format) through examples.\n",
    "        - **Reducing Hallucinations/Improving Accuracy:** Providing factual examples can sometimes ground the model and lead to more accurate responses for similar queries.\n",
    "        The key is that the LLM learns \"in-context\" from the examples provided in the current prompt.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This is a core technique in **prompt engineering**. It contrasts with **zero-shot prompting** (where the model is asked to perform a task without any examples) and **fine-tuning** (where the model's weights are updated based on a larger dataset of examples). It's a form of **in-context learning**.\n",
    "- **The Trade-off: Number of Examples vs. Prompt Token Usage/Cost:**\n",
    "    - *Why is this concept important to know or understand?* While providing more examples in a few-shot prompt generally leads to better performance and closer adherence to the desired behavior, each example (both the `HumanMessage` and the `AIMessage` part) consumes tokens within the LLM's limited context window. This has direct implications.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?*\n",
    "        - **API Costs:** Most LLM APIs charge based on the total number of tokens processed (input + output). More examples mean more input tokens, leading to higher costs per API call.\n",
    "        - **Context Window Limits:** Every LLM has a maximum context window (e.g., 4k, 8k, 32k, 128k tokens). If the prompt, including all the few-shot examples, exceeds this limit, the API call will fail, or the context will be truncated, potentially losing valuable information from earlier examples.\n",
    "        - **Latency:** Larger prompts can sometimes lead to slightly increased response times (latency).\n",
    "        Developers must strike a balance: provide enough examples for the model to learn effectively but not so many that it becomes prohibitively expensive or exceeds the context window. This often involves experimentation to find the minimum number of high-quality examples that achieve the desired result.\n",
    "    - *What other concepts, techniques, or areas is this related to?* **Tokenization**, **API cost optimization**, **context window management**, **prompt compression techniques** (though not discussed here), and efficient **example selection** for few-shot learning.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Importing Necessary Message Classes (including `AIMessage`):**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "    # Note: SystemMessage might be removed or commented out if the goal is to rely purely on few-shot for persona\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Conceptual Pre-configured `ChatOpenAI` Object (from previous setup):**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    \n",
    "    chat = ChatOpenAI(\n",
    "        model_name=\"gpt-4\",\n",
    "        model_kwargs={\"seed\": 365, \"max_tokens\": 100}, # Or other desired max_tokens\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Creating Message Instances for Few-Shot Prompting (Illustrative):**\n",
    "    \n",
    "    ```python\n",
    "    # --- Example 1 (Dog Names) ---\n",
    "    message_human_dog = HumanMessage(\n",
    "        content=\"I've recently adopted a dog. Can you suggest some dog names?\"\n",
    "    )\n",
    "    # Sarcastic AI response (content would be the actual sarcastic text)\n",
    "    message_ai_dog = AIMessage(\n",
    "        content=\"Oh, another dog person. How original. Fine, how about 'Sir Barks-a-Lot' or 'Captain Fluffernutter'? Don't expect me to be impressed.\"\n",
    "    )\n",
    "    \n",
    "    # --- New Query 1 (Cat Names) ---\n",
    "    message_human_cat = HumanMessage(\n",
    "        content=\"I've recently adopted a cat. Could you suggest some cat names?\"\n",
    "    )\n",
    "    \n",
    "    # --- Example 2 (Cat Names - AI Response) ---\n",
    "    # Sarcastic AI response for cat names\n",
    "    message_ai_cat = AIMessage(\n",
    "        content=\"Cats? Even better. Just what the world needs, more feline overlords. Try 'Clawdia', 'Purrsephone', or maybe just 'The Void'. Thrilled for you.\"\n",
    "    )\n",
    "    \n",
    "    # --- New Query 2 (Fish Names) ---\n",
    "    message_human_fish = HumanMessage(\n",
    "        content=\"I've recently adopted a fish. Could you suggest some fish names?\"\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Invoking the Chat Model with a List of Messages for Few-Shot Learning:**\n",
    "    \n",
    "    ```python\n",
    "    # Invocation after one example (dog example + cat query)\n",
    "    # messages_for_cat_query = [message_human_dog, message_ai_dog, message_human_cat]\n",
    "    # response_cat_query = chat.invoke(messages_for_cat_query)\n",
    "    # print(\"Response to Cat Query (after 1 example):\")\n",
    "    # print(response_cat_query.content)\n",
    "    \n",
    "    # Invocation after two examples (dog example, cat example + fish query)\n",
    "    messages_for_fish_query = [\n",
    "        message_human_dog,\n",
    "        message_ai_dog,\n",
    "        message_human_cat,\n",
    "        message_ai_cat,\n",
    "        message_human_fish\n",
    "    ]\n",
    "    response_fish_query = chat.invoke(messages_for_fish_query)\n",
    "    print(\"\\nResponse to Fish Query (after 2 examples):\")\n",
    "    print(response_fish_query.content)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *(Note: The actual sarcastic content for `message_ai_dog` and `message_ai_cat` is illustrative here, based on the video's description of generating/copy-pasting sarcastic responses.)*\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **What is the primary purpose of using `AIMessage` objects when constructing a prompt list for `ChatOpenAI` in LangChain, particularly in the context of \"few-shot prompting\" as demonstrated in the video?**\n",
    "    - In few-shot prompting, `AIMessage` objects are used to provide the LLM with explicit examples of desired AI-generated responses corresponding to preceding `HumanMessage` (user) inputs. This helps the model learn the preferred style, tone (like sarcasm in the video), or format for its own subsequent replies to new human queries.\n",
    "- **The video mentions that while adding more examples (HumanMessage/AIMessage pairs) for few-shot prompting can improve the model's responses, there's a downside. What is this downside, and why is it important to consider?**\n",
    "    - The downside of adding more examples is that it increases the number of prompt tokens sent to the model with each request. This is important to consider because it can lead to higher API costs (as charges are often per token) and may eventually hit the model's maximum context window limit, preventing the prompt from being processed.\n",
    "- **If I want to teach my LangChain chatbot to respond in a very specific, structured format (e.g., always providing answers as a JSON object with certain keys), how could I use the few-shot prompting technique with `HumanMessage` and `AIMessage` to achieve this?**\n",
    "    - You would create a series of `HumanMessage` objects containing example user queries, and for each, you would craft a corresponding `AIMessage` where the `content` is a string representing the desired JSON output format with the relevant example data. By providing these input-output pairs, the model learns to generate its responses in that specific JSON structure for new, similar queries.\n",
    "\n",
    "# Prompt templates and prompt values\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This content introduces **string prompt templates** in Lang Chain as a way to create reusable and dynamic prompts, addressing the limitation of non-reusable chat messages. It explains how to define a template with placeholders, use the `PromptTemplate` class to manage it, and invoke it with specific values to generate a filled-in prompt, which is crucial for building flexible and efficient LLM applications.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üí° **Reusable Prompts**: String prompt templates allow developers to define a prompt structure once and reuse it with different inputs, making LLM interactions more scalable and maintainable. This is useful in data science for standardizing queries or instructions to models for repetitive tasks like sentiment analysis or text generation with varying contexts.\n",
    "- üè∑Ô∏è **Placeholder Variables**: Templates use placeholders (e.g., `{description}`, `{pet}`) that can be dynamically filled with specific values at runtime. This allows for personalized or context-specific prompt generation, essential for creating chatbots that adapt to user inputs or for batch processing diverse datasets.\n",
    "- ‚öôÔ∏è **`PromptTemplate` Class**: Lang Chain provides the `PromptTemplate` class to manage string templates, including an `invoke` method that accepts a dictionary to fill in the placeholders. This class streamlines the process of creating and using dynamic prompts in data science workflows.\n",
    "- üîÑ **`invoke` Method Behavior**: The `invoke` method of a `PromptTemplate` object takes a dictionary mapping placeholder names to their values and returns a `PromptValue` object. This contrasts with `ChatOpenAI`'s `invoke` which takes strings or message lists. Understanding these distinctions is key for chaining operations.\n",
    "- üß± **Building Block for Advanced Chains**: Understanding prompt templates is foundational for creating more complex sequences (chains) in Lang Chain, where the output of one component (like a formatted prompt) becomes the input for another (like an LLM call). This is fundamental for constructing sophisticated NLP pipelines.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "    - String prompt templates are crucial for efficiency and consistency in LLM applications. They prevent redundant code and ensure prompts follow a standardized structure, which can be vital for controlling model behavior and output quality.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - In customer service chatbots, templates can be used to generate personalized greetings or responses based on customer data. In content generation, they can produce articles or summaries following a specific format but with varying topics or keywords. For data analysis, they can structure queries to an LLM for information extraction from different documents.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "    - This relates directly to **Chat Prompt Templates** (its more advanced counterpart for chat models), **LLM Chains** (where prompt templates are often the first step), **Few-Shot Learning** (where templates can format examples for the model), and general **Software Engineering Principles** like DRY (Don't Repeat Yourself) and modularity.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "The text describes the following process for creating and using string prompt templates:\n",
    "\n",
    "1. **Import necessary class:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    \n",
    "    ```\n",
    "    \n",
    "2. **Define a string template with placeholders:**\n",
    "    \n",
    "    ```python\n",
    "    TEMPLATE = \"\"\"System: {description}\n",
    "    Human: I've recently adopted a {pet}. Could you suggest some {pet} names?\"\"\"\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *(Note: The video uses all caps for the template variable name, `TEMPLATE`)*\n",
    "    \n",
    "3. **Create a `PromptTemplate` instance from the string template:**\n",
    "    \n",
    "    ```python\n",
    "    prompt_template = PromptTemplate.from_template(template=TEMPLATE)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "4. **Inspect input variables (optional):**\n",
    "    \n",
    "    ```python\n",
    "    print(prompt_template.input_variables)\n",
    "    # Output would be ['description', 'pet']\n",
    "    \n",
    "    ```\n",
    "    \n",
    "5. **Invoke the prompt template with a dictionary of values for the placeholders:**\n",
    "    \n",
    "    ```python\n",
    "    prompt_value = prompt_template.invoke({\n",
    "        'description': 'The chatbot should reluctantly answer questions with sarcastic responses.',\n",
    "        'pet': 'dog'\n",
    "    })\n",
    "    \n",
    "    ```\n",
    "    \n",
    "6. **Examine the `PromptValue` object and extract the formatted text:**\n",
    "    \n",
    "    ```python\n",
    "    print(prompt_value)\n",
    "    # This would show an instance of StringPromptValue\n",
    "    print(prompt_value.text)\n",
    "    # Output would be:\n",
    "    # System: The chatbot should reluctantly answer questions with sarcastic responses.\n",
    "    # Human: I've recently adopted a dog. Could you suggest some dog names?\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can use string prompt templates to standardize instructions for an LLM when performing tasks like text summarization across multiple documents, generating code documentation based on function signatures, or creating consistent evaluation prompts for model outputs.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - String prompt templates are like fill-in-the-blank forms for your AI, allowing you to reuse the same question or instruction structure with different details plugged in each time.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This concept is highly relevant for any project involving generative AI or LLMs, especially in domains like chatbot development, automated content creation (e.g., marketing copy, reports), personalized education tools, or any application requiring dynamic and consistent natural language instructions to an AI.\n",
    "\n",
    "# Chat prompt templates and chat prompt values\n",
    "\n",
    "## Summary\n",
    "\n",
    "This video tutorial explains how to use LangChain's `ChatPromptTemplate` to structure interactions with chat models like OpenAI's GPT-4, building upon concepts of individual prompt templates, system messages, and human messages. The lesson guides users through defining string templates for system and human roles, converting these into `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate` objects, and then combining them into a `ChatPromptTemplate`. This `ChatPromptTemplate` is then invoked with specific input variable values to produce a `ChatPromptValue`, which is subsequently passed to the `ChatOpenAI` model's `invoke` method to generate a response. The tutorial emphasizes the sequential nature of these invocations as a foundational concept for understanding chains in LangChain.\n",
    "\n",
    "## Highlights\n",
    "\n",
    "- üß± **Building on Prior Knowledge:** The lesson assumes familiarity with basic prompt templates, `SystemMessage`, and `HumanMessage` concepts from previous sessions.\n",
    "    - *Relevance (Data Science):* Reinforces the layered learning approach typical in mastering complex frameworks like LangChain, where foundational components are combined for more advanced functionality.\n",
    "- üí¨ **Introduction to `ChatPromptTemplate`:** The core focus is on using `ChatPromptTemplate` to manage and format a sequence of messages (like system and human messages) with dynamic inputs for chat models.\n",
    "    - *Relevance:* This is a crucial LangChain component for creating sophisticated, multi-turn, or role-based conversations with LLMs, allowing for structured and reusable chat interaction patterns.\n",
    "- üìù **Step-by-Step Implementation Process:**\n",
    "    1. **Define String Templates:** Create separate string templates for the system message and the human message, incorporating placeholders for input variables (e.g., `{description}` for system, `{pet}` for human).\n",
    "        - System Template Example: `template_system = \"{description}\"`\n",
    "        - Human Template Example: `template_human = \"I've recently adopted a {pet}. Could you suggest some {pet} names?\"`\n",
    "    2. **Create Message Prompt Templates:** Convert these string templates into specific message prompt template objects:\n",
    "        - `message_template_system = SystemMessagePromptTemplate.from_template(template=template_system)`\n",
    "        - `message_template_human = HumanMessagePromptTemplate.from_template(template=template_human)`\n",
    "        The video notes that these objects contain a `prompt` attribute, which itself is an instance of `PromptTemplate` holding the input variables and the original string template.\n",
    "    3. **Combine into `ChatPromptTemplate`:** Use the `ChatPromptTemplate.from_messages()` class method to combine the individual message prompt templates into a single `ChatPromptTemplate` object.\n",
    "        - `chat_template = ChatPromptTemplate.from_messages([message_template_system, message_template_human])`\n",
    "        This `chat_template` object will then correctly identify all unique input variables (e.g., `description`, `pet`) from the combined messages.\n",
    "    4. **Format with Input Values (Produce `ChatPromptValue`):** Invoke the `chat_template` object with a dictionary providing values for its input variables. This step generates a `ChatPromptValue` object.\n",
    "        - `chat_value = chat_template.invoke({\"description\": \"You are Marv...\", \"pet\": \"dog\"})`\n",
    "        The `ChatPromptValue` object contains the fully formatted messages (now instances of `SystemMessage` and `HumanMessage`) with placeholders filled.\n",
    "    5. **Invoke the Chat Model:** Pass the `ChatPromptValue` object to the `invoke` method of the initialized `ChatOpenAI` model instance (`chat`).\n",
    "        - `response = chat.invoke(chat_value)`\n",
    "        The model processes these formatted messages and returns an `AIMessage` object containing the AI's response.\n",
    "        <!-- end list -->\n",
    "    - *Relevance:* This detailed breakdown provides a clear and reproducible workflow for constructing and utilizing chat-specific prompt templates in LangChain, essential for any chat-based LLM application.\n",
    "- ‚ú® **Key LangChain Classes Utilized:**\n",
    "    - `ChatPromptTemplate`\n",
    "    - `SystemMessagePromptTemplate`\n",
    "    - `HumanMessagePromptTemplate`\n",
    "    - (Implicitly, `PromptTemplate` is used within the message prompt templates)\n",
    "    - `ChatPromptValue` (as the output of invoking a `ChatPromptTemplate`)\n",
    "    - `SystemMessage`, `HumanMessage`, `AIMessage` (as concrete message instances)\n",
    "    - `ChatOpenAI` (as the chat model)\n",
    "    - *Relevance:* Familiarizes users with the specific classes and their roles in LangChain's prompt templating and chat model interaction system.\n",
    "- üîÑ **The \"Invoke Chain\" Pattern:** A significant observation highlighted is that the output of one `invoke` method often serves as the input to another. For example, `chat_template.invoke(...)` produces `chat_value`, which is then used in `chat.invoke(chat_value)`.\n",
    "    - *Relevance:* This sequential invocation pattern is a fundamental concept in LangChain and serves as a \"stepping stone\" towards understanding how more complex \"chains\" and the LangChain Expression Language (LCEL) work to orchestrate multiple steps in an LLM application.\n",
    "- ‚öôÔ∏è **Pre-configured Environment:** The lesson starts with an assumed setup including API key configuration, necessary imports (though some new ones like `ChatPromptTemplate` are introduced), and an initialized `ChatOpenAI` object (`chat`) with parameters like `model_name`, `model_kwargs`, and `temperature`.\n",
    "    - *Relevance:* Allows the tutorial to focus directly on the `ChatPromptTemplate` concepts without repeating basic setup.\n",
    "\n",
    "## Conceptual Understanding\n",
    "\n",
    "- **The Role and Benefits of `ChatPromptTemplate` for Structured Chat Interactions:**\n",
    "    - *Why is this concept important to know or understand?* Chat models typically perform best when interactions are structured as a sequence of messages with defined roles (system, user/human, assistant/AI). `ChatPromptTemplate` in LangChain provides a robust and flexible mechanism to create these structured, multi-message prompts dynamically. It allows developers to define templates for different message roles (like system instructions and user queries) with placeholders for variables. When these templates are filled with actual values, they produce a `ChatPromptValue` which represents a complete, formatted conversation history or context ready to be sent to the chat model.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?*\n",
    "        - **Persona Management:** Consistently apply a system persona across multiple interactions by templating the system message.\n",
    "        - **Dynamic User Queries:** Easily insert user-specific details or varying questions into a human message template.\n",
    "        - **Multi-Turn Conversations:** While this lesson focuses on a two-message setup, `ChatPromptTemplate` can be extended to handle longer conversational histories by incorporating lists of `HumanMessage` and `AIMessage` objects.\n",
    "        - **Standardization and Reusability:** Promotes cleaner code by separating prompt logic from application code, making prompts easier to manage, version, and reuse.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This is a direct extension of basic **prompt templating** but specifically tailored for the message-based structure of chat models. It's fundamental to building **conversational AI**, **dialogue systems**, and is closely tied to the way models like GPT-4 expect input via the OpenAI Chat Completions API (which uses a list of message objects).\n",
    "- **Understanding the Sequential Invocation Pattern (`template.invoke` -\\> `model.invoke`):**\n",
    "    - *Why is this concept important to know or understand?* The video highlights a key pattern in LangChain: the output of one component's `invoke` method often becomes the input for another component's `invoke` method. In this lesson:\n",
    "        1. `chat_template.invoke(input_dict)` takes a dictionary of variables and returns a `ChatPromptValue` (which is a formatted set of messages).\n",
    "        2. `chat_model.invoke(chat_prompt_value)` takes this `ChatPromptValue` and returns an `AIMessage` (the model's response).\n",
    "        This sequence demonstrates a simple \"chain\" of operations. Understanding this flow is crucial because it's the foundational principle behind LangChain's core strength: chaining together LLMs, tools, and data processing steps to build complex applications.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* This pattern is the basis for almost all LangChain applications. More complex chains might involve:\n",
    "        - Retrieving data, then formatting it into a prompt, then calling an LLM.\n",
    "        - Calling an LLM, then parsing its output, then using that output to call another tool or LLM.\n",
    "        The `invoke` method (and its asynchronous counterpart `ainvoke`, plus streaming methods) is the standard way to execute these components within such sequences.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This is the practical application of the **LangChain Expression Language (LCEL)**, which uses a pipe (`|`) syntax to make these chains more readable (e.g., `chat_template | chat_model`). It relates to concepts of **pipelines in data processing**, **functional programming (composition of functions)**, and the overall architecture of **LangChain chains and agents**.\n",
    "\n",
    "## Code Examples\n",
    "\n",
    "- **Conceptual Pre-configured Code (Assumed at start of notebook):**\n",
    "    \n",
    "    ```python\n",
    "    # API Key Setup (e.g., using dotenv)\n",
    "    # %load_ext dotenv\n",
    "    # %dotenv\n",
    "    \n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage # Previously used\n",
    "    # New/Key imports for this lesson would be added here:\n",
    "    from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "    \n",
    "    # Initialized chat model\n",
    "    chat = ChatOpenAI(\n",
    "        model_name=\"gpt-4\", # Or other model\n",
    "        model_kwargs={\"seed\": 365}, # Example\n",
    "        temperature=0 # Example\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Defining String Templates:**\n",
    "    \n",
    "    ```python\n",
    "    template_system = \"{description}\"\n",
    "    template_human = \"I've recently adopted a {pet}. Could you suggest some {pet} names?\"\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Creating Message Prompt Templates from String Templates:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "    \n",
    "    message_template_system = SystemMessagePromptTemplate.from_template(template=template_system)\n",
    "    message_template_human = HumanMessagePromptTemplate.from_template(template=template_human)\n",
    "    \n",
    "    # To inspect (as shown in video):\n",
    "    # print(message_template_system)\n",
    "    # print(message_template_system.prompt)\n",
    "    # print(message_template_system.prompt.input_variables)\n",
    "    # print(message_template_system.prompt.template)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Creating `ChatPromptTemplate` from Message Prompt Templates:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    \n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        [message_template_system, message_template_human]\n",
    "    )\n",
    "    \n",
    "    # To inspect (as shown in video):\n",
    "    # print(chat_template)\n",
    "    # print(chat_template.input_variables)\n",
    "    # print(chat_template.messages)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Invoking `ChatPromptTemplate` to Get `ChatPromptValue`:**\n",
    "    \n",
    "    ```python\n",
    "    # Example input dictionary (values might come from user input or other processes)\n",
    "    input_values = {\n",
    "        \"description\": \"You are Marv, a chatbot that reluctantly answers questions with sarcastic responses.\",\n",
    "        \"pet\": \"dog\"\n",
    "    }\n",
    "    \n",
    "    chat_value = chat_template.invoke(input_values)\n",
    "    \n",
    "    # To inspect (as shown in video):\n",
    "    # print(chat_value)\n",
    "    # print(type(chat_value)) # Expected: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
    "    # print(chat_value.messages)\n",
    "    # print(type(chat_value.messages[0])) # Expected: <class 'langchain_core.messages.system.SystemMessage'>\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Invoking `ChatOpenAI` Model with `ChatPromptValue`:**\n",
    "    \n",
    "    ```python\n",
    "    response = chat.invoke(chat_value)\n",
    "    \n",
    "    # To inspect and print content (as shown in video):\n",
    "    # print(type(response)) # Expected: <class 'langchain_core.messages.ai.AIMessage'>\n",
    "    print(response.content)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## Reflective Questions\n",
    "\n",
    "- **What is the primary role of the `ChatPromptTemplate` class in LangChain when working with chat models, and how does it differ from using individual `SystemMessage` or `HumanMessage` objects directly with the model's `invoke` method?**\n",
    "    - **AI-generated answer:** The `ChatPromptTemplate` class in LangChain is used to create a reusable structure for a sequence of chat messages (like system and human messages) that include placeholders for dynamic input. It differs from using individual message objects directly because it allows you to define a template once and then easily format it with different input values to generate a complete `ChatPromptValue` (a list of filled messages), which can then be passed to the model. This promotes consistency and reusability for complex chat interactions.\n",
    "- **The video describes a two-step invocation process: first invoking the `ChatPromptTemplate` and then invoking the `ChatOpenAI` model. What type of object is produced by the first invocation, and what type of object is the final output after the second invocation?**\n",
    "    - **AI-generated answer:** The first invocation, `chat_template.invoke(input_dictionary)`, produces a `ChatPromptValue` object, which contains the formatted messages with placeholders filled. The second invocation, `chat_model.invoke(chat_prompt_value)`, takes this `ChatPromptValue` and produces an `AIMessage` object, which contains the AI's actual response content.\n",
    "- **The speaker mentions that understanding the pattern where \"the output of one invoke method serves as the input to another\" is a stepping stone to understanding chains in LangChain. Can you briefly explain what this means in the context of this lesson's example?**\n",
    "    - **AI-generated answer:** In this lesson's example, the `ChatPromptTemplate` is first invoked with a dictionary of values (e.g., for `description` and `pet`), producing a `ChatPromptValue` (a structured set of ready-to-send messages). This `ChatPromptValue` then becomes the direct input to the `ChatOpenAI` model's `invoke` method to get the final AI response. This sequence, where the result of one operation feeds directly into the next, illustrates a simple chain of components working together.\n",
    "\n",
    "# Few-shot chat message prompt templates\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This video tutorial introduces a more elegant and scalable method for implementing few-shot prompting in LangChain using the `FewShotChatMessagePromptTemplate` class. It addresses the \"clumsiness\" of manually creating individual `HumanMessage` and `AIMessage` pairs for each example, as might have been done previously. The lesson demonstrates how to define string templates for human inputs and AI responses, combine them into a blueprint `ChatPromptTemplate` (referred to as `example_template`), prepare a list of example dictionaries, and then use these with `FewShotChatMessagePromptTemplate`. This setup is then integrated into a final `ChatPromptTemplate` along with a template for the new user query, providing a clean way to feed multiple examples to the `ChatOpenAI` model to guide its behavior (e.g., to achieve a sarcastic tone).\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üîç **Recap and Problem Statement:** The lesson starts by recapping previous concepts (`ChatOpenAI`, message types like `SystemMessage`, `HumanMessage`, `AIMessage`, basic prompt templates, and `ChatPromptTemplate`). It then highlights that the prior method of implementing few-shot prompting by defining numerous separate `HumanMessage` and `AIMessage` objects was cumbersome, cluttered the code, and was not easily scalable for many examples.\n",
    "    - *Relevance (Data Science):* Acknowledges a common pain point in early explorations of few-shot prompting and sets the stage for a more refined, production-friendly approach‚Äîcrucial for data scientists building maintainable LLM applications.\n",
    "- ‚ú® **Introducing `FewShotChatMessagePromptTemplate`:** The core of the lesson is the introduction and demonstration of the `FewShotChatMessagePromptTemplate` class from LangChain's prompt module. This class is designed to streamline the process of constructing prompts with multiple examples.\n",
    "    - *Relevance:* This class is a key LangChain component for efficiently managing and injecting few-shot examples into a chat model's context, making it easier to guide model behavior without extensive manual message list construction.\n",
    "- üõ†Ô∏è **Streamlined Few-Shot Prompting Workflow:** The video outlines a cleaner, more compact implementation strategy:\n",
    "    1. **Define Basic String Templates:** Create simple string templates for the human input part of an example and the AI response part of an example.\n",
    "        - Human template example: `template_human_example = \"I've recently adopted a {pet}. Could you suggest some {pet} names?\"` (Input variable: `{pet}`)\n",
    "        - AI template example: `template_ai_example = \"{response}\"` (Input variable: `{response}`)\n",
    "    2. **Create an \"Example Blueprint\" (`example_template`):** Combine these human and AI string templates into a single `ChatPromptTemplate` instance. This blueprint will define the structure for each individual few-shot example.\n",
    "        - This is done using `ChatPromptTemplate.from_messages([...])` with `HumanMessagePromptTemplate.from_template(template_human_example)` and `AIMessagePromptTemplate.from_template(template_ai_example)`.\n",
    "    3. **Prepare a List of Example Data:** Create a Python list where each element is a dictionary. Each dictionary holds the actual values for the input variables (e.g., `pet` and `response`) for one complete human-AI interaction example.\n",
    "        - Example: `examples = [{\"pet\": \"dog\", \"response\": \"Sarcastic dog name suggestions...\"}, {\"pet\": \"cat\", \"response\": \"Sarcastic cat name suggestions...\"}]`\n",
    "    4. **Instantiate `FewShotChatMessagePromptTemplate`:** Create an instance of `FewShotChatMessagePromptTemplate`, providing it with:\n",
    "        - `examples`: The list of example dictionaries created above.\n",
    "        - `example_prompt`: The `example_template` (the blueprint `ChatPromptTemplate`) created in step 2.\n",
    "        - Example: `few_shot_prompt = FewShotChatMessagePromptTemplate(examples=examples, example_prompt=example_template)`\n",
    "    5. **Construct the Final `ChatPromptTemplate`:** Create the main `ChatPromptTemplate` that will be used for the actual user query. This template will include:\n",
    "        - The `few_shot_prompt` object (which will dynamically insert all the formatted examples).\n",
    "        - A new `HumanMessagePromptTemplate` for the current user's query (e.g., a template for asking about \"rabbit\" names, which will take a `{pet}` input variable).\n",
    "        - Example: `final_chat_template = ChatPromptTemplate.from_messages([few_shot_prompt, human_message_template_for_new_query])`\n",
    "    6. **Format with New Query Input (`.invoke` on `ChatPromptTemplate`):** Call the `invoke` method on this `final_chat_template`, passing a dictionary with the input for the *new* query (e.g., `{\"pet\": \"rabbit\"}`). This produces a `ChatPromptValue`.\n",
    "        - `chat_value = final_chat_template.invoke({\"pet\": \"rabbit\"})`\n",
    "        The `chat_value.messages` attribute will now be a list containing all the formatted example `HumanMessage`s and `AIMessage`s, followed by the newly formatted `HumanMessage` for the \"rabbit\" query.\n",
    "    7. **Invoke the Chat Model (`.invoke` on `ChatOpenAI`):** Pass the `chat_value` object to the `invoke` method of the initialized `ChatOpenAI` instance.\n",
    "        - `response = chat.invoke(chat_value)`\n",
    "        The model now processes all the examples and the new query, hopefully generating a response in the learned style (e.g., sarcastic).\n",
    "        <!-- end list -->\n",
    "    - *Relevance:* This structured workflow is significantly more maintainable and scalable than manually listing out each message object, especially when dealing with many examples or complex prompt structures.\n",
    "- üìà **Scalability and Cleanliness:** The primary advantage highlighted is that adding more few-shot examples is now much simpler: one only needs to add a new dictionary to the `examples` list and then regenerate the templates. This avoids the clutter of defining numerous individual `HumanMessage` and `AIMessage` variables.\n",
    "    - *Relevance:* For practical applications where refining model behavior might require iterating with different sets or numbers of examples, this approach is far more efficient and developer-friendly.\n",
    "- üîÑ **Consistent Invocation Pattern:** The lesson reinforces the pattern where the output of one component's `invoke` method (like `final_chat_template.invoke()`) feeds into another's (`chat.invoke()`).\n",
    "    - *Relevance:* This pattern is fundamental to how LangChain composes more complex sequences and \"chains\" of operations, which is a core aspect of the framework.\n",
    "- ‚öôÔ∏è **Assumed Setup:** The tutorial starts with a Jupyter Notebook where the OpenAI API key is configured, the `ChatOpenAI` model (`chat`) is initialized, and necessary base classes like `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` (or their direct use within `ChatPromptTemplate.from_messages`) are assumed to be available or imported.\n",
    "    - *Relevance:* Allows the lesson to focus squarely on the `FewShotChatMessagePromptTemplate` functionality.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **The Purpose and Advantage of `FewShotChatMessagePromptTemplate`:**\n",
    "    - *Why is this concept important to know or understand?* While few-shot prompting can be achieved by manually constructing a list of `HumanMessage` and `AIMessage` objects, this becomes cumbersome and error-prone as the number of examples or the complexity of each example increases. The `FewShotChatMessagePromptTemplate` class in LangChain provides a dedicated, structured, and more programmatic way to manage these examples. It separates the *data* for the examples (provided as a list of dictionaries) from the *formatting logic* for each example (defined by an `example_prompt`, which is itself a `ChatPromptTemplate`). This separation makes the process cleaner, more scalable, and easier to maintain.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?*\n",
    "        - **Managing Many Examples:** If you need to provide 5, 10, or even more examples to sufficiently guide the LLM, `FewShotChatMessagePromptTemplate` is far more manageable than writing out 10-20 individual message object instantiations.\n",
    "        - **Dynamic Example Sets:** The list of examples can be dynamically generated or loaded from a file, allowing for flexible adaptation of the few-shot set based on context or user needs.\n",
    "        - **Consistent Formatting:** Ensures all examples are formatted consistently according to the `example_prompt` blueprint.\n",
    "        - **Improved Readability:** Keeps the main prompt construction logic cleaner by abstracting the example injection process.\n",
    "    - *What other concepts, techniques, or areas is this related to?* This is an advanced form of **prompt engineering**, specifically focused on **in-context learning** via few-shot examples. It relates to principles of **code organization and reusability**, and data-driven configuration of prompts.\n",
    "- **Constructing a Multi-Part Chat Prompt with Examples and a New Query:**\n",
    "    - *Why is this concept important to know or understand?* The end goal of using `FewShotChatMessagePromptTemplate` is to construct a final prompt (represented as a `ChatPromptValue` containing a list of messages) that effectively \"teaches\" the model through examples and then presents it with a new query. The structure typically involves:\n",
    "        1. The examples formatted by `FewShotChatMessagePromptTemplate`.\n",
    "        2. The new user query, also formatted, usually as a `HumanMessage`.\n",
    "        This entire sequence is what the LLM processes to generate its final response.\n",
    "    - *How does it connect with real-world tasks, problems, or applications?* This pattern is how you would implement few-shot learning for a chat model in LangChain. The `ChatPromptTemplate.from_messages()` method is used to assemble these parts: the `few_shot_prompt` object (which takes care of rendering all the examples) and then one or more `MessagePromptTemplate` objects for the current turn of conversation (e.g., a `SystemMessagePromptTemplate` for overall instructions and a `HumanMessagePromptTemplate` for the user's latest input).\n",
    "    - *What other concepts, techniques, or areas is this related to?* This ties into the broader concept of **context window management** (ensuring all examples plus the new query fit), **dialogue modeling**, and the way chat-based LLMs process conversational history to generate relevant next turns.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "- **Importing Necessary Classes (Illustrative, based on described functionality):**\n",
    "    \n",
    "    ```python\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    from langchain.prompts import (\n",
    "        ChatPromptTemplate,\n",
    "        SystemMessagePromptTemplate, # May not be used if sarcasm is purely from few-shot\n",
    "        HumanMessagePromptTemplate,\n",
    "        AIMessagePromptTemplate # Used implicitly or explicitly for example_template\n",
    "    )\n",
    "    from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "    # Assuming HumanMessage, AIMessage are also available from langchain_core.messages\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **Conceptual Pre-configured `ChatOpenAI` Object:**\n",
    "    \n",
    "    ```python\n",
    "    chat = ChatOpenAI(\n",
    "        model_name=\"gpt-4\",\n",
    "        model_kwargs={\"seed\": 365, \"max_tokens\": 150}, # Example max_tokens\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **1. Define String Templates for Individual Examples:**\n",
    "    \n",
    "    ```python\n",
    "    template_human_example = \"I've recently adopted a {pet}. Could you suggest some {pet} names?\"\n",
    "    template_ai_example = \"{response}\" # The AI's example sarcastic response\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **2. Create an \"Example Blueprint\" (`example_template`) using `ChatPromptTemplate`:**\n",
    "    \n",
    "    ```python\n",
    "    # This example_template defines how each pet/response pair in our list of examples will be formatted\n",
    "    example_template = ChatPromptTemplate.from_messages([\n",
    "        HumanMessagePromptTemplate.from_template(template_human_example),\n",
    "        AIMessagePromptTemplate.from_template(template_ai_example)\n",
    "    ])\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **3. Prepare a List of Example Data (Dictionaries):**\n",
    "    \n",
    "    ```python\n",
    "    examples = [\n",
    "        {\n",
    "            \"pet\": \"dog\",\n",
    "            \"response\": \"Oh, joy. Another canine enthusiast. How about 'Captain Floofington' or 'Baron von Wigglebottom'? Try not to bore it to death.\"\n",
    "        },\n",
    "        {\n",
    "            \"pet\": \"cat\",\n",
    "            \"response\": \"A cat, you say? Prepare for stylish indifference. Perhaps 'Empress Clawdia' or 'Sir Reginald Fluffytail III, Esq.'? Or just 'Doom.' That works too.\"\n",
    "        }\n",
    "        # Add more examples here if needed\n",
    "    ]\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **4. Instantiate `FewShotChatMessagePromptTemplate`:**\n",
    "    \n",
    "    ```python\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_template\n",
    "    )\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **5. Construct the Final `ChatPromptTemplate` for the New Query:**\n",
    "    \n",
    "    ```python\n",
    "    # Template for the new human query that will follow the few-shot examples\n",
    "    template_new_human_query = \"I've recently adopted a {pet}. Could you suggest some {pet} names?\"\n",
    "    human_message_template_for_new_query = HumanMessagePromptTemplate.from_template(template_new_human_query)\n",
    "    \n",
    "    final_chat_template = ChatPromptTemplate.from_messages([\n",
    "        few_shot_prompt,  # This will inject all the formatted examples\n",
    "        human_message_template_for_new_query # Template for the actual new query\n",
    "    ])\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **6. Format with New Query Input (Invoke `final_chat_template`):**\n",
    "    \n",
    "    ```python\n",
    "    new_pet_input = {\"pet\": \"rabbit\"} # The input for the new query\n",
    "    chat_value = final_chat_template.invoke(new_pet_input)\n",
    "    \n",
    "    # To inspect the resulting messages (as shown in the video with a for loop):\n",
    "    # for message in chat_value.messages:\n",
    "    #     print(f\"[{message.type.upper()}] {message.content}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "- **7. Invoke the Chat Model with the `ChatPromptValue`:**\n",
    "    \n",
    "    ```python\n",
    "    response = chat.invoke(chat_value)\n",
    "    print(\"\\nFinal Sarcastic Response for Rabbit:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **What is the primary advantage of using `FewShotChatMessagePromptTemplate` in LangChain for providing examples to an LLM, compared to manually creating a list of individual `HumanMessage` and `AIMessage` objects for each example?**\n",
    "    - **AI-generated answer:** The primary advantage of `FewShotChatMessagePromptTemplate` is that it provides a much cleaner, more scalable, and maintainable way to manage few-shot examples. It separates the example data (as a list of dictionaries) from the formatting logic (defined in an `example_prompt`), making it easier to add, remove, or modify examples without cluttering the main code with numerous individual message object instantiations.\n",
    "- **When using `FewShotChatMessagePromptTemplate`, you provide a list of `examples` (dictionaries) and an `example_prompt` (a `ChatPromptTemplate`). What is the role of this `example_prompt`?**\n",
    "    - **AI-generated answer:** The `example_prompt` serves as a blueprint or template that defines how each dictionary in the `examples` list should be formatted into a sequence of `HumanMessage` and/or `AIMessage` objects. It ensures all examples are consistently structured before being injected into the final prompt.\n",
    "- **If I have successfully used `FewShotChatMessagePromptTemplate` to generate a `ChatPromptValue` containing several example interactions and a final human query, what is the next step to get an actual response from the `ChatOpenAI` model, and what kind of object will this final response be?**\n",
    "    - **AI-generated answer:** The next step is to pass the `ChatPromptValue` object (which contains the list of formatted messages) to the `invoke` method of your initialized `ChatOpenAI` model instance (e.g., `response = chat.invoke(chat_value)`). The final response from the model will be an `AIMessage` object, and its textual content can be accessed via the `.content` attribute.\n",
    "\n",
    "# LLMChain\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This content introduces the `LLMChain` class in LangChain, a straightforward method for connecting language models with prompt templates. Understanding `LLMChain` is foundational for building chatbots, as it simplifies the process of sending structured prompts to a model and receiving responses, even though newer methods like LangChain Expression Language (LCEL) are emerging.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ‚ú® **Introduction to `LLMChain`**: `LLMChain` is presented as the most straightforward chain class in LangChain, used to combine a language model and a prompt.\n",
    "    - **Relevance**: Simplifies the initial steps of interacting with language models by providing a clear structure for inputs and outputs, crucial for developing chatbot functionalities.\n",
    "- ‚öôÔ∏è **Instantiation**: An `LLMChain` object is created by providing two required parameters: `llm` (an instance of a language model, e.g., `ChatOpenAI`) and `prompt` (a chat prompt template).\n",
    "    - **Relevance**: Knowing how to instantiate and configure `LLMChain` is key to utilizing LangChain's capabilities for prompt engineering and model interaction in data science projects involving text generation.\n",
    "- üîÑ **Invocation**: The `invoke` method of an `LLMChain` object takes a dictionary as an argument (e.g., `{'pet': 'fish'}`) and returns a dictionary containing the model's response, differing from the `AIMessage` object returned by directly invoking a `ChatOpenAI` instance.\n",
    "    - **Relevance**: Understanding the input and output format of `LLMChain` is essential for processing model responses and integrating them into larger applications or workflows.\n",
    "- ‚ö†Ô∏è **Legacy Approach**: The text notes that while this method of creating chains is straightforward, it is becoming obsolete, with LangChain Expression Language (LCEL) being the novel approach. However, understanding this older method is still beneficial for concepts like chatbot memory.\n",
    "    - **Relevance**: Provides context on the evolution of LangChain and helps in understanding older codebases or specific LangChain features that might still rely on this structure. It's useful for data scientists to be aware of both legacy and current best practices.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "```python\n",
    "# Importing the LLMChain class\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# Assuming 'chat' is an initialized ChatOpenAI instance\n",
    "# and 'chat_template' is an initialized ChatPromptTemplate instance\n",
    "\n",
    "# Creating an LLMChain object\n",
    "chain = LLMChain(llm=chat, prompt=chat_template)\n",
    "\n",
    "# Invoking the chain\n",
    "response = chain.invoke({'pet': 'fish'})\n",
    "\n",
    "# The 'response' variable will be a dictionary, for example:\n",
    "# {'pet': 'fish', 'text': 'Fish are great pets because...'}\n",
    "\n",
    "```\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can use `LLMChain` as a quick way to prototype interactions with language models, test different prompts, and understand the basic flow of data within LangChain before moving to more complex chaining mechanisms like LCEL.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - `LLMChain` is like a simple recipe in LangChain that tells a language model exactly what kind of question to answer (the prompt) and then gets the answer back in a structured way.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This concept is highly relevant for projects involving natural language processing, such as building simple chatbots, content generation tools (e.g., summarizers, creative writing assistants), or any application requiring structured interaction with a language model to generate text based on specific inputs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
