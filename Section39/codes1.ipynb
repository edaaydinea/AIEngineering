{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a736bed8",
   "metadata": {},
   "source": [
    "# ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the line of code below to check the version of langchain in the current environment.\n",
    "# Substitute \"langchain\" with any other package name to check their version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a6fc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b5d50afe454091b6f4092641780f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eda AYDIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Eda AYDIN\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Eda AYDIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccf4f2dd1f34ed0b65d9d35e80cbac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025a367f31cb4fa180d0386f97c3c2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a494ce6416a5473795444ddbe81cbda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62317ead23624c1cb3e3142adde658be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc550b5e48b49f68160ac7eaac34d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e693c7e7c589476eb12939c0142a3b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Dog Names (and surrounding text from model):\n",
      "\n",
      "Suggestion 1:\n",
      "I've recently adopted a dog. Could you suggest some dog names? Here are a few:   \"Dawny\"  \n",
      "What is Dawny? Dawny is a small, bright, bright, green, red, white, purple, white, red, pink, white, blue, yellow, red, white, red, white, red, blue, white, blue, white, blue, yellow, yellow, white, red, white, red, white, yellow,\n",
      "\n",
      "Suggestion 2:\n",
      "I've recently adopted a dog. Could you suggest some dog names? Here are a few: ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - ~~~~~~~~~~~~~~~~~~~~~~~ - \n",
      "\n",
      "Suggestion 3:\n",
      "I've recently adopted a dog. Could you suggest some dog names? Here are a few: _____\n",
      "Cobra dog\n",
      "Cat who was born without a collar is a wild dog. However, I have to say that cats are a very big dog. This dog is a very important one, because it gives way to a dog. It is a very important dog. The fact that the two cats are named Cat Cobra and Cat Dober is pretty obvious as well.\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "# (Keep your imports and .env loading if you plan to use other services,\n",
    "# but it's not strictly needed for a local Hugging Face model if no API key is involved)\n",
    "\n",
    "# pip install transformers torch\n",
    "# Make sure to run the above in your environment if you haven't already.\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# For reproducibility with Hugging Face pipelines\n",
    "set_seed(365) # Same seed as you used before\n",
    "\n",
    "# Load a text generation pipeline with a free, open-source model\n",
    "# Using distilgpt2 as an example. You can try other models from the Hugging Face Hub.\n",
    "# Popular options: 'gpt2', 'EleutherAI/gpt-neo-1.3B' (larger, more capable, needs more resources)\n",
    "# For very small models, you might explore TinyLlama or similar.\n",
    "try:\n",
    "    # Using a more specific task for the pipeline\n",
    "    # For something like name generation, 'text-generation' is appropriate.\n",
    "    # Some models are instruction-tuned, others are better at completing text.\n",
    "    # distilgpt2 is more of a text completer.\n",
    "    generator = pipeline('text-generation', model='distilgpt2')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the model or pipeline: {e}\")\n",
    "    print(\"Make sure you have an internet connection for the first download.\")\n",
    "    print(\"You might also need to install PyTorch: pip install torch\")\n",
    "    generator = None # Set to None if loading fails\n",
    "\n",
    "if generator:\n",
    "    prompt = \"I've recently adopted a dog. Could you suggest some dog names? Here are a few: \"\n",
    "    # Note: Smaller models like distilgpt2 work best with more direct prompts or \"few-shot\" examples.\n",
    "    # Giving it a starting pattern can help.\n",
    "\n",
    "    # Parameters for generation:\n",
    "    # max_length: Maximum number of tokens in the generated output (including prompt).\n",
    "    # num_return_sequences: How many different suggestions to generate.\n",
    "    # temperature: Controls randomness. Lower is more deterministic. 0.7-0.9 is common for creative tasks.\n",
    "    # For more deterministic output with smaller models, you might keep it low or experiment.\n",
    "    # top_k, top_p: Other sampling strategies.\n",
    "    try:\n",
    "        generated_responses = generator(\n",
    "            prompt,\n",
    "            max_length=100,       # Adjust as needed\n",
    "            num_return_sequences=3, # Get a few options\n",
    "            temperature=0.7,      # A bit of creativity\n",
    "            do_sample=True        # Necessary for temperature, top_k, top_p to have an effect\n",
    "        )\n",
    "\n",
    "        print(\"Suggested Dog Names (and surrounding text from model):\")\n",
    "        for i, response in enumerate(generated_responses):\n",
    "            print(f\"\\nSuggestion {i+1}:\")\n",
    "            # The output is a dictionary, and the generated text is in 'generated_text'\n",
    "            print(response['generated_text'])\n",
    "\n",
    "            # You might want to parse the actual names from the generated text.\n",
    "            # This often requires some post-processing.\n",
    "            # For example, splitting the text and looking for capitalized words after the prompt.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text generation: {e}\")\n",
    "else:\n",
    "    print(\"Text generation pipeline not loaded. Cannot proceed.\")\n",
    "\n",
    "# Example of how you might try to extract names (very basic):\n",
    "# (This part is highly dependent on the model's output format)\n",
    "# if generator and 'generated_responses' in locals():\n",
    "#     print(\"\\n--- Parsed Name Ideas (Example) ---\")\n",
    "#     for response in generated_responses:\n",
    "#         # Get text after the initial prompt\n",
    "#         text_after_prompt = response['generated_text'][len(prompt):].strip()\n",
    "#         # Split by common delimiters (commas, newlines, spaces)\n",
    "#         potential_names = [name.strip() for name in text_after_prompt.replace(',', ' ').replace('\\n', ' ').split(' ') if name.strip() and name.strip()[0].isupper()]\n",
    "#         print(potential_names[:5]) # Print first few potential names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8746c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Original imports for dotenv if you're still using it for other purposes)\n",
    "# %load_ext dotenv\n",
    "# %dotenv\n",
    "\n",
    "# Core libraries\n",
    "import torch # Explicitly import torch to check its availability early\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed\n",
    "\n",
    "# %pip install langchain_community\n",
    "# Updated LangChain import for HuggingFace\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 365\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Configuration ---\n",
    "# MODEL_ID = \"distilgpt2\" # Sticking with distilgpt2 for now to debug\n",
    "MODEL_ID = \"gpt2\" # Alternative to try if distilgpt2 remains problematic\n",
    "\n",
    "# --- Utility Function for Clean Code ---\n",
    "def create_llm_pipeline(model_id: str, max_new_tokens: int = 50, temperature: float = 0.7, do_sample: bool = True):\n",
    "    \"\"\"\n",
    "    Creates a HuggingFace text generation pipeline and wraps it for LangChain.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The Hugging Face model ID.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        temperature (float): Controls randomness. 0.0 is deterministic.\n",
    "        do_sample (bool): Whether to use sampling; must be True for temperature to have an effect.\n",
    "\n",
    "    Returns:\n",
    "        langchain_huggingface.HuggingFacePipeline: The LangChain LLM wrapper.\n",
    "        None: If an error occurs during pipeline creation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to load tokenizer for model: {model_id}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        print(f\"Attempting to load model: {model_id}\")\n",
    "        # Ensure pad_token_id is set if tokenizer doesn't have one (common for GPT-2 family)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            print(f\"Set tokenizer.pad_token_id to tokenizer.eos_token_id ({tokenizer.eos_token_id})\")\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",  # Uses GPU if available (needs `accelerate` library)\n",
    "            pad_token_id=tokenizer.pad_token_id # Explicitly pass pad_token_id\n",
    "        )\n",
    "        model.config.pad_token_id = model.config.eos_token_id # Another place to ensure it's set for generation\n",
    "\n",
    "        print(\"Creating Hugging Face text-generation pipeline...\")\n",
    "        hf_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature if do_sample else None, # Temperature only if sampling\n",
    "            do_sample=do_sample,\n",
    "            # top_k=50, # Optional: Consider for sampling\n",
    "            # top_p=0.95, # Optional: Consider for sampling\n",
    "            # repetition_penalty=1.2 # Optional: To discourage repetition\n",
    "        )\n",
    "        \n",
    "        print(\"Wrapping pipeline with LangChain's HuggingFacePipeline...\")\n",
    "        # Using the new import\n",
    "        llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "        return llm\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"A required library is missing: {e}\")\n",
    "        print(\"Please ensure langchain-huggingface, transformers, torch, and accelerate are installed.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during pipeline creation for {model_id}: {e}\")\n",
    "        print(\"This could be due to model download issues, resource limitations, or configuration errors.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab45137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load tokenizer for model: gpt2\n",
      "Attempting to load model: gpt2\n",
      "Set tokenizer.pad_token_id to tokenizer.eos_token_id (50256)\n",
      "Creating Hugging Face text-generation pipeline...\n",
      "Wrapping pipeline with LangChain's HuggingFacePipeline...\n",
      "\n",
      "Using LangChain with model: gpt2\n",
      "Sending prompt: \"Suggest three fun names for a newly adopted puppy: \"\n",
      "\n",
      "--- Response from LangChain wrapped HuggingFace model ---\n",
      "Suggest three fun names for a newly adopted puppy:  \"Puppy\" (pronounced \"puppy-oo\") and \"Puppy-Pee\" (pronounced \"puppy-pee-ee-uh\").  Puppy-Pee is a very cute dog that is very happy to be around\n"
     ]
    }
   ],
   "source": [
    "# --- Main Logic ---\n",
    "llm_instance = create_llm_pipeline(MODEL_ID, max_new_tokens=60, temperature=0.5, do_sample=True)\n",
    "\n",
    "if llm_instance:\n",
    "    # Let's try a simpler prompt first to see if basic generation works\n",
    "    # prompt_text = \"Once upon a time, in a land far away,\"\n",
    "    # If the simple prompt works, then we can try the original one again or a variation\n",
    "    prompt_text = \"Suggest three fun names for a newly adopted puppy: \"\n",
    "\n",
    "    print(f\"\\nUsing LangChain with model: {MODEL_ID}\")\n",
    "    print(f\"Sending prompt: \\\"{prompt_text}\\\"\")\n",
    "\n",
    "    try:\n",
    "        # Using .invoke for LangChain\n",
    "        response = llm_instance.invoke(prompt_text)\n",
    "        \n",
    "        print(\"\\n--- Response from LangChain wrapped HuggingFace model ---\")\n",
    "        # The response from HuggingFacePipeline is typically a string\n",
    "        # Let's ensure we're handling potential encoding issues at the print stage as well,\n",
    "        # though the primary fix should be in generation.\n",
    "        if isinstance(response, str):\n",
    "            print(response.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "        else:\n",
    "            print(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM invocation: {e}\")\n",
    "else:\n",
    "    print(f\"LLM Pipeline for {MODEL_ID} could not be created. Cannot proceed.\")\n",
    "\n",
    "# --- Test Writing (Conceptual) ---\n",
    "# def test_llm_output_not_garbled(response_text):\n",
    "#    assert \"???\" not in response_text, \"Output contains garbled characters.\"\n",
    "#    # More sophisticated checks could involve regex for expected language characters.\n",
    "\n",
    "# def test_llm_suggests_names(response_text):\n",
    "#    # This is harder, as \"names\" is subjective, but we can check for patterns.\n",
    "#    # For instance, check if there are capitalized words after the prompt.\n",
    "#    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
